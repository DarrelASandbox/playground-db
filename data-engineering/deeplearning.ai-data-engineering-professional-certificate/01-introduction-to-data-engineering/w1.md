- [Introduction to Data Engineering](#introduction-to-data-engineering)
  - [The Fundamentals of Data Engineering: Processes, Life Cycle, and Undercurrents](#the-fundamentals-of-data-engineering-processes-life-cycle-and-undercurrents)
  - [The Evolution of Data Engineering](#the-evolution-of-data-engineering)
  - [Finding and Delivering Business Value as a Data Engineer](#finding-and-delivering-business-value-as-a-data-engineer)
  - [Insights from Sol Rashidi: Data Engineering, Leadership, and Communication](#insights-from-sol-rashidi-data-engineering-leadership-and-communication)
  - [Jordan Morrow on Data Literacy and Business Alignment for Data Engineers](#jordan-morrow-on-data-literacy-and-business-alignment-for-data-engineers)
  - [Requirements Gathering in Data Engineering: A Simulated Stakeholder Conversation](#requirements-gathering-in-data-engineering-a-simulated-stakeholder-conversation)
  - [Key Elements of Requirements Gathering in Data Engineering](#key-elements-of-requirements-gathering-in-data-engineering)
  - [A Framework for Thinking Like a Data Engineer: From Requirements to Deployment](#a-framework-for-thinking-like-a-data-engineer-from-requirements-to-deployment)
- [Data Engineering on the Cloud](#data-engineering-on-the-cloud)
  - [Introduction to Cloud-Based Data Engineering with AWS](#introduction-to-cloud-based-data-engineering-with-aws)
  - [Introduction to AWS Cloud Computing for Data Engineers](#introduction-to-aws-cloud-computing-for-data-engineers)
  - [Understanding AWS Cloud Infrastructure and Services](#understanding-aws-cloud-infrastructure-and-services)
  - [AWS Regions and Availability Zones](#aws-regions-and-availability-zones)
    - [AWS Regions](#aws-regions)
    - [Availability Zones (AZs)](#availability-zones-azs)
  - [Introduction to Core AWS Services for Data Engineering](#introduction-to-core-aws-services-for-data-engineering)
  - [Compute - Amazon Elastic Compute Cloud (EC2)](#compute---amazon-elastic-compute-cloud-ec2)
    - [What is a server? How is a virtual server different from a regular server?](#what-is-a-server-how-is-a-virtual-server-different-from-a-regular-server)
    - [Amazon EC2](#amazon-ec2)
    - [References](#references)
  - [Networking - Virtual Private Cloud (VPC) \& Subnets](#networking---virtual-private-cloud-vpc--subnets)
    - [What is a network?](#what-is-a-network)
    - [What is an IP address?](#what-is-an-ip-address)
    - [What is a VPC?](#what-is-a-vpc)
    - [What is a subnet?](#what-is-a-subnet)
    - [References](#references-1)
  - [Security - AWS Shared Responsibility Model](#security---aws-shared-responsibility-model)
  - [References](#references-2)
- [Week 1 Quiz](#week-1-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Introduction to Data Engineering

- [The Present and Future of Data Engineering](https://datastackshow.com/podcast/29-the-present-and-future-of-data-engineering-with-joe-reis-and-matthew-housley-from-ternary-data/)

## The Fundamentals of Data Engineering: Processes, Life Cycle, and Undercurrents

Your job is to get **raw data** from somewhere, turn it into **something useful**, and then make it **available** for downstream use cases.

- ğŸ—ï¸ **Definition of Data Engineering**: It involves developing, implementing, and maintaining systems that transform raw data into high-quality information for downstream use cases.
- ğŸŒ **Origins of Data Engineering**: Early data engineers focused on logs as byproducts, but their role evolved as the value of data became clear.
- ğŸ”„ **The Data Engineering Life Cycle**: Comprising data generation, ingestion, transformation, storage, serving, and downstream use cases like analytics and machine learning.
- ğŸ“Š **Focus Areas for Data Engineers**: Storage is a central component, integrating into every stage of the life cycle.
- ğŸš€ **End Use Cases**: Outputs include analytics, machine learning, and reverse ETL to provide value across various organizational functions.
- âš™ï¸ **Undercurrents**: These include security, data management, DataOps, data architecture, orchestration, and software engineering, influencing all life cycle stages.
- ğŸ› ï¸ **Data Pipeline**: A combination of architecture, systems, and processes that move data through the life cycle to serve end use cases.
- ğŸ’¡ **Holistic Approach**: A successful data engineer focuses not only on tools but also on aligning systems with stakeholder needs and business objectives.
- ğŸ“š **Visualizing the Life Cycle**: Simplified diagrams help conceptualize the process but may not fully reflect real-world complexity.
- ğŸ“ˆ **Next Steps**: Understanding the history of data engineering provides context, while future videos in the course will dive deeper into each life cycle stage and undercurrent.

## The Evolution of Data Engineering

- ğŸŒ **Omnipresence of Data**: Data exists in many forms, from physical memories to digital records, and has been a cornerstone of knowledge since the dawn of time.
- ğŸ’¾ **The Birth of Digital Data**: The journey began in the 1960s with the introduction of computerized databases, progressing to relational databases and SQL in the 1970s.
- ğŸ—ï¸ **Data Warehousing**: The 1980s saw the creation of data warehouses by Bill Inmon, laying the groundwork for analytical decision-making.
- ğŸŒ **The Internet Revolution**: The 1990s brought a surge in web-first companies, fostering the need for advanced databases, storage, and reporting systems.
- ğŸš€ **Big Data Era**: The 2000s introduced tools like MapReduce and Hadoop, enabling the management of massive datasets with high velocity, variety, and volume.
- â˜ï¸ **Cloud Computing**: Amazonâ€™s AWS and competitors revolutionized how businesses manage data, making scalable computing and storage accessible globally.
- ğŸ”„ **Real-Time Data Streaming**: Transitioning from batch processing to event streaming enabled businesses to analyze data continuously.
- ğŸ’¡ **Simplified Data Engineering**: By the 2010s, data tools became more user-friendly, reducing the need for costly maintenance and increasing accessibility.
- ğŸ§© **Interoperable Ecosystems**: Today, data engineering involves integrating diverse technologies to achieve business objectives efficiently.
- ğŸŒŸ **Future of Data Engineering**: Modern data engineers leverage past innovations to build impactful systems and contribute to technological advancements.

## Finding and Delivering Business Value as a Data Engineer

- ğŸ’¡ **Focus on Business Value**: Prioritize solutions that align with organizational goals and deliver tangible benefits.
- ğŸ§­ **Stakeholder Perception**: Success depends on how stakeholders perceive the engineerâ€™s work in achieving their objectives.
- ğŸ¦ **Advice from Bill Inman**: Focus on â€œwhere the money isâ€ by identifying areas where technology can deliver business value.
- ğŸ“ˆ **Forms of Business Value**: Value can be seen as revenue growth, cost reduction, efficiency improvements, or strategic contributions like product launches.
- ğŸ› ï¸ **Perception Over Action**: Itâ€™s not just about what you do, but how stakeholders perceive your contributions.
- âš–ï¸ **Resource Constraints**: Stakeholder needs often exceed available resources, necessitating careful prioritization of projects.
- ğŸ¯ **Prioritization Skills**: Knowing what to focus on is critical for maximizing impact within organizational constraints.
- ğŸ¤ **Collaborative Engagement**: Understanding stakeholder goals ensures alignment and fosters better decision-making.
- ğŸ” **Insightful Advice**: Additional resources, such as podcasts, can provide deeper perspectives on aligning with business value.
- ğŸš€ **Next Steps**: Once business goals are clear, identifying system requirements is the next step in delivering impactful solutions.

## Insights from Sol Rashidi: Data Engineering, Leadership, and Communication

- [Podcast with Sol Rashidi - Getting business value from Data](https://www.listennotes.com/podcasts/monday-morning/154-sol-rashidi-getting-b2Bjy6Fr-4P/#google_vignette)
- ğŸ’¡ **Critical Role of Data Engineers**: Data engineers are the foundation of business operations, providing pipelines and analysis essential for decision-making.
- ğŸ”„ **The Art of Translation**: Effective communication involves bridging the gap between technical expertise and business relevance.
- ğŸ† **Career Resilience**: Aspiring data engineers need both technical expertise and resilience, as the role demands adaptability and persistence.
- ğŸŒ **Context is Key**: Data engineers must understand business language and goals to ensure their work aligns with organizational objectives.
- ğŸ¤ **Know Your Audience**: Tailor your communication style to the stakeholderâ€™s technical proficiencyâ€”functional, technical, or hybrid.
- ğŸ§© **Data Engineering as Art and Science**: The role combines creativity and precision, requiring a balance of technical skills and business acumen.
- ğŸ“¢ **Advocacy for Engineers**: Sol stresses that data engineers deserve more recognition for their contributions and encourages them to seek more visibility.
- ğŸ¯ **The Power of Communication**: Focus on how your message is received, not just how it is delivered, to ensure clarity and impact.
- ğŸš€ **From Back Office to Front Office**: Data engineers should advocate for roles that allow greater interaction with business functions if they aspire to leadership.
- ğŸ“ˆ **Learning from Experience**: Titles and reporting structures can indicate how technical or functional a stakeholder may be, guiding how you engage with them.

## Jordan Morrow on Data Literacy and Business Alignment for Data Engineers

- ğŸ’¡ **Data Literacy Defined**: Comfort and confidence in reading, working with, analyzing, and communicating data are essential for all professionals, not just technical experts.
- ğŸ“Š **Adoption of Data Products**: Successful data products require user adoption, which hinges on usersâ€™ familiarity and comfort with data.
- ğŸ” **Requirements Gathering**: Effective requirements gathering involves understanding not just the business goals but also the specific needs and motivations of stakeholders.
- ğŸ¤ **Audience Understanding**: Tailor your communication to different audiences, such as sales, marketing, or finance, focusing on what drives their success.
- ğŸ§  **Business Literacy**: Aspiring data engineers should understand how businesses operate to align their work with organizational goals.
- ğŸš€ **Dual Focus**: Be authentic as a data engineer while developing skills to communicate effectively in a business context.
- ğŸ’¬ **Effective Communication**: Clear communication with stakeholders helps tie technical work to measurable business outcomes, driving adoption.
- ğŸŒ **Varied Stakeholder Needs**: Different stakeholdersâ€”C-suite, marketing, financeâ€”have distinct goals, requiring tailored data solutions.
- ğŸ—ï¸ **Building for Adoption**: Data engineers should think of their work as creating products that are user-friendly and impactful.
- ğŸ¯ **Practical Advice**: Learn to identify what motivates stakeholders, communicate effectively, and align data engineering efforts with their objectives.

## Requirements Gathering in Data Engineering: A Simulated Stakeholder Conversation

- ğŸ—ï¸ **Stakeholder Role**: The data scientist focuses on creating dashboards and training a recommendation model but is constrained by manual and delayed processes.
- ğŸ“Š **Marketingâ€™s Needs**: The marketing team wants real-time regional sales data for better campaign targeting and product trend analysis.
- ğŸ”„ **Current Challenges**: Manual data extraction and cleaning consume 80% of the data scientistâ€™s time, delaying actionable insights.
- ğŸ“… **Latency Expectations**: The marketing team seeks â€œreal-timeâ€ data, but the exact latency requirements (e.g., seconds, minutes, or hours) remain unclear.
- âš™ï¸ **Pain Points**: Data dumps are cumbersome, error-prone, and frequently change formats, requiring constant script adjustments.
- ğŸ“ˆ **Recommendation System**: A future goal is deploying a content-based recommendation engine requiring live user activity data and integration with the sales platform.
- ğŸ” **Refinement of Data**: Automating ingestion and transformation to provide clean, aggregated data in the desired format would streamline workflows.
- ğŸ¤ **Follow-Up Needs**: The data engineer plans to engage marketing stakeholders to refine understanding of their goals and latency requirements.
- ğŸ› ï¸ **Proposed Solutions**: Build systems for direct, automated ingestion and transformation while supporting personalized recommendations and dashboards.
- ğŸ“š **Educational Takeaway**: The exercise illustrates the importance of clear communication, identifying pain points, and tying technical solutions to stakeholder needs.

## Key Elements of Requirements Gathering in Data Engineering

- ğŸ—ï¸ **Existing Systems and Pain Points**: Understand current workflows and identify challenges, such as manual processes or data format inconsistencies.
- â±ï¸ **â€œReal-Timeâ€ Needs**: Clarify what stakeholders mean by â€œreal-timeâ€ to determine latency requirements, whether itâ€™s seconds, minutes, or hours.
- ğŸ“Š **Functional Requirements**: Define what the system needs to do, such as automating ingestion, transformation, and serving data in specific formats.
- ğŸ¯ **Non-Functional Requirements**: Determine system performance metrics, such as latency, reliability, and scalability, based on stakeholder needs.
- ğŸ” **Stakeholder Actions**: Ask what actions stakeholders plan to take with the data, rather than relying on their interpretation of technical requirements.
- ğŸ¤ **Confirming Understanding**: Summarize findings with stakeholders to ensure alignment and prevent misunderstandings.
- ğŸ› ï¸ **Engaging Additional Stakeholders**: Identify others, like software engineers or marketing teams, to gather comprehensive system requirements.
- âš™ï¸ **Data Schema Stability**: Plan for schema changes by building checks and working closely with source system owners for advance notice.
- ğŸ“š **Iterative Process**: Requirements gathering is ongoing and evolves as more information is uncovered through conversations with stakeholders.
- ğŸš€ **Preparation for Design**: Once requirements are clear, the next step is choosing tools and technologies to implement the system effectively.

## A Framework for Thinking Like a Data Engineer: From Requirements to Deployment

- ğŸ¢ **Business Goals and Stakeholder Needs**: Start by understanding the organizationâ€™s goals and how stakeholder needs align with them. Conversations are key to uncovering pain points and desired outcomes.
- ğŸ¯ **Functional and Non-Functional Requirements**: Define what the system must do (functional) and how it will perform (non-functional). Confirm these requirements with stakeholders to ensure alignment.
- ğŸ› ï¸ **Tool and Technology Selection**: Evaluate tools and technologies based on requirements, costs, scalability, and ease of maintenance. Conduct cost-benefit analyses to choose the best components.
- ğŸ”„ **Prototype and Test**: Build a prototype to validate the systemâ€™s design and assess whether it meets stakeholder needs. Iterate as needed before full-scale deployment.
- ğŸš€ **Build and Deploy**: After testing, build the full system and deploy it. Continuously monitor, evaluate, and improve the system to adapt to changing requirements.
- ğŸ” **Iterative Process**: The framework is not linear; itâ€™s a cyclical process that evolves as stakeholder needs and business goals change.
- ğŸ¤ **Collaboration and Communication**: Throughout the process, maintain open communication with stakeholders to ensure the system aligns with their needs.
- âš™ï¸ **Scalability and Evolution**: Design systems with flexibility to incorporate new tools, technologies, and requirements over time.
- ğŸ“š **Holistic Learning**: Practical application of this framework, as explored in future lessons, ties abstract concepts to real-world data engineering practices.
- ğŸŒ **Focus on the Cloud**: The framework integrates seamlessly with modern cloud-based tools, offering scalability and efficiency for data engineering projects.

# Data Engineering on the Cloud

## Introduction to Cloud-Based Data Engineering with AWS

- â˜ï¸ **Cloud-First Focus**: The course emphasizes cloud-based tools and technologies, as they dominate modern data engineering practices.
- ğŸ¢ **On-Premises Systems**: While some companies still maintain on-premises systems due to regulatory or legacy constraints, cloud migration is increasingly common.
- ğŸŒ **Public Cloud Providers**: AWS remains the most widely used platform, but learners may encounter Google Cloud, Microsoft Azure, or other providers in the workplace.
- ğŸ› ï¸ **AWS Tools and Resources**: The course partners with AWS to provide hands-on training with tools widely used by companies worldwide.
- ğŸ“š **Just-in-Time Learning**: AWS tools are introduced as they become relevant in lab exercises, making the course accessible even to beginners.
- ğŸ¯ **Outcome-Oriented Learning**: By the end of the course, learners will be able to design and implement a cloud-based data pipeline to meet specific requirements.
- ğŸ“– **No Prerequisites Required**: While familiarity with cloud concepts is helpful, the course is structured to teach foundational knowledge along the way.
- âš™ï¸ **Practical Application**: The focus is on building pipelines and architectures that reflect real-world data engineering needs.
- ğŸ—ï¸ **AWS Dominance**: AWSâ€™s expansive ecosystem of tools makes it a central player in modern cloud-based data engineering.
- ğŸ¤ **Industry Partnership**: Collaborating with AWS ensures that learners gain experience with tools actively used by businesses.

## Introduction to AWS Cloud Computing for Data Engineers

- ğŸ‘©â€ğŸ« **Morgan Willis**: Morgan brings experience as a software developer, bootcamp instructor, and AWS trainer, making her a knowledgeable guide for learners.
- ğŸŒ **AWS Basics**: The course introduces key concepts like regions, availability zones, VPCs, subnets, compute, databases, storage, and security.
- ğŸ› ï¸ **Hands-On Labs**: Learners will practice building data engineering solutions on AWS, gaining practical experience with industry-standard tools.
- ğŸ“ **AWS Certification**: The AWS Certified Data Engineer Associate exam validates skills in data ingestion, transformation, pipeline orchestration, data modeling, lifecycle management, and data quality.
- ğŸ“š **Learning Resources**: Additional resources, such as AWS Cloud Practitioner and Cloud Technical Essentials courses, are recommended for in-depth learning.
- ğŸ¯ **Focus on Practicality**: The course takes a just-in-time approach, ensuring learners acquire the necessary technical knowledge to complete labs successfully.
- ğŸ—ï¸ **Cloud-First Approach**: Learners will focus on designing scalable, cloud-based data pipelines aligned with modern data engineering practices.
- ğŸ“– **No Prerequisites Needed**: The course is beginner-friendly, introducing concepts and tools progressively to support learners at all levels.
- ğŸ” **Certification Pathway**: Topics covered in the course align with the AWS certification exam, offering a pathway to professional validation.
- ğŸš€ **Next Steps**: The following lessons will cover the basics of cloud computing on AWS, providing a strong foundation for data engineering projects.

## Understanding AWS Cloud Infrastructure and Services

- â˜ï¸ **Cloud as On-Demand Service**: AWS delivers IT resources over the Internet with pay-as-you-go pricing, eliminating the need for upfront hardware investments.
- ğŸ–¥ï¸ **Core Resources**: AWS offers compute (e.g., EC2, Lambda), storage (e.g., S3, Elastic Block Store), and networking (e.g., VPC) services.
- ğŸ“‚ **Storage Scalability**: Services like S3 automatically scale as data grows or shrinks, without requiring manual intervention.
- ğŸ“¡ **Global Infrastructure**: AWS operates data centers worldwide, grouped into regions and availability zones (AZs) for reliability and resilience.
- ğŸŒ **Regions and AZs**: Regions are geographic areas containing multiple AZs, which are smaller groupings of data centers designed to provide redundancy.
- ğŸ”„ **Elasticity and Scalability**: AWS resources automatically adjust to handle spikes in demand, reducing costs and improving efficiency.
- âš¡ **Utility-Like Model**: AWS functions like electricity usageâ€”you pay only for what you use, and infrastructure management is abstracted away.
- ğŸ“¡ **Low-Latency Connectivity**: Data centers and AZs are connected via AWSâ€™s global network of fiber cables, ensuring high-speed communication.
- ğŸ› ï¸ **Building Block Approach**: AWSâ€™s 200+ services allow engineers to combine tools to build tailored solutions for specific use cases.
- ğŸ“ **Focus for Data Engineers**: As data engineers, understanding these concepts is critical for designing scalable, resilient data systems.

## AWS Regions and Availability Zones

### AWS Regions

AWS hosts its data centers across many geographical areas known as regions. At the time of the creation of this course, AWS has 34 launched regions around the world. Each dot on this map represents a region, which contains a cluster of data centers spread across the geographical area.

- [AWS global-infrastructure](https://aws.amazon.com/about-aws/global-infrastructure/)
- [Regions and Availability Zones](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/)

Each AWS region is assigned a geographical name and a region code. The geographical name reflects the region's location. For example, in the United States, there is a region in Northern Virginia called the Northern Virginia Region (N. Virginia). This region has the code us-east-1 meaning it was the first one created in the eastern US.

AWS regions are independent from one another, meaning your data is not replicated from one region to another without your authorization.

To host your applications or data pipelines, you need to choose an AWS region. Consider these four main factors:

- **Latency**: choose a region close to where your end users are located to minimize latency;
- **Cost**: the resource costs may differ between regions;
- **Compliance**: certain regulations may require hosting your data in a specific geographic region;
- **Service availability**: not all services are available in all regions.

### Availability Zones (AZs)

In each region, data centers are spread out into availability zones. Each region contains at least 3 isolated and physically separated availability zones. From Amazon's documentation about their global infrastructure: â€œAZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each otherâ€<sup>(1)</sup>.

Each availability zone contains a group of one or more discrete data centers with redundant power, networking, and physical security. All AZs in an AWS Region are interconnected with high-speed low-latency links.

![aws-region](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/aws-region.png)

AZs are assigned a code name. The code name consists of the region code followed by a letter. For example, _us-east-1a_ represents an availability zone in us-east-1 (Northern Virginia Region).

The main purpose of having more than one availability zone within a region is to allow you to host your applications and data resources in multiple AZs for high availability and fault-tolerance. If one availability zone becomes unavailable due to power outage or natural disaster, your work will not be impacted.

## Introduction to Core AWS Services for Data Engineering

- ğŸ–¥ï¸ **Compute (EC2)**: Amazon EC2 provides virtual machines (instances) that users control entirely, allowing them to run applications, machine learning models, web servers, and more.
- ğŸŒ **Networking (VPC)**: Virtual Private Cloud (VPC) enables users to create isolated private networks in AWS, partitioned into subnets and bound to specific regions.
- ğŸ“‚ **Storage Options**:
  - **Object Storage (S3)**: Ideal for unstructured data like documents, logs, photos, and videos.
  - **Block Storage (EBS)**: Used for low-latency applications like virtual machine file systems and databases.
  - **File Storage (EFS)**: Provides hierarchical file systems, familiar to end users.
- ğŸ—„ï¸ **Databases**:
  - **Amazon RDS**: A managed relational database service for structured, tabular data.
  - **Amazon Redshift**: A data warehouse for transforming and serving data for analytics and machine learning.
- ğŸ” **Security (Shared Responsibility Model)**:
  - AWS secures infrastructure, including data centers and hypervisors.
  - Users are responsible for securing operating systems, applications, and data within the cloud.
- ğŸ”„ **Elasticity and Scalability**: AWS resources scale up or down automatically, ensuring cost efficiency and adaptability to demand.
- ğŸ—ï¸ **Building Block Approach**: AWS services can be combined to create customized solutions tailored to specific use cases.
- ğŸŒ **Global Reach**: AWS resources are hosted in regions and availability zones (AZs) worldwide, ensuring resilience and low latency.
- ğŸ“ **Hands-On Learning**: Upcoming labs will provide learners with guided exposure to these services.
- ğŸ“š **Optional Account Creation**: Learners can create free AWS accounts to explore independently, but itâ€™s not required for course success.

## Compute - Amazon Elastic Compute Cloud (EC2)

One of the basic services provided on the cloud is compute service, which means that AWS provides you with the compute resources needed to run your applications. An example of a compute service is Amazon EC2 (Elastic Compute Cloud), which represents a virtual server or virtual machine (the two terms are used interchangeably).

### What is a server? How is a virtual server different from a regular server?

A server is like a computer or a set of computers that hosts and runs your applications. It consists of physical hardware (CPU, RAM, storage, networking components), an operating system installed on top of the hardware, and finally the applications that run on top of the operating system.

When you run your application on the cloud, your application doesn't interact directly with the actual hardware. Instead, it interacts with virtual hardware, which is a software representation of the actual hardware that can emulate its behavior. So on top of the virtual hardware, an operating system can be installed to run your application. The virtual hardware, operating system and application are known as the components of a virtual machine or virtual server (a software representation or emulation of an actual server).

The benefit of this virtualization or abstraction is that you can create more than one virtual machine that shares the same underlying physical resources. This helps achieve efficient and cost-effective use of resources. The sharing of these resources is done through a software component called the hypervisor, which enables the sharing of the underlying hardware. The hypervisor distributes the underlying physical computing resources, such as CPU and memory, to individual virtual machines as required.

![server](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/server.png)

### Amazon EC2

In AWS, these virtual machines or virtual servers are called Amazon Elastic Compute Cloud or Amazon EC2. EC2 is one of the primary building blocks that you may directly use to run your applications or indirectly use by interacting with other services built on top of EC2 instances.

â€œElasticâ€ in EC2 means that you can acquire the necessary compute and memory resources that you need for your work. When you run your applications on EC2 instances, you can configure as many instances as you need, and you only pay for what you use. When you no longer need an instance, you can stop or terminate it. You can also pick the size of an EC2 instance, where size corresponds to the amount of compute, memory, and network capabilities for a given instance. Itâ€™s easy to resize based on your needs.

EC2 instances are grouped into several types, such as general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing, which you can choose based on your use case.

AWS uses a specific naming convention for the instance types. For example, t3a.micro breaks down as follows:

- **t**: family name
- **3**: generation
- **a**: optional capabilities
- **micro**: size

### References

1. [Hardware virtualization](https://www.techtarget.com/searchitoperations/definition/hardware-virtualization)
2. [Virtual servers vs physical servers](https://www.techtarget.com/searchitoperations/tip/Virtual-servers-vs-physical-servers-What-are-the-differences)
3. [Physical servers vs. Virtual machines](https://www.veeam.com/blog/why-virtual-machine-backups-different.html)
4. [What is hypervisor?](https://aws.amazon.com/what-is/hypervisor/)
5. [What is virtualization?](https://aws.amazon.com/what-is/virtualization/)
6. [Traditional virtualization primer](https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/traditional-virtualization-primer.html)
7. [What is amazon EC2?](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)
8. [Amazon EC2 instance types](https://aws.amazon.com/ec2/instance-types/)
9. [Instance type naming conventions](https://docs.aws.amazon.com/ec2/latest/instancetypes/instance-type-names.html)
10. [EC2 instance purchasing options](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html)

## Networking - Virtual Private Cloud (VPC) & Subnets

Networking is another building block for hosting your work on the cloud.

### What is a network?

A network is simply a collection of devices connected together, where each connection can be a request sent from one device to another or a response to a request. When you create and use resources on AWS, you want these resources to communicate with each other and possibly with the outside internet. Enabling the communication between resources and with the outside world requires an understanding of some basic cloud networking concepts. This includes understanding what an IP address is and how to create a network for your resources on AWS using VPC (Virtual Private Cloud) and subnets.

### What is an IP address?

In a given network, each device is assigned an IP (Internet Protocol) address, which is a series of digits that uniquely identifies each device within the network. These addresses ensure that responses and requests are sent to the correct device.

There are many types of IP addresses. IPv4 is the most widely integrated version of the IP address system. An IPv4 address is a 32-bit integer expressed in the form x.x.x.x, where each x is an 8-bit number that can take a value between 0 and 255. For example, 192.101.0.2 is a valid IPv4 address.

Another related term you will encounter when working on the cloud is CIDR (Classless Inter-Domain Routing) notation. A CIDR notation represents a range of IP addresses that could be assigned to devices within a particular network. CIDR is used to provision the required number of IP addresses for a particular network and reduce wastage of IP addresses. The following is an example of CIDR notation:

192.101.0.0/24

This notation means that the first 24 bits are fixed and the last 8 bits can be any bits. In other words, 192.101.0.0/24 represents all IP addresses between 192.101.0.0 and 192.101.0.255.

### What is a VPC?

A VPC (Virtual Private Cloud) is an isolated private network where you can launch your AWS resources. A VPC exists inside a region, which can contain more than one VPC, and a VPC spans multiple availability zones inside the region. VPC is a way to isolate your resources (for example EC2) from the outside world. Think of it as a box or a wall that protects and organizes your resources. Resources within the same VPC can communicate with each other. By default, thereâ€™s no communication between resources from different VPCs or with the internet unless you allow it to happen by properly configuring the VPC.

When you create a VPC, you need to specify the range of IP addresses or the CIDR block for the network, which determines the size of the network. Each resource created inside the VPC will be assigned an IP address from the specified range. When you launch resources such as EC2, you need to make sure theyâ€™re launched inside a VPC.

### What is a subnet?

Inside your VPC, you may need some resources to be public and others to be private. You can achieve this by creating subnets within your VPC. Subnets provide you with more detailed control over access to your resources. Each VPC consists of subnets created inside availability zones. You can create a public subnet if you want to allow for outside traffic to access your resources, and you can create a private subnet if you donâ€™t want to allow for outside traffic to access your resources.

You can think of a subnet as a smaller network inside your base network. Each subnet is assigned a CIDR block, which must be a subset of the VPC CIDR block. Resources in multiple subnets of the same VPC can communicate because they are part of the same VPC.

![subnet](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/subnet.png)

### References

1. [What is IPv4?](https://www.whatismyip.com/ipv4/)
2. [What is my IP?](https://www.whatismyip.com/)
3. [What is CIDR?](https://aws.amazon.com/what-is/cidr/)
4. [What is Amazon VPC?](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html)

## Security - AWS Shared Responsibility Model

When you host your applications and resources in the cloud, youâ€™re offloading the heavy duty of managing the physical hardware to the cloud provider. The security of the physical facility is the responsibility of the cloud provider. However, you still own your data in AWS and you have complete control of it, so you are responsible for managing its security. This is known as the shared responsibility model on AWS.

- **AWS** is responsible for **security of the cloud**: this means AWS is responsible for maintaining, protecting, and securing the physical facilities that contain the compute, storage, and networking equipment. It is also responsible for protecting the global infrastructure -- the cables that connect the regions, as well as the software and hardware that run AWS services.
- **You** are responsible for **security in the cloud**: this means youâ€™re responsible for protecting the data, ensuring itâ€™s secured whether it is at rest or in transit, and managing who can access the data and for how long. Depending on the type of AWS services you choose, you might be responsible for additional configurations.

Itâ€™s essential to keep this principle in mind because itâ€™s your responsibility to manage who has access to the data youâ€™re storing in cloud storage systems, and who has access to any pipeline youâ€™re designing on the cloud.

## References

1. [AWS shared responsibility model](https://aws.amazon.com/compliance/shared-responsibility-model/)
2. [Understanding the security scope](https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-accelerating-security-maturity/understanding-the-security-scope.html)

# Week 1 Quiz

## Questions

1. What are the key elements of any requirements gathering conversation as discussed in the videos? Select all that apply.
   1. Learn what action stakeholders plan to take based on the data you serve them.
   2. Identify any additional stakeholders you will need to talk to.
   3. Learn what problems or pain points there are with existing systems.
   4. Learn what existing data systems or solutions are already in place.
2. Which of the following best describes data engineering as defined in this course?
   1. Data engineering is about using tools and technologies to build data pipelines that move data through the stages of the data engineering lifecycle.
   2. Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning.
   3. Data engineering is focused on deriving insights from data that can add value for the business..
   4. Data engineering is about setting up storage systems to store raw data from different sources before serving it to its end-users.
3. What is the difference between a region and an availability zone on the AWS cloud?
   1. An availability zone spreads across multiple regions and each region contains one or more data centers.
   2. A data center consists of multiple regions and each region consists of multiple availability zones.
   3. A data center consists of multiple availability zones and an availability zone is spread across multiple regions.
   4. A region consists of multiple availability zones and an availability zone contains one or more data centers.
4. Which of the following statements are correct about virtual servers? Select all that apply.
   1. An Amazon Elastic Compute Cloud or EC2 instance is a virtual server in the AWS Cloud
   2. A virtual server is a software representation or emulation of an actual physical server.
   3. A virtual server is an actual physical server that is hosted on the cloud and to which you can connect to virtually through the internet.
   4. Multiple virtual servers can share the same underlying physical resources, which helps achieve efficient and cost-effective use of resources.
5. Whatâ€™s the difference between business requirements, stakeholder requirements, and system requirements as described in this weekâ€™s videos?
   1. There are functional and nonfunctional business, stakeholder, and system requirements.
   2. Business requirements define the high level goals of the business. Stakeholder requirements define the needs of the individuals within the organization to meet those business goals. System requirements define what a system needs to be able to do in order to meet business and stakeholder requirements.
   3. Business and stakeholder requirements are the same thing, you gather both of these in the first stage of any data engineering project. System requirements are what you define after collecting high level requirements.
   4. Business requirements are defined by the company leadership team, stakeholder requirements are defined by individual contributors within the organization, and system requirements are defined by the data engineer.
6. What are the key steps you should take in the first stage of any data engineering project as presented in the â€œframework for thinking like a data engineerâ€? Select all that apply.
   1. Explore existing systems and stakeholder needs.
   2. Identify the business goals and stakeholders you will serve.
   3. Translate stakeholder needs into functional requirements for your system.
   4. Perform a cost-benefit analysis to choose between comparable tools and technologies.
   5. Ask what action stakeholders plan to take based on the data you serve them.
7. Which of the following statements are true about VPC (Virtual Private Cloud)? Select all that apply.
   1. A VPC exists within one subnet and can belong to a single availability zone.
   2. A VPC is an isolated private network in which you can launch your AWS resources.
   3. A VPC can only contain private subnets.
   4. A VPC exists inside a region and can span multiple availability zones inside the region.
8. This week you were introduced to the diagram of the data engineering lifecycle, depicted below. ![data-engineering-lifecycle](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/data-engineering-lifecycle.png) Which of the following statements are true about the diagram of the data engineering lifecycle? Select all that apply.
   1. Storage is part of all stages of the data engineering lifecycle.
   2. Analytics, machine learning, and reverse ETL represent the final stage of the data engineering lifecycle.
   3. The undercurrents are not stages of the lifecycle but rather represent aspects of data engineering that cut across the entire lifecycle.
   4. The middle gray box (around ingestion, transformation, serving and storage) represents the stages that will be the focus of your work as a data engineer.
9. Consider the following scenario: You work as a data engineer at a financial institution. You were tasked with building a pipeline to serve data for the data science team of the â€œclients servicesâ€ department. You learned that the data scientists are interested in predicting customer churn, or in other words, being able to predict whether a given customer is likely to stop doing business with the bank. You met with the data scientists who told you that theyâ€™d like you to provide them with historical information about active and inactive customers, such as their past transactions, credit information, and account information. The data scientists will then analyze this data and build a churn prediction model. You learned that the information requested by the data scientists exists in several databases maintained by software engineers from different bank departments. Identify the upstream and downstream stakeholders.
   1. The downstream stakeholders are the software engineers that maintain the source databases and the upstream stakeholders are the data scientists of the â€œclient servicesâ€ department.
   2. The downstream stakeholders are both the data scientists and the software engineers that maintain the source databases. The upstream stakeholders are the bank clients.
   3. The upstream stakeholders are the software engineers that maintain the source databases and the downstream stakeholders are the data scientists of the â€œclient servicesâ€ department.
10. Which of the following statements best describes the AWS â€œshared responsibility modelâ€?
    1. AWS is responsible for security in the cloud and you are responsible for the security of the cloud.
    2. AWS is responsible for security behind the scenes in cloud systems and you are responsible for the security of any public facing components of your systems.
    3. AWS is responsible for the security of the cloud and you are responsible for security in the cloud.
    4. Depending on the resources and services you choose to build your systems, AWS will be responsible for the security of some of them and you will be responsible for the security of others.

## Answers

1. 1, 2, 3 & 4
   1. Indeed, this is critical because stakeholders may describe their needs using phrases like â€œreal-timeâ€ but only when you understand the action they plan to take can you start to translate their needs into actual system requirements. This was described as one of the key elements of requirements gathering in the [â€œTranslate Stakeholder Needs into Specific Requirementsâ€](https://www.coursera.org/learn/intro-to-data-engineering/lecture/vWYP2/translate-stakeholder-needs-into-specific-requirements) video.
   2. Thatâ€™s right. Oftentimes stakeholder conversations will lead you to discover there are others you need to talk to as well. This was described as one of the key elements of requirements gathering in the [â€œTranslate Stakeholder Needs into Specific Requirementsâ€](https://www.coursera.org/learn/intro-to-data-engineering/lecture/vWYP2/translate-stakeholder-needs-into-specific-requirements) video
   3. Yes, this is important because your stakeholderâ€™s pain points with any existing systems will be important to consider when designing a new system. This was described as one of the key elements of requirements gathering in the [â€œTranslate Stakeholder Needs into Specific Requirementsâ€](https://www.coursera.org/learn/intro-to-data-engineering/lecture/vWYP2/translate-stakeholder-needs-into-specific-requirements) video
   4. Thatâ€™s right, itâ€™s important to first understand any existing systems or solutions before starting your work to modify or replace them. This was described as one of the key elements of requirements gathering in the [â€œTranslate Stakeholder Needs into Specific Requirementsâ€](https://www.coursera.org/learn/intro-to-data-engineering/lecture/vWYP2/translate-stakeholder-needs-into-specific-requirements) video
2. 2
   1. The complete definition as stated in this course and in the book also included a second sentence mentioning the undercurrents of the data engineering lifecycle: â€œData engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering.â€
3. 4
   1. Each AWS region contains at least 3 isolated and physically separated availability zones. Each availability zone contains a group of one or more discrete data centers with redundant power, networking and physical security.
4. 1, 2 & 4
   1. The benefit of this virtualization is that you can create more than one virtual machine that shares the same underlying physical resources.
5. 3
6. 1, 2 & 5
   1. In the first stage of any data engineering project, you should have conversations with individual stakeholders to learn what systems are currently in place and what needs stakeholders have beyond what the current systems are providing. This will inform the requirements for the system you need to build.
   2. Before starting any data engineering project itâ€™s important to understand the goals of the business and know who the stakeholders are that you will serve. This will inform the requirements for the system you need to build.
   3. Before designing and building any systems, you should understand what action stakeholders plan to take, as this will inform the requirements for your system.
7. 2 & 4
   1. A VPC exists inside a region, which can contain more than one VPC, and a VPC spans multiple availability zones inside the region. VPC is a way to isolate your resources (for example EC2) from the outside world.
8. 1, 3 & 4
   1. The undercurrents represent aspects of data engineering that will be relevant no matter which stage youâ€™re working in.
   2. These are the stages that are the focus of your work as a data engineer.
9. 3
   1. Upstream stakeholders represent the stakeholders that create and/or maintain the source systems you will ingest data from. Downstream stakeholders are the end-users to whom youâ€™re serving the data.
10. 3
    1. The AWS shared responsibility model states that the security of the infrastructure and services that make up the cloud is AWSâ€™s responsibility and the security of the systems you build on the cloud is your responsibility.
