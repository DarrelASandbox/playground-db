- [Data Engineering Life Cycle](#data-engineering-life-cycle)
  - [Data Generation and Source Systems](#data-generation-and-source-systems)
    - [Overview of Data Generation](#overview-of-data-generation)
    - [Common Source Systems](#common-source-systems)
      - [Databases](#databases)
      - [Files](#files)
      - [APIs](#apis)
      - [Data Sharing Platforms](#data-sharing-platforms)
      - [IoT Devices](#iot-devices)
    - [Challenges with Source Systems](#challenges-with-source-systems)
    - [Collaboration with Source System Owners](#collaboration-with-source-system-owners)
  - [Ingestion](#ingestion)
    - [Overview of Data Ingestion](#overview-of-data-ingestion)
    - [Ingestion Frequency: Batch vs. Streaming](#ingestion-frequency-batch-vs-streaming)
      - [Batch Ingestion](#batch-ingestion)
      - [Streaming Ingestion](#streaming-ingestion)
    - [Trade-offs Between Batch and Streaming](#trade-offs-between-batch-and-streaming)
    - [Additional Ingestion Considerations](#additional-ingestion-considerations)
  - [Storage](#storage)
    - [Everyday Interactions with Data Storage](#everyday-interactions-with-data-storage)
    - [Physical Components of Storage](#physical-components-of-storage)
      - [Magnetic Disks vs. Solid State Drives (SSD)](#magnetic-disks-vs-solid-state-drives-ssd)
      - [Random Access Memory (RAM)](#random-access-memory-ram)
      - [Storage Hierarchy in Modern Architectures](#storage-hierarchy-in-modern-architectures)
    - [Data Storage Systems in Data Engineering](#data-storage-systems-in-data-engineering)
      - [Common Storage Systems](#common-storage-systems)
      - [Storage Abstractions](#storage-abstractions)
    - [Importance of Understanding Storage Systems](#importance-of-understanding-storage-systems)
  - [Queries, Modeling, and Transformation](#queries-modeling-and-transformation)
    - [Importance of Transformation](#importance-of-transformation)
    - [Key Components of Transformation](#key-components-of-transformation)
      - [Queries](#queries)
      - [Data Modeling](#data-modeling)
      - [Data Transformation](#data-transformation)
    - [Practical Applications](#practical-applications)
  - [Serving Data](#serving-data)
    - [Overview of Serving Data](#overview-of-serving-data)
    - [Key Use Cases for Serving Data](#key-use-cases-for-serving-data)
      - [Analytics](#analytics)
      - [Machine Learning](#machine-learning)
      - [Reverse ETL](#reverse-etl)
- [The Undercurrents of the Data Engineering Lifecycle](#the-undercurrents-of-the-data-engineering-lifecycle)
  - [Security](#security)
    - [Introduction to Data Security](#introduction-to-data-security)
    - [Foundational Security Principles](#foundational-security-principles)
      - [Principle of Least Privilege](#principle-of-least-privilege)
      - [Data Sensitivity](#data-sensitivity)
    - [Security in the Cloud Era](#security-in-the-cloud-era)
    - [The Human Factor in Security](#the-human-factor-in-security)
      - [Defensive Mindset](#defensive-mindset)
      - [Organizational Security Culture](#organizational-security-culture)
  - [Data Management](#data-management)
    - [Introduction to Data Management](#introduction-to-data-management)
    - [Key Aspects of Data Management](#key-aspects-of-data-management)
      - [Definition of Data Management](#definition-of-data-management)
      - [Data Knowledge Areas](#data-knowledge-areas)
    - [Focus on Data Governance](#focus-on-data-governance)
      - [Role of Data Governance](#role-of-data-governance)
      - [Data Quality](#data-quality)
  - [Data Architecture](#data-architecture)
    - [Introduction to Data Architecture](#introduction-to-data-architecture)
    - [Key Principles of Data Architecture](#key-principles-of-data-architecture)
      - [Evolving Data Needs](#evolving-data-needs)
      - [Trade-Off Evaluation](#trade-off-evaluation)
    - [Principles of Good Data Architecture](#principles-of-good-data-architecture)
  - [DataOps](#dataops)
    - [Introduction to DataOps](#introduction-to-dataops)
    - [Cultural Practices in DataOps](#cultural-practices-in-dataops)
    - [Technical Pillars of DataOps](#technical-pillars-of-dataops)
      - [Automation](#automation)
      - [Observability and Monitoring](#observability-and-monitoring)
      - [Incident Response](#incident-response)
    - [DataOps Maturity in Organizations](#dataops-maturity-in-organizations)
  - [Orchestration](#orchestration)
    - [Introduction to Orchestration](#introduction-to-orchestration)
    - [Evolution of Data Pipeline Management](#evolution-of-data-pipeline-management)
      - [Manual Execution](#manual-execution)
      - [Pure Scheduling](#pure-scheduling)
      - [Modern Orchestration Frameworks](#modern-orchestration-frameworks)
    - [Directed Acyclic Graphs (DAGs)](#directed-acyclic-graphs-dags)
  - [Software Engineering](#software-engineering)
    - [Introduction to Software Engineering](#introduction-to-software-engineering)
    - [Evolution of Data Engineering](#evolution-of-data-engineering)
    - [Importance of Coding in Data Engineering](#importance-of-coding-in-data-engineering)
      - [Key Programming Languages and Frameworks](#key-programming-languages-and-frameworks)
      - [Code Quality](#code-quality)
    - [Applications of Software Engineering in Data Engineering](#applications-of-software-engineering-in-data-engineering)
      - [Data Processing](#data-processing)
      - [Open Source Contributions](#open-source-contributions)
      - [Everyday Problem-Solving](#everyday-problem-solving)
    - [Collaboration with Software Engineers](#collaboration-with-software-engineers)
- [Practical Examples on AWS](#practical-examples-on-aws)
  - [Data Engineering Lifecycle on AWS](#data-engineering-lifecycle-on-aws)
    - [Introduction to AWS Tools for Data Engineering](#introduction-to-aws-tools-for-data-engineering)
    - [Source Systems on AWS](#source-systems-on-aws)
      - [Relational Databases](#relational-databases)
      - [NoSQL Databases](#nosql-databases)
      - [Streaming Sources](#streaming-sources)
    - [Ingestion on AWS](#ingestion-on-aws)
      - [Database Ingestion](#database-ingestion)
      - [Streaming Ingestion](#streaming-ingestion-1)
    - [Storage on AWS](#storage-on-aws)
      - [Data Warehousing](#data-warehousing)
      - [Lakehouse Architecture](#lakehouse-architecture)
    - [Transformation on AWS](#transformation-on-aws)
    - [Serving Data on AWS](#serving-data-on-aws)
      - [Analytics and Business Intelligence](#analytics-and-business-intelligence)
      - [AI and Machine Learning](#ai-and-machine-learning)
  - [The Undercurrents on AWS](#the-undercurrents-on-aws)
    - [Security](#security-1)
    - [Data Management](#data-management-1)
    - [DataOps](#dataops-1)
    - [Orchestration](#orchestration-1)
    - [Architecture](#architecture)
    - [Software Engineering](#software-engineering-1)
- [Week 2 Quiz](#week-2-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Data Engineering Life Cycle

## Data Generation and Source Systems

### Overview of Data Generation

- Data engineers consume data from various sources, such as internal databases, APIs, and IoT devices.
- Source systems are typically maintained by other teams (e.g., software engineers, external vendors).
- Understanding these systems is crucial for building reliable data pipelines.

### Common Source Systems

#### Databases

- Relational databases (e.g., tables of related data) and NoSQL systems (e.g., key-value stores, document databases).
- Often part of web or mobile application backends.
- Data engineers need to understand database structures for effective pipeline development.

#### Files

- Data can come in the form of text, audio (e.g., MP3s), video, or other file types.
- Engineers often start their work by accessing or downloading these files.

#### APIs

- APIs allow data retrieval via web requests, returning data in formats like XML or JSON.
- Commonly used to fetch data from external platforms or services.

#### Data Sharing Platforms

- Organizations use these platforms to share data internally or with third parties.
- Data engineers may need to access datasets from these platforms for their pipelines.

#### IoT Devices

- IoT devices (e.g., GPS trackers) stream real-time data, often sent to databases or made accessible via APIs.
- Engineers may need to ingest and combine multiple data streams for downstream workflows.

### Challenges with Source Systems

- **Source systems can be unpredictable**: they may go down, change data formats, or alter schemas without notice.
- **Example**: A database column rearrangement by a software team halted downstream workflows, causing stakeholder dissatisfaction.
- Engineers must anticipate changes and work closely with source system owners to mitigate risks.

### Collaboration with Source System Owners

- Building strong relationships with source system stakeholders is critical.
- Engineers should understand how systems generate data, potential changes, and their impact on downstream processes.
- Proactive communication ensures smoother data pipeline operations and reduces disruptions.

## Ingestion

### Overview of Data Ingestion

- Data ingestion involves moving raw data from source systems into data pipelines for processing.
- It is often the most challenging phase of the data engineering life cycle.
- Collaboration with source system owners is critical to avoid common pitfalls.

### Ingestion Frequency: Batch vs. Streaming

- Data ingestion frequency is a key decision: **batch** or **streaming**.

#### Batch Ingestion

- Data is ingested in large chunks at set intervals (e.g., hourly, daily).
- Suitable for analytics and machine learning use cases.
- Example: Processing a full day’s data in one batch.
- Remains a practical and widely used approach.

#### Streaming Ingestion

- Data is ingested and processed in near real-time (e.g., less than one second after production).
- Requires tools like event streaming platforms or message queues.
- Ideal for use cases requiring immediate action (e.g., real-time anomaly detection).
- More complex and costly than batch ingestion.

### Trade-offs Between Batch and Streaming

- **Batch Ingestion**: Simpler, cost-effective, and suitable for periodic tasks (e.g., weekly reporting).
- **Streaming Ingestion**: Justified for real-time use cases but involves higher costs and maintenance.
- Most architectures combine both approaches (e.g., streaming for real-time alerts, batch for model training).

### Additional Ingestion Considerations

- **Change Data Capture (CDC)**: Triggers ingestion based on changes in source systems.
- **Push vs. Pull**: Deciding whether the source system pushes data or the pipeline pulls it.
- These nuances will be explored further in the specialization.

## Storage

### Everyday Interactions with Data Storage

- Daily activities like creating, deleting, or moving files on devices involve interacting with storage systems.
- Applications load into RAM for faster access, and files are often backed up to cloud storage.
- Smartphones also rely on storage systems for messaging, app usage, and cloud interactions.
- Common issues include running out of storage space or dealing with file size limitations.

### Physical Components of Storage

#### Magnetic Disks vs. Solid State Drives (SSD)

- Magnetic disks remain cost-effective and are widely used in modern storage systems.
- SSDs, while faster, are more expensive than magnetic disks.

#### Random Access Memory (RAM)

- RAM offers fast read/write speeds but is volatile and loses data when power is lost.
- RAM is significantly more expensive than SSDs or magnetic disks.

#### Storage Hierarchy in Modern Architectures

- Data typically moves through magnetic storage, SSDs, and RAM during processing.
- Networking, CPU, serialization, compression, and caching are critical for modern distributed storage systems.

### Data Storage Systems in Data Engineering

1. If we were to arrange these various aspects of data storage as a hierarchy, what should be the correct order of this hierarchy from the bottom to the top?
   1. Bottom layer: Storage abstractions, Middle layer: Raw storage ingredients & Top layer: Storage systems
   2. Bottom layer: Storage systems, Middle layer: Storage abstractions & Top layer: Raw storage ingredients
   3. Bottom layer: Raw storage ingredients, Middle layer: Storage systems & Top layer: Storage abstractions
   4. Bottom layer: Raw storage ingredients, Middle layer: Storage abstractions & Top layer: Storage systems
   5. Ans: 3
   6. The bottom layer of the hierarchy consists of the raw storage ingredients since all storage solutions are built on top of the physical ingredients like SSD and magnetic disks, along with processes like networking, serialization, and compression. The middle layer consists of storage systems like databases and object storage that are built on top of the raw ingredients. Then the top layer consists of the these storage systems arranged into storage abstractions like data warehouses, data lakes, and data lakehouses.

#### Common Storage Systems

- Database management systems (e.g., MySQL, PostgreSQL).
- Object storage platforms (e.g., Amazon S3).
- Advanced systems like Apache Iceberg, Hoodie, and streaming storage.

#### Storage Abstractions

- Data warehouses, data lakes, and data lakehouses combine multiple storage systems.
- These abstractions allow engineers to focus on high-level needs like latency, scalability, and cost without worrying about low-level details.

### Importance of Understanding Storage Systems

- Data engineers often operate at the top of the storage hierarchy but benefit from understanding underlying components.
- Lack of knowledge can lead to inefficiencies, such as high costs or poor performance.
- Example: A team wasted time and budget by using inefficient row-by-row inserts instead of bulk ingestion in a data warehouse.

## Queries, Modeling, and Transformation

### Importance of Transformation

- Transformation is the stage where raw data is turned into useful information for downstream stakeholders.
- Raw data ingestion and storage alone do not add value; transformation bridges the gap between raw data and actionable insights.
- Downstream users, such as business analysts and data scientists, rely on transformed data for reporting, analytics, and machine learning.

### Key Components of Transformation

#### Queries

- Queries involve reading data from databases or storage systems, often using SQL.
- Poorly written queries can lead to performance issues, row explosion, or delays in downstream processes.
- Understanding how queries work under the hood is crucial for optimizing data pipelines.

#### Data Modeling

- Data modeling involves structuring data to reflect real-world relationships and business needs.
- **Example**: Denormalizing normalized data from relational databases to simplify reporting for business analysts.
- Effective data modeling requires collaboration with stakeholders to align on terminology and business goals.

#### Data Transformation

- Data is transformed multiple times throughout the data engineering life cycle.
- Transformations can occur:
  - **Before ingestion**: Adding timestamps or other metadata.
  - **During ingestion**: Mapping data to correct types and standardizing formats.
  - **After ingestion**: Enriching records, denormalizing schemas, or aggregating data for reporting and machine learning.

### Practical Applications

- Business analysts benefit from transformed data for quick and efficient querying.
- Data scientists and machine learning engineers use transformed data for predictive analytics and model training.
- Hands-on exercises in the course will cover querying, modeling, and transforming data.

## Serving Data

### Overview of Serving Data

- **Serving data** is the final stage of the data engineering lifecycle, enabling stakeholders to extract **business value** from data.
- **Value** is derived from practical use cases such as **analytics**, **machine learning**, **reverse ETL**, and more.
- This stage focuses on making data accessible for actionable insights and operational decisions.

### Key Use Cases for Serving Data

#### Analytics

- **Analytics** involves identifying key insights and patterns within data.
- Common forms of analytics include:
  - **Business Intelligence (BI)**: Historical and current data exploration for strategic decision-making.
    - Data is presented in **reports** or **dashboards**.
    - **Example**: Sales and marketing teams use BI to monitor campaign engagement, regional sales, and customer experience metrics.
  - **Operational Analytics**: Real-time data monitoring for immediate action.
    - **Example**: E-commerce platforms monitor real-time website performance metrics to quickly address downtime.
  - **Embedded Analytics**: Customer-facing analytics integrated into applications.
    - **Example**: Bank dashboards showing spending trends or smart thermostat apps displaying historical temperature data.

#### Machine Learning

- **Machine learning** use cases often require serving data for:
  - **Feature stores**: Data used for model training.
  - **Real-time inference**: Serving data for immediate predictions.
  - **Metadata and cataloging systems**: Tracking data history and lineage.
- Machine learning introduces additional complexities, such as managing data for model training and inference.

#### Reverse ETL

- **Reverse ETL** involves feeding transformed data, analytics, or machine learning outputs back into source systems.
  - **Example**: Data from a CRM system is transformed, stored in a data warehouse, and used to train a lead scoring model. The results are then pushed back into the CRM.
- This process is increasingly common, though the term "reverse ETL" is more of a placeholder for a longstanding practice.

# The Undercurrents of the Data Engineering Lifecycle

## Security

### Introduction to Data Security

- **Personal data analogy**: Just as you protect personal information like bank accounts and passwords, data engineers must safeguard sensitive data entrusted to them.
- **Responsibility**: Data engineers are responsible for ensuring the security of client and business data by applying principles, protocols, and cultural practices.

### Foundational Security Principles

#### Principle of Least Privilege

- **Definition**: Grant users and applications access only to the data and resources they need, and only for the required duration.
- **Application**: Apply this principle to both team members and yourself, avoiding unnecessary use of administrator or superuser permissions.

#### Data Sensitivity

- **Visibility**: Limit access to sensitive data to only those who absolutely need it.
- **Ingestion**: Avoid ingesting sensitive data unless there is a clear purpose, reducing the risk of accidental leaks.

### Security in the Cloud Era

- **Key Concepts**: Understand **identity and access management (IAM)**, encryption methods, and networking protocols.
- **Integration**: Security considerations will be explored in-depth throughout the specialization, focusing on securing data pipelines.

### The Human Factor in Security

#### Defensive Mindset

- **Approach**: Always be cautious when handling credentials or sensitive data, and design systems with potential attack scenarios in mind.
- **Common Pitfalls**: Many security breaches result from human errors, such as sharing passwords, falling for phishing attacks, or misconfiguring systems (e.g., exposing AWS S3 buckets to the public internet).

#### Organizational Security Culture

- **Security Theater**: Avoid superficial compliance with security regulations without fostering a genuine culture of security.
- **Culture of Security**: Security should be a shared responsibility, with every team member prioritizing and habitually practicing it.

## Data Management

### Introduction to Data Management

- **Importance**: Proper data management is critical for transforming data into a valuable business asset.
- **DAMA International**: The **Data Management Association International (DAMA)** provides resources, including the **Data Management Book of Knowledge (DMBOK)**, to guide effective data management practices.

### Key Aspects of Data Management

#### Definition of Data Management

- **DMBOK Definition**: Data management involves the development, execution, and supervision of plans, programs, and practices that deliver, control, protect, and enhance the value of data throughout its lifecycle.
- **Shared Responsibility**: Data engineers collaborate with teams like software engineering and IT to manage data effectively.

#### Data Knowledge Areas

- **DMBOK Framework**: Data management is divided into 11 knowledge areas, including **data governance**, **data modeling**, **data integration**, **metadata**, and **security**.
- **Interconnectedness**: **Data governance** is central, influencing and interacting with all other areas of data management.

### Focus on Data Governance

#### Role of Data Governance

- **Core Function**: Ensures the **quality**, **integrity**, **security**, and **usability** of organizational data.
- **Scope**: Covers data security, privacy, quality, and usability, making it a foundational aspect of data management.

#### Data Quality

- **Definition**: High-quality data is **accurate**, **complete**, **discoverable**, and **timely**, with well-defined schemas and data definitions.
- **Impact**:
  - **High-quality data**: Enhances decision-making and adds value to the organization.
  - **Low-quality data**: Leads to wasted time, poor decisions, and potential organizational consequences.

## Data Architecture

### Introduction to Data Architecture

- **Definition**: Data architecture is the **blueprint** for designing systems that support an organization's evolving data needs.
- **Role of Data Engineers**: While some organizations have dedicated **data architects**, data engineers benefit from thinking like architects, especially in smaller teams or startups.

### Key Principles of Data Architecture

#### Evolving Data Needs

- **Flexibility**: A good architecture supports both current and future data needs, requiring ongoing adjustments rather than a one-time design.
- **Reversible Decisions**: Design choices should be flexible and reversible to adapt to unforeseen changes in organizational needs.

#### Trade-Off Evaluation

- **Balancing Act**: Architects must evaluate trade-offs in **performance**, **cost**, **scalability**, and other factors to create effective systems.
- **Cloud Advantage**: Unlike on-premises systems, cloud-based architectures allow for more flexible and reversible decisions.

### Principles of Good Data Architecture

1. **Choose Common Components Wisely**: Select components used across teams to facilitate collaboration and meet project-specific needs.
2. **Plan for Failure**: Design systems to handle both normal operations and unexpected failures.
3. **Architect for Scalability**: Build systems that can scale up or down based on demand, optimizing for cost and performance.
4. **Architecture is Leadership**: Thinking like an architect helps in mentoring others and potentially advancing to a data architect role.
5. **Always Be Architecting**: Architecture is an ongoing process, requiring continuous evaluation and re-architecting as organizational needs evolve.
6. **Build Loosely Coupled Systems**: Use interchangeable components to make systems adaptable and reversible.
7. **Make Reversible Decisions**: Ensure design choices can be easily reversed or updated as needs change.
8. **Prioritize Security**: Incorporate security principles like **least privilege** and **zero trust** into architectural decisions.
9. **Embrace FinOps**: Combine financial and operational priorities to optimize costs and revenue in cloud-based systems.

## DataOps

### Introduction to DataOps

- **Origin**: Inspired by **DevOps**, DataOps emerged to improve the development and quality of **data products** by breaking silos and fostering collaboration.
- **Core Focus**: Combines **cultural practices** and **technical elements** to enhance data pipeline efficiency, quality, and reliability.

### Cultural Practices in DataOps

- **Collaboration**: Prioritize communication with stakeholders across the organization.
- **Continuous Learning**: Learn from successes and failures to iteratively improve systems.
- **Agile Methodology**: Adopt rapid iteration and incremental delivery, borrowed from **DevOps** and **Agile**.

### Technical Pillars of DataOps

#### Automation

- **CI/CD in DataOps**: Borrows from **Continuous Integration/Continuous Delivery (CI/CD)** in DevOps to automate data pipeline tasks like ingestion, transformation, and deployment.
- **Orchestration Frameworks**: Tools like **Airflow** automate task execution, manage dependencies, and notify errors, reducing manual intervention and errors.
- **Change Management**: Automates changes in code, configuration, and data processing pipelines.

#### Observability and Monitoring

- **Importance**: Data pipelines will inevitably fail; observability ensures issues are detected early.
- **Consequences of Failure**: Undetected failures can lead to **bad data**, poor decisions, and loss of stakeholder trust.
- **Proactive Monitoring**: Essential to identify and resolve issues before they impact downstream users.

#### Incident Response

- **Root Cause Analysis**: Rapidly identify and resolve issues using monitoring tools.
- **Communication**: Foster open, blameless communication and coordination among team members during incidents.
- **Proactive Approach**: Data engineers should identify and address issues before stakeholders report them.

### DataOps Maturity in Organizations

- **Varied Adoption**: Some organizations have mature DataOps practices, while others are still in the early stages.
- **Role of Data Engineers**: Whether in mature or nascent DataOps environments, engineers play a key role in implementing and improving DataOps practices.

## Orchestration

### Introduction to Orchestration

- **Analogy**: Orchestration in data pipelines is like a conductor guiding an orchestra, coordinating tasks to achieve a seamless flow of data.
- **Role of Data Engineers**: Data engineers act as conductors, managing and coordinating tasks in data pipelines to ensure efficient and reliable data processing.

### Evolution of Data Pipeline Management

- **Spanning the Lifecycle**: Orchestration is integral to the **data engineering lifecycle** and **DataOps**, ensuring efficient, reliable, and automated data processing.

#### Manual Execution

- **Initial Approach**: In early stages or small startups, data engineers may manually execute each task in a pipeline.
- **Use Case**: Useful for prototyping but unsustainable for long-term operations due to inefficiency and error-proneness.

#### Pure Scheduling

- **Definition**: Tasks are scheduled to run automatically at specific times or frequencies.
- **Limitations**:
  - **Failure Propagation**: If an upstream task fails, downstream tasks may also fail.
  - **Timing Issues**: Tasks may start before previous ones complete, leading to incomplete or stale data.

#### Modern Orchestration Frameworks

- **Open Source Tools**: Frameworks like **Apache Airflow**, **Dagster**, **Prefect**, and **Mage** enable sophisticated orchestration for teams of all sizes.
- **Key Features**:
  - **Dependency Management**: Tasks are triggered based on the completion of previous tasks, not just time.
  - **Event-Driven Triggers**: Tasks can start based on events, such as new data availability.
  - **Monitoring and Alerts**: Built-in monitoring ensures failures are detected and addressed promptly.

### Directed Acyclic Graphs (DAGs)

- **Definition**: A **DAG** is a flow chart representing how data moves through a pipeline, with tasks as nodes and dependencies as edges.
- **Characteristics**:
  - **Directed**: Data flows in one direction.
  - **Acyclic**: No loops or backward flows.
- **Example**:
  - **Ingestion**: Data is extracted from multiple sources.
  - **Transformation**: Data is transformed in-flight or after storage.
  - **Serving**: Data is stored and served for analytics or machine learning use cases.

## Software Engineering

### Introduction to Software Engineering

- **Core Skill**: As a data engineer, you must be proficient in **reading and writing code**.
- **Goal**: Write **production-grade code** that is clean, readable, testable, and deployable.

### Evolution of Data Engineering

- **Historical Context**: Data engineering emerged from **software engineering**, where engineers occasionally handled data.
- **Modern Data Engineering**: With the growth of data volume and complexity, data engineering became a distinct field, leveraging **managed services** and tools to streamline workflows.

### Importance of Coding in Data Engineering

#### Key Programming Languages and Frameworks

- **Common Languages**:
  - **SQL**: For querying and managing databases.
  - **Python**: Widely used for data processing and scripting.
  - **Bash**: For command-line operations and automation.
- **Other Languages**:
  - **Java/Scala**: For big data frameworks like **Spark**.
  - **Rust/Go**: Emerging languages for performance-critical tasks.

#### Code Quality

- **Clean Code**: Write code that is **readable**, **maintainable**, and **testable**.
- **Value Addition**: High-quality code translates to **organizational value** by ensuring reliability and efficiency in data pipelines.

### Applications of Software Engineering in Data Engineering

#### Data Processing

- **Stages**: Write code for **ingestion**, **transformation**, and **serving** of data.
- **Frameworks**: Use tools like **Spark**, **Kafka**, and **Airflow** for efficient data processing.

#### Open Source Contributions

- **Collaboration**: Contribute to **open source frameworks** by developing solutions for specific use cases and submitting **pull requests**.
- **Infrastructure as Code**: Develop **infrastructure as code** and **pipeline as code** solutions to automate and manage data systems.

#### Everyday Problem-Solving

- **General-Purpose Coding**: Write code to solve problems across all stages of the **data engineering lifecycle**.

### Collaboration with Software Engineers

- **Learning Opportunity**: Collaborate with software engineers to improve coding skills and adopt best practices.
- **Cross-Functional Value**: Building strong relationships with software engineers enhances the quality and efficiency of data engineering solutions.

# Practical Examples on AWS

## Data Engineering Lifecycle on AWS

### Introduction to AWS Tools for Data Engineering

- **Purpose**: AWS provides a suite of tools and services to implement the **data engineering lifecycle** and its **undercurrents**.
- **Focus**: This video connects lifecycle stages and undercurrents to specific AWS technologies used in data engineering.

### Source Systems on AWS

#### Relational Databases

- **Amazon RDS**: A managed service for relational databases like **MySQL** and **PostgreSQL**, handling provisioning, patching, and upgrades.
- **Use Case**: Simplifies operational overhead for relational database management.

#### NoSQL Databases

- **Amazon DynamoDB**: A **serverless NoSQL database** with flexible schemas, ideal for low-latency applications like gaming, IoT, and real-time analytics.
- **Features**: Virtually unlimited table size and no need for complex migrations.

#### Streaming Sources

- **Amazon Kinesis Data Streams**: Used for real-time data streaming, such as user activity from a sales platform.
- **Other Options**:
  - **Amazon SQS**: For message queuing.
  - **Apache Kafka**: Open-source streaming platform, with a managed option (**Amazon MSK**) available on AWS.

### Ingestion on AWS

#### Database Ingestion

- **AWS DMS (Database Migration Service)**: Automates data migration and replication between source and target databases.
- **AWS Glue ETL**: A primary tool for data integration and ingestion in the labs.

#### Streaming Ingestion

- **Amazon Kinesis Data Streams** and **Amazon Data Firehose**: Used for ingesting streaming data in labs.
- **Other Tools**: **SQS**, **Kafka**, and similar services for real-time data ingestion.

### Storage on AWS

#### Data Warehousing

- **Amazon Redshift**: A managed data warehouse service for structured data.
- **Amazon S3**: **Object storage** for data lakes, supporting unstructured data.

#### Lakehouse Architecture

- **Combination**: Integrates **Redshift** (structured data) and **S3** (unstructured data) for seamless access.

### Transformation on AWS

- **AWS Glue**: A managed ETL service for data transformation.
- **Additional Tools**: **Apache Spark** and **DBT** for advanced transformation needs, used alongside or as alternatives to Glue.

### Serving Data on AWS

#### Analytics and Business Intelligence

- **Amazon Athena**: For querying structured and unstructured data.
- **Amazon Redshift**: For analytics workloads.
- **Visualization Tools**: **Amazon QuickSight**, **Apache Superset**, and **Metabase** for dashboards and reporting.

#### AI and Machine Learning

- **Batch Data Serving**: For model training.
- **Vector Databases**: For serving data in product recommenders and large language models.

## The Undercurrents on AWS

### Security

- **Shared Responsibility Model**: AWS secures infrastructure, while users secure their applications and data.
- **Identity and Access Management (IAM)**: Manages roles and permissions for secure access to AWS resources.
  - IAM roles provide temporary credentials for secure API access.
- **Network Security**: Key tools include:
  - **Amazon Virtual Private Cloud (VPC)**: Isolated network environments.
  - **Security Groups**: Instance-level firewalls for secure data pipelines.

### Data Management

- **AWS Glue**:
  - **Glue Crawlers**: Discover and create metadata for data in **Amazon S3** or other systems.
  - **Glue Data Catalog**: Manages metadata for data discovery.
- **Lake Formation**: Centralizes fine-grained data access permissions, enhancing data privacy and security.

### DataOps

- **Amazon CloudWatch**:
  - Monitors cloud and on-premises resources.
  - **CloudWatch Logs**: Stores and analyzes operational logs.
- **Amazon Simple Notification Service (SNS)**: Sets up notifications triggered by system events.
- **Open-source Observability Tools**: Examples include Monte Carlo and BigEye.

### Orchestration

- **Apache Airflow**: Industry-standard orchestration tool.
  - Available as open-source or managed by AWS.
- **Emerging Tools**: Dagster, Prefect, and Mage address Airflow limitations.

### Architecture

- **AWS Well-Architected Framework**: Guides building systems for operational efficiency, security, scalability, and sustainability.

### Software Engineering

- **Amazon CodeDeploy**: Automates code deployment.
- **CI/CD Tools**: Used for continuous integration and deployment.
- **Version Control**: Managed with **Git** and **GitHub**.

# Week 2 Quiz

## Questions

1. Which of the following statements is true about the transformation stage of the data engineering lifecycle?
   1. The transformation stage of the data engineering lifecycle is made up of two parts: queries and transformation.
   2. Data transformation only occurs after data has been ingested and before it is stored.
   3. Data transformation is the “turn data into something useful” stage of the data engineering lifecycle.
2. Which of the following statements are true about data generation and source systems? Select all that apply.
   1. Databases are the most common source systems you’ll interact with.
   2. As a data engineer, you are typically responsible for setting up a source system and generating data before the ingestion stage.
   3. As a data engineer, you’ll be most successful if you work directly with source system owners to understand how those systems work.
   4. As a data engineer, you will typically not create or maintain the source systems from which you ingest data.
3. Which of the following are examples of storage abstractions?
   1. Relational Database
   2. Object Storage
   3. Data Lake
   4. Data Warehouse
4. Imagine you are a data engineer at a company that provides online courses through a mobile app. Which of the following examples represents an embedded analytics use case?
   1. A user-facing dashboard that shows learners how many courses they have completed and the total time they have spent learning each week.
   2. An internal dashboard showing course enrollments and ratings.
   3. A recommendation engine that provides learners with suggestions on the next course they should take.
   4. A real-time dashboard that tracks critical performance metrics in the mobile app – like the load time for course content pages and videos – and sends alerts to engineers when errors occur or app features are not performing as expected.
5. When it comes to the security of your data systems, what does the principle of least privilege imply?
   1. You give users or applications access to only the essential data and resources to perform their job or intended function, and only for the duration that is required.
   2. Only the data engineers should operate as “admin”, “superuser” or from the root shell.
   3. Only the most senior members of the team should be given admin access to systems, while more junior members should be given restricted access.
   4. You should always ingest sensitive data, but only allow access to sensitive data to those individuals or applications that require it.
6. Which of the following statements are true about magnetic disk drives, solid-state drives, and Random Access Memory (RAM), which are some of the raw ingredients of storage systems? Select all that apply.
   1. Magnetic disk drives are significantly cheaper than solid-state drives.
   2. RAM is volatile, meaning that if your system loses power, data stored in RAM is typically lost very quickly.
   3. Magnetic disk drives are not used anymore in modern storage systems.
   4. Solid-state drives typically have faster read and write speeds than RAM.
7. True or False: data engineers need to know how to code.
   1. True
   2. False
8. For which of the following use cases would you consider streaming ingestion over batch ingestion? Select all that apply.
   1. To serve data to a data analyst who wants to create dashboards showing online user activity on the company’s website within a second or less after the activity data has been recorded.
   2. To serve data for a fraud detection system in bank transactions, where fraudulent transactions or attempted transactions must be detected and mitigated immediately while they are underway.
   3. To serve data for an operational analytics use case, where instantaneous alerts must be issued if a website goes down or a live product feature stops working.
   4. In a pipeline to serve data to data analysts who are interested in analyzing the company’s weekly sales. Each Monday, the data analysts would like to look at the sales from the previous week.
   5. To serve data to a data scientist who is interested in training a machine learning model. The data scientist would like to use historical data to train the model.
9. What are the three pillars of DataOps?
   1. Orchestration, Version Control, and Incident Response
   2. Automation, Observability & Monitoring, and Incident Response
   3. Automation, Observability, and Monitoring
   4. Automation, Version Control, and Data Quality
10. In the lab this week, you used AWS Glue, which is an ETL service that helps you prepare and integrate data from multiple sources. Which stage(s) of the data engineering lifecycle did you implement using AWS Glue? Select all that apply.
    1. Ingestion
    2. Data generation in source systems
    3. Storage
    4. Transformation
11. In the lab, one of the AWS services you used was Amazon Athena. Which of the following statements best describes this service?
    1. Amazon Athena is a visualization tool that allows you to create Business Intelligence dashboards.
    2. Amazon Athena is a type of object storage system.
    3. Amazon Athena is a query service that allows you to directly query data from Amazon S3 using SQL queries.
    4. Amazon Athena is a relational database service for normalized data.

## Answers

1. 3
   1. The transformation phase of the data engineering lifecycle is where you as a data engineer really start to add value to data by transforming it into a useful form that serves downstream use cases.
2. 1, 3 & 4
   1. The most common source systems are databases, which could be a relational database or other type of NoSQL systems.
   2. You’ll be most successful if you work with the source system owner to understand how those systems generate data, how the data might change over time, and how those changes will impact the downstream systems you build.
   3. In general, source systems are created and maintained by upstream stakeholders so they’re typically out of your control.
3. 3 & 4
   1. A data lake is an example of a storage abstraction that combines other storage systems.
   2. A data warehouse is an example of a storage abstraction that combines other storage systems.
4. 1
   1. This is an example of embedded analytics, which is typically an externally focused, customer-facing type of analytics.
5. 1
6. 1 & 2
   1. At the time of the creation of these courses, disk storage is 2-3 times cheaper than solid-state storage.
7. 1
   1. Within the software engineering undercurrent but also across all stages of the data engineering lifecycle you will need to be able to write clean performant code in your job as a data engineer.
8. 1, 2 & 3
   1. Since the data has to be shown on the dashboard shortly after it is produced, this is a good use case for streaming ingestion.
   2. Since the transaction fraud has to be detected and dealt with as soon as possible after it occurs, this is a good use case for streaming ingestion.
   3. Operational analytics is typically about monitoring real-time data for immediate action, so this is a good use case for streaming ingestion.
9. 2
10. 1 & 4
    1. In the lab, you used AWS Glue to ingest and transform data before storing the data in object storage.
    2. In the lab, you used AWS Glue to ingest and transform data before storing the data in object storage.
11. 3
    1. Amazon Athena allows you to query data from S3 without needing to store the data in a traditional database system.
