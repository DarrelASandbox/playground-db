- [Data Architecture](#data-architecture)
  - [What is Data Architecture?](#what-is-data-architecture)
    - [Enterprise Architecture](#enterprise-architecture)
    - [Change Management in Architecture](#change-management-in-architecture)
    - [Role of Data Engineers](#role-of-data-engineers)
  - [Conway's Law](#conways-law)
    - [Introduction to Conway's Law](#introduction-to-conways-law)
    - [Practical Implications of Conway's Law](#practical-implications-of-conways-law)
    - [Key Takeaway for Data Engineers](#key-takeaway-for-data-engineers)
  - [Principles of Good Data Architecture](#principles-of-good-data-architecture)
    - [Introduction to Data Architecture Principles](#introduction-to-data-architecture-principles)
    - [Core Principles of Good Data Architecture](#core-principles-of-good-data-architecture)
      - [Scalability](#scalability)
      - [Reliability](#reliability)
      - [Flexibility](#flexibility)
      - [Alignment with Business Goals](#alignment-with-business-goals)
      - [Data Quality](#data-quality)
    - [Practical Application](#practical-application)
  - [Always Architecting](#always-architecting)
    - [Reversible Decisions and Always Architecting](#reversible-decisions-and-always-architecting)
    - [Amazon's API Mandate](#amazons-api-mandate)
    - [Principles for Data Architecture](#principles-for-data-architecture)
      - [Make Reversible Decisions](#make-reversible-decisions)
      - [Build Loosely Coupled Systems](#build-loosely-coupled-systems)
      - [Always Be Architecting](#always-be-architecting)
  - [When your Systems Fail](#when-your-systems-fail)
    - [Plan for Failure](#plan-for-failure)
      - [**RTO** and **RPO**](#rto-and-rpo)
    - [Prioritize Security](#prioritize-security)
    - [Architect for Scalability](#architect-for-scalability)
    - [Embrace **FinOps**](#embrace-finops)
  - [Batch Architectures](#batch-architectures)
    - [Batch Architecture Overview](#batch-architecture-overview)
      - [ETL vs. ELT](#etl-vs-elt)
    - [Downstream Use Cases](#downstream-use-cases)
      - [Data Marts](#data-marts)
    - [Architectural Considerations](#architectural-considerations)
  - [Streaming Architectures](#streaming-architectures)
    - [Streaming vs. Batch Processing](#streaming-vs-batch-processing)
      - [Streaming Architecture Components](#streaming-architecture-components)
    - [Historical Architectures](#historical-architectures)
      - [Lambda Architecture](#lambda-architecture)
      - [Kappa Architecture](#kappa-architecture)
    - [Modern Approaches](#modern-approaches)
    - [Compliance Considerations](#compliance-considerations)
  - [Architecting for Compliance](#architecting-for-compliance)
    - [Importance of Compliance](#importance-of-compliance)
    - [Key Data Regulations](#key-data-regulations)
      - [General Data Protection Regulation (GDPR)](#general-data-protection-regulation-gdpr)
      - [Industry-Specific Regulations](#industry-specific-regulations)
    - [Data Engineer Responsibilities](#data-engineer-responsibilities)
- [Choosing the Right Technologies](#choosing-the-right-technologies)
  - [Choosing Tools and Technologies](#choosing-tools-and-technologies)
    - [Role of Tools in Data Architecture](#role-of-tools-in-data-architecture)
    - [Challenges in Technology Selection](#challenges-in-technology-selection)
    - [Key Considerations](#key-considerations)
      - [Location](#location)
      - [Cost Optimization](#cost-optimization)
      - [Future-Proofing](#future-proofing)
    - [Guiding Principles](#guiding-principles)
  - [Location](#location-1)
    - [On-Premises Systems](#on-premises-systems)
    - [Cloud Data Systems](#cloud-data-systems)
      - [Hybrid Approaches](#hybrid-approaches)
    - [Industry Momentum](#industry-momentum)
  - [Monolithic vs. Modular Data Systems](#monolithic-vs-modular-data-systems)
    - [Monolithic Architecture](#monolithic-architecture)
    - [Modular Architecture](#modular-architecture)
      - [**Microservices**](#microservices)
    - [Key Takeaways](#key-takeaways)
  - [Cost Optimization and Business Value](#cost-optimization-and-business-value)
    - [Total Cost of Ownership (**TCO**)](#total-cost-of-ownership-tco)
    - [Total Opportunity Cost of Ownership (**TOCO**)](#total-opportunity-cost-of-ownership-toco)
      - [**Immutable vs. Transitory** Technologies](#immutable-vs-transitory-technologies)
    - [**FinOps** (Financial Operations)](#finops-financial-operations)
  - [Build vs. Buy](#build-vs-buy)
    - [Key Considerations](#key-considerations-1)
      - [**Open-Source** vs. **Proprietary**](#open-source-vs-proprietary)
    - [Recommended Approach](#recommended-approach)
  - [Server, Container, and Serverless Compute Options](#server-container-and-serverless-compute-options)
    - [Compute Options Overview](#compute-options-overview)
    - [Server-Based Compute](#server-based-compute)
    - [Container-Based Compute](#container-based-compute)
    - [Serverless Compute](#serverless-compute)
      - [Serverless Trade-offs](#serverless-trade-offs)
    - [Choosing Between Compute Options](#choosing-between-compute-options)
  - [How the Undercurrents Impact Your Decisions](#how-the-undercurrents-impact-your-decisions)
    - [**Security**](#security)
    - [**Data Management**](#data-management)
    - [**Data Ops**](#data-ops)
    - [**Data Architecture**](#data-architecture-1)
    - [**Orchestration**](#orchestration)
    - [**Software Engineering**](#software-engineering)
- [Investigating Your Architecture on AWS](#investigating-your-architecture-on-aws)
  - [The AWS Well-Architected Framework](#the-aws-well-architected-framework)
    - [Framework Background](#framework-background)
    - [Key Pillars](#key-pillars)
      - [**Operational Excellence**](#operational-excellence)
      - [**Security**](#security-1)
      - [**Reliability**](#reliability-1)
      - [**Performance Efficiency**](#performance-efficiency)
      - [**Cost Optimization**](#cost-optimization-1)
      - [**Sustainability**](#sustainability)
    - [Well-Architected Tools and **Lenses**](#well-architected-tools-and-lenses)
    - [Next Steps](#next-steps)
- [Week 3 Quiz](#week-3-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Data Architecture

## What is Data Architecture?

### Enterprise Architecture

- **Definition**: The design of systems to support change in an enterprise through flexible and reversible decisions, achieved by evaluating trade-offs.
- **Alignment with Data Architecture**: Data architecture is a subset of enterprise architecture, focusing on supporting the evolving data needs of an enterprise.
- **Domains of Enterprise Architecture**:
  - **Business Architecture**: Pertains to product/service strategy and business models.
  - **Application Architecture**: Describes the structure and interaction of key applications serving business needs.
  - **Technical Architecture**: Involves software and hardware components supporting business systems.
  - **Data Architecture**: Focuses on meeting the data needs of the enterprise.

### Change Management in Architecture

- **Central Role**: Change management is crucial in both enterprise and data architecture, as organizational needs evolve over time.
- **One-Way vs. Two-Way Door Decisions**:
  - **One-Way Door**: Irreversible decisions with high stakes (e.g., Amazon shutting down AWS).
  - **Two-Way Door**: Reversible decisions with low stakes (e.g., changing storage classes in Amazon S3).
- **Flexibility**: Aim for two-way door decisions to adapt to organizational changes effectively.

### Role of Data Engineers

- **Architectural Thinking**: Data engineers should build technical solutions that directly support business goals.
- **Problem Solving**: Architects must actively solve business problems and create opportunities, not just map processes.

## Conway's Law

### Introduction to Conway's Law

- **Definition**: Organizations that design systems will produce designs that mirror their communication structures.
- **Origin**: Coined by Melvin Conway, this principle highlights the relationship between organizational structure and system design.

### Practical Implications of Conway's Law

- **Siloed Departments**:
  - In organizations with isolated departments (e.g., sales, marketing, finance, operations), data systems will also be siloed.
  - **Example**: Each department builds its own isolated data system.
- **Cross-Functional Collaboration**:
  - In organizations with strong cross-departmental communication, data systems will reflect collaboration and integration.
  - **Example**: Shared and interconnected data systems across departments.

### Key Takeaway for Data Engineers

- **Understanding Communication Structures**: To design effective data architectures, data engineers must first understand the organization's communication patterns.
- **Alignment with Organizational Culture**: Attempting to build architectures that clash with the organization's communication structure often leads to inefficiencies and failures.

## Principles of Good Data Architecture

### Introduction to Data Architecture Principles

- **Purpose**: Good data architecture ensures systems are scalable, reliable, and aligned with organizational goals.
- **Focus**: Balancing technical requirements with business needs to create adaptable and efficient data systems.

### Core Principles of Good Data Architecture

#### Scalability

- **Definition**: Systems should handle increasing amounts of data and users without performance degradation.
- **Key Considerations**:
  - **Horizontal scaling**: Adding more resources to distribute the load.
  - **Vertical scaling**: Enhancing the capacity of existing resources.

#### Reliability

- **Definition**: Systems must consistently perform as expected, even under failure conditions.
- **Key Considerations**:
  - **Redundancy**: Implementing backup systems to ensure continuous operation.
  - **Fault tolerance**: Designing systems to recover quickly from failures.

#### Flexibility

- **Definition**: Systems should adapt to changing business needs and technological advancements.
- **Key Considerations**:
  - **Modular design**: Building systems with interchangeable components.
  - **Reversible decisions**: Prioritizing two-way door decisions to allow for easy changes.

#### Alignment with Business Goals

- **Definition**: Data architecture should directly support the organization's strategic objectives.
- **Key Considerations**:
  - Stakeholder collaboration: Engaging with business units to understand their needs.
  - Value-driven design: Focusing on solutions that deliver measurable business value.

#### Data Quality

- **Definition**: Ensuring data is accurate, consistent, and usable for decision-making.
- **Key Considerations**:
  - **Data validation**: Implementing checks to maintain data integrity.
  - **Data governance**: Establishing policies and procedures for data management.

### Practical Application

- **Case Studies**: Real-world examples of organizations successfully applying these principles.
- **Best Practices**: Tips for implementing these principles in various scenarios.

## Always Architecting

### Reversible Decisions and Always Architecting

- **One-Way vs. Two-Way Doors**:
  - **One-Way Doors**: Decisions that are difficult or impossible to reverse (e.g., shutting down a major service).
  - **Two-Way Doors**: Reversible decisions that allow flexibility and adaptation (e.g., changing data storage classes).
- **Always Architecting**: Systems built around reversible decisions enable continuous adaptation and evolution.

### Amazon's API Mandate

- [The API Mandate — Install API Thinking at your Company](https://medium.com/api-university/the-api-mandate-install-api-thinking-at-your-company-4335433b7d0b)
- **Origin**: Jeff Bezos's 2002 email to Amazon employees emphasized the importance of **service interfaces (APIs)**.
- **Key Points**:
  - All teams must use APIs to communicate, serve data, and provide functionality.
  - Teams must expose their systems through **stable and predictable service interfaces**.
  - Non-compliance with the mandate could result in termination.
- **Impact**:
  - Enabled teams to function as **loosely coupled systems**, where changes in one team’s system did not disrupt others.
  - Laid the foundation for **Amazon Web Services (AWS)**, making internal APIs public-facing.

### Principles for Data Architecture

#### Make Reversible Decisions

- **Two-Way Doors**: Prioritize decisions that can be easily undone or adjusted.
- **Flexibility**: Ensures systems can adapt to changing business needs and technologies.

#### Build Loosely Coupled Systems

- **Definition**: Systems composed of components that can be swapped or modified independently.
- **Benefits**:
  - Easier to update or replace individual components without redesigning the entire system.
  - Enhances scalability and adaptability.

#### Always Be Architecting

- **Continuous Evolution**: Data architectures must evolve to meet the **changing data needs** of the organization.
- **Future-Proofing**: Data engineers must design systems that not only address current needs but also anticipate future requirements.

## When your Systems Fail

### Plan for Failure

- **Failure is inevitable**: Systems must be designed with contingencies to handle unexpected scenarios.
- **Availability**: The percentage of time a service is operable. For instance, **Amazon S3** offers classes ranging from 99.5% to 99.99%.
  - **99.5% availability** can mean ~44 hours of downtime per year.
  - **99.99% availability** can mean ~1 hour of downtime per year.
- **Reliability**: The probability that a service performs its intended function within a specified time.
- **Durability**: The ability to protect data from loss. **Amazon S3** provides **99.999999999%** durability.

#### **RTO** and **RPO**

- **RTO (Recovery Time Objective)**: Maximum acceptable downtime for a service or system.
- **RPO (Recovery Point Objective)**: Maximum acceptable data loss in case of an outage.

### Prioritize Security

- **Security breaches**: A system can fail if it’s compromised by internal or external threats.
- **Zero trust security**: All actions require authentication, and no user or application is trusted by default.
- **Least privilege principle**: Grant only the minimum necessary access.

### Architect for Scalability

- **Horizontal and vertical scaling**: Ensure systems can handle spikes in usage by increasing capacity on demand.
- **Pay-as-you-go model**: Cloud resources (e.g., **AWS EC2**) can be scaled up for high demand and scaled down to save costs.

### Embrace **FinOps**

- **Dynamic spending**: Monitor and manage cloud costs proactively as workloads and resources fluctuate.
- **AWS on-demand vs. spot instances**: Balance cost and reliability by mixing on-demand and discounted spot resources.
- **Avoid unforeseen costs**: Implement cost controls and alerts to prevent overrunning budgets.

## Batch Architectures

### Batch Architecture Overview

- **Batch processing** involves ingesting, transforming, and storing data in chunks at fixed intervals (e.g., daily).
- Ideal for use cases where real-time analysis is **not critical**, such as daily sales reporting in e-commerce.
- Core patterns include **ETL (Extract, Transform, Load)** and **ELT (Extract, Load, Transform)**.

#### ETL vs. ELT

- **ETL (Extract, Transform, Load)**:
  - Data is extracted from sources, transformed in a **staging area**, then loaded into a data warehouse.
- **ELT (Extract, Load, Transform)**:
  - Data is loaded directly into a data warehouse first, then transformed.
  - Gaining popularity due to enhanced computational power in **modern cloud data warehouses**.

### Downstream Use Cases

- Processed data serves analytics, machine learning, or **reverse ETL** (sending analyzed data back to source systems).
- **Data warehouse** acts as the central storage layer for transformed data.

#### Data Marts

- **Data marts**: Curated subsets of a data warehouse focused on specific departments (e.g., sales, marketing).
- Enable additional transformations (e.g., joins, aggregations) to optimize query performance.
- Improve accessibility for reporting and analytics teams.

### Architectural Considerations

- **Interoperability**: Choose common components (e.g., shared data warehouses) to facilitate collaboration across teams.
- **Failure planning**:
  - Monitor **source system reliability** (e.g., outages, schema changes).
  - Validate component **availability** and **reliability** metrics.
- **Flexibility**: Design systems to handle variable batch cadences or data volumes over time.
- **FinOps integration**:

  - Conduct **cost-benefit analyses** to balance performance and business value.
  - Optimize resource usage (e.g., storage, compute) in cloud environments.

- **Key technologies** present risks and opportunities; align choices with organizational goals and principles.

## Streaming Architectures

### Streaming vs. Batch Processing

- **Streaming data pipelines** ingest and process data in **near real-time** (e.g., within seconds of generation).
- **Batch processing** handles accumulated data at fixed intervals (e.g., daily), as seen in traditional ETL/ELT workflows.
- **Key distinction**: Streaming prioritizes low-latency delivery, while batch optimizes for bulk processing.

#### Streaming Architecture Components

- **Producer**: Source of event streams (e.g., clickstream data, IoT sensors).
- **Consumer**: Service or system processing data (e.g., analytics tools, data lakes).
- **Streaming broker** (e.g., **Apache Kafka**): Manages data flow between producers and consumers.

### Historical Architectures

- [A brief introduction to two data processing architectures — Lambda and Kappa for Big Data](https://medium.com/towards-data-science/a-brief-introduction-to-two-data-processing-architectures-lambda-and-kappa-for-big-data-4f35c28005bb)

#### Lambda Architecture

- [Questioning the Lambda Architecture](https://www.oreilly.com/radar/questioning-the-lambda-architecture/)
- Processes data in **parallel batch and streaming layers**, merging results for queries.
- **Challenges**: Code duplication, complexity in maintaining dual systems.

#### Kappa Architecture

- Uses **stream processing as the backbone** for all data (real-time and historical).
- Enables batch processing by replaying retained event streams.

### Modern Approaches

- **Batch as a subset of streaming**: Treats batch data as **bounded event streams** (e.g., **Apache Beam’s Data Flow Model**).
- **Unified frameworks**: Tools like **Apache Flink** enable shared codebases for batch/stream processing.
- **Key philosophy**: Simplify architectures by leveraging streaming-first systems for all data types.

### Compliance Considerations

- Ensure systems comply with **laws, regulations, and organizational policies** (e.g., GDPR, data privacy agreements).
- Integrate compliance checks into architecture design to avoid legal risks and breaches.

- **Design principles**: Prioritize flexibility, scalability, and failure mitigation while aligning with compliance needs.

## Architecting for Compliance

### Importance of Compliance

- **Compliance prevents legal failures**: Non-compliance risks lawsuits, fines, and reputational damage (e.g., GDPR violations).
- **Data engineers must prioritize compliance** as part of system design, even if regulations seem "boring."

### Key Data Regulations

#### General Data Protection Regulation (GDPR)

- **GDPR**: EU regulation (2018) protecting personal data, including **PII (Personally Identifiable Information)** and other identifiable data.
- **Requirements**: Obtain explicit user consent and enable timely data deletion ("right to be forgotten").
- **Global impact**: Similar laws exist in 100+ countries/states (e.g., CCPA in California), affecting companies beyond the EU.

#### Industry-Specific Regulations

- **Healthcare (HIPAA)**: US law mandating protection of sensitive patient health data.
- **Finance (Sarbanes-Oxley)**: US regulation for financial reporting accuracy and record-keeping.
- **Global variations**: Most industries/countries have sector-specific data laws.

### Data Engineer Responsibilities

- **Build future-proof systems**: Design for current and future regulations (e.g., stricter privacy laws).
- **Flexible architecture**: Use loosely coupled systems to adapt to regulatory changes efficiently.
- **Proactive compliance**:

  - Align with strict standards (e.g., GDPR) even if local laws are lenient.
  - Stay updated on evolving regulations in operational regions and expansion markets.

- **Value delivery**: Mitigate legal risks to protect organizational reputation and finances.

# Choosing the Right Technologies

## Choosing Tools and Technologies

### Role of Tools in Data Architecture

- **Tools translate architecture into reality**: They define the "how" to achieve business data needs (the "what," "why," and "when").
- **End goal alignment**: Prioritize tools that deliver **high-quality data products** meeting user requirements.

### Challenges in Technology Selection

- **Overabundance of options**: Choices include **open-source**, **managed services**, and **proprietary solutions** across ingestion, storage, transformation, and serving stages.
- **Risk of poor choices**: Even with good architecture, incorrect tools can lead to system failures or inefficiencies.

### Key Considerations

#### Location

- **On-premises vs. cloud**: Evaluate trade-offs in control, scalability, and maintenance.
- **Hybrid approaches**: Combine on-premises and cloud systems for flexibility.

#### Cost Optimization

- **Build vs. buy**: Assess team capabilities and business value drivers (e.g., custom tools vs. off-the-shelf solutions).

#### Future-Proofing

- **Current vs. future needs**: Select adaptable tools that accommodate evolving organizational demands.

### Guiding Principles

- **Flexibility**: Build **loosely coupled systems** to enable scalability and iterative changes.
- **Lifecycle alignment**: Align tool choices with **data engineering lifecycle** stages (ingestion, storage, etc.).

- **Value-driven decisions**: Focus on technologies that enhance system resilience, compliance, and stakeholder outcomes.

## Location

### On-Premises Systems

- Historically the only option before modern cloud platforms emerged.
- **Complete ownership**: Companies manage and maintain hardware, software, and infrastructure.
- **Operational overhead**: Requires provisioning, updating, and scaling in-house resources.

### Cloud Data Systems

- **Provider-managed**: Vendors like **AWS** maintain hardware and data centers.
- **Scalability**: Easily scale up or down to match demand, reducing costs and complexity.
- **Flexibility**: Rapidly switch tools or configurations without heavy upfront investment.
- **Minimal maintenance**: No need for on-site hardware or large IT teams.

#### Hybrid Approaches

- **Partial migration**: Some organizations keep sensitive systems on-premises while moving others to the cloud.
- **Regulatory or security concerns**: May require retaining certain workloads or data in a private environment.

### Industry Momentum

- **Cloud adoption**: Increasingly favored due to **flexibility** and **scalability**.
- **On-premises drawbacks**: High maintenance and slower adaptability.
- **Future focus**: Most data engineering roles now emphasize **cloud-based** solutions and architectures.

## Monolithic vs. Modular Data Systems

### Monolithic Architecture

- **Single code base**: All functionality is tightly coupled and deployed together.
- **Simplicity**: Easier to understand when there is only one primary programming language and framework.
- **Maintenance challenges**: A single change may require updates to multiple components. Large **ETL pipelines** can be difficult to modify or restart (e.g., a 48-hour process).

### Modular Architecture

- **Loosely coupled**: Components are self-contained, allowing independent updates without impacting the entire system.
- **Flexible upgrades**: Easier to replace or improve parts of the system without massive rewrites or downtime.
- **Interoperability**: Modern data tools (e.g., **Parquet** format) integrate well, supporting a wide range of processing engines.

#### **Microservices**

- **Independent services**: Each microservice handles a specific function and is deployed separately.
- **Scalability**: Allows teams to scale individual services based on demand.
- **Fault isolation**: Failures in one service do not necessarily bring down the entire system.

### Key Takeaways

- **Cloud-first** and **modular** approaches dominate modern data engineering.
- **Reversible decisions**: Loosely coupled components enable continuous improvement and tool swaps as technology evolves.
- Aim for **flexibility** and **scalability** to ensure efficient, long-term data system success.

## Cost Optimization and Business Value

### Total Cost of Ownership (**TCO**)

- **Definition**: Total estimated cost of a solution over its lifecycle (hardware, software, salaries, maintenance).
- **Direct costs**: Salaries, AWS bills, licenses.
- **Indirect costs**: Downtime, IT support, lost productivity.
- **CapEx vs. OpEx**:
  - **Capital Expenses (CapEx)**: Large upfront investments in hardware/software (depreciation over time).
  - **Operational Expenses (OpEx)**: Ongoing costs, often in **pay-as-you-go** models (e.g., cloud subscriptions).

### Total Opportunity Cost of Ownership (**TOCO**)

- **Definition**: Cost of lost opportunities when choosing one tool/technology over another.
- **Evolving landscape**: Rapidly changing data tools mean yesterday’s best choice may become obsolete.
- **Mitigation**: Build **flexible, loosely coupled systems** to allow easy replacement of outdated components.

#### **Immutable vs. Transitory** Technologies

- **Immutable**: Well-established, stable technologies (e.g., **SQL**, **cloud storage**, **networking**).
- **Transitory**: Rapidly evolving areas (e.g., **stream processing**, **AI**, **orchestration**).

### **FinOps** (Financial Operations)

- **Goal**: Minimize **TCO** and **TOCO** while maximizing revenue opportunities.
- **Cloud-first approach**: Use **OpEx-first** and **pay-as-you-go** services to avoid large upfront costs.
- **Modular design**: Facilitates quick iteration and cost-effective scaling.

## Build vs. Buy

### Key Considerations

- **Undifferentiated heavy lifting**: Building from scratch or customizing **Open-Source** tools may provide limited additional value.
- **Team bandwidth**: Smaller teams might benefit from **managed or proprietary** services rather than running their own infrastructure.
- **Implementation and maintenance**: Even if **Open-Source** appears free, the **Total Cost of Ownership (TCO)** includes staff time and overhead.
- **Opportunity cost**: Focus on tools that let you deliver unique value rather than reinventing existing solutions.

#### **Open-Source** vs. **Proprietary**

- **Fully Open-Source**: Offers flexibility and no license fees but requires in-house expertise for deployment and support.
- **Commercial Open-Source**: Vendor-managed distributions reduce operational burden.
- **Proprietary solutions**: May offer specialized features and support but can lock you into vendor-specific ecosystems.

### Recommended Approach

- **Evaluate existing options**: Check established **Open-Source** or commercial tools before considering proprietary.
- **Use modular services**: Select tools that integrate easily, letting you swap out components without rebuilding everything.
- **Focus on business value**: Ensure the time and effort spent on building or customizing solutions directly benefits your organization.

## Server, Container, and Serverless Compute Options

### Compute Options Overview

- Three primary compute models: **server-based**, **container-based**, and **serverless**.
- Choice depends on control needs, scalability, cost, and operational complexity.

### Server-Based Compute

- **Servers** (e.g., **Amazon EC2**):
  - Require full management of OS updates, security, scaling, and networking.
  - Provide full control over infrastructure but increase maintenance overhead.

### Container-Based Compute

- **Containers** (e.g., Docker, Kubernetes):
  - Lightweight, isolated environments packaging code and dependencies.
  - Abstract OS/networking layers but require managing orchestration (e.g., **Kubernetes**).
  - Ideal for complex workflows needing flexibility and portability.

### Serverless Compute

- **Serverless** (e.g., **AWS Lambda**, **AWS Glue**, **Amazon Athena**):
  - No server management: Infrastructure fully abstracted.
  - Auto-scaling, built-in fault tolerance, and **pay-as-you-go pricing**.
  - Best for event-driven, short-lived tasks (e.g., data transformation triggers).

#### Serverless Trade-offs

- **Cost risks**: High-frequency usage can lead to unexpected expenses.
- **Execution limits**: Constraints on runtime duration, concurrency, and memory.

### Choosing Between Compute Options

- **Start with serverless**: For simple, low-volume tasks (e.g., sporadic ETL jobs).
- **Move to containers**: When workflows require customization, longer runtime, or complex orchestration.
- **Cost modeling**: Compare serverless pricing (per-event) vs. server/container (fixed/resource-based).

- **Key recommendation**: Prioritize serverless for agility, then adopt containers as needs scale.

## How the Undercurrents Impact Your Decisions

### **Security**

- Different tools offer varying **authentication** and **encryption** features.
- Use **reputable** or verified **Open-Source** solutions; verify code to avoid malicious components.
- Ensure clear practices for **data privacy** and handling compliance.

### **Data Management**

- Check how tools handle **data governance** (e.g., **GDPR** compliance, data quality).
- Confirm protection against **internal** and **external** breaches.
- Understand how data lineage and **metadata** management are supported.

### **Data Ops**

- Look for **automation** and **monitoring** capabilities to streamline pipelines.
- Review **SLAs** for reliability and availability, especially if using **managed services**.

### **Data Architecture**

- Aim for **modularity** and **interoperability** to ensure loose coupling between components.
- Choose tools that integrate well (e.g., **common file formats** like Parquet).

### **Orchestration**

- Consider popular options such as **Apache Airflow**, **Prefect**, **Dagster**, and **Mage**.
- Evaluate how each tool aligns with your **data workflow** and evolves with changing requirements.

### **Software Engineering**

- Decide on the **level of custom development** vs. using existing **Open-Source** or **proprietary** solutions.
- Avoid **undifferentiated heavy lifting**: hard work that does not add direct value.
- Start with **Open-Source** or **commercial Open-Source**; move to proprietary only if unmet needs remain.

# Investigating Your Architecture on AWS

## The AWS Well-Architected Framework

- [AWS Well-Architected](https://aws.amazon.com/architecture/well-architected)
  - [AWS Well-Architected Framework](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html)
  - [AWS Well-Architected Labs](https://www.wellarchitectedlabs.com/)

### Framework Background

- **AWS Solutions Architects** and experts compiled best practices for building secure, reliable, and efficient systems on AWS.
- Focuses on **six key pillars** that guide decision-making rather than prescribing fixed designs.

### Key Pillars

#### **Operational Excellence**

- Improve how workloads are developed, tested, and run in production.
- Emphasize **monitoring**, consistent deployment practices, and continuous improvement.

#### **Security**

- Protect **data, systems, and assets** through secure design and monitoring.
- Encourage a **culture of security** and use reputable tools to avoid vulnerabilities.

#### **Reliability**

- Design for **fault tolerance**: Ensure systems consistently perform intended functions.
- **Plan for failure** and adapt to changing demands or conditions.

#### **Performance Efficiency**

- Use a **data-driven approach** to meet performance targets.
- Ensure flexible resource allocation to handle variable workloads and evolving technologies.

#### **Cost Optimization**

- Use tools like **AWS Cost Explorer** and **Cost Optimization Hub** for visibility and recommendations.

#### **Sustainability**

- Reduce **environmental impact** by optimizing energy consumption.
- Focus on efficiency in storage, compute, and data transfer to minimize carbon footprint.

### Well-Architected Tools and **Lenses**

- **Well-Architected Tool**: Helps assess current AWS architectures for potential **risks** and **improvement** areas.
- **Lenses**: Domain-specific extensions, such as the **Data Analytics Lens**, provide targeted best practices for data workloads (e.g., scalability, security, cost).

### Next Steps

- Explore the **AWS Well-Architected Framework** resources for deeper guidance.
- Apply the **Data Analytics Lens** to evaluate and enhance your data architectures.
- Use the pillars to iteratively refine systems for **reliability**, **security**, **performance**, **cost**, and **sustainability**.

# Week 3 Quiz

## Questions

1. Which of the following statements are true about "loosely-coupled systems" ? Select all that apply.
   1. With a loosely-coupled system, if you need to swap out one of the components, you’ll likely have to redesign the entire system for it to function correctly.
   2. Building loosely-coupled systems helps you ensure your decisions are reversible.
   3. Stable and predictable APIs enabled teams at Amazon to function as a loosely-coupled system.
   4. Loosely-coupled systems give you the ability to always be architecting.
2. In which of the following scenarios would you consider running your data engineering workloads on premises vs. on the cloud? Select all that apply.
   1. You’re working on an application where you might experience fluctuating demands for compute resources and you need your system to handle transient load spikes.
   2. You’re working with data that your company is required by law to have full control over the systems and subsystems used for compute and storage and the data must be kept within a specific geographic location.
   3. You’re working on a task that has stable and predictable demands for computing resources, and your company already has these resources established in its own data centers.
   4. You’re looking to run your workload in a highly resilient environment that ensures data recovery in case of disaster or system failures.
3. Which of the following is true about serverless computing?
   1. It means that you wrap your code and dependencies in a single unit that can be run on any server.
   2. It means that the service is a low-code environment that requires less coding effort.
   3. It means that the server that runs the service or application is abstracted from you, so that you don’t need to worry about scaling, managing, and maintaining it.
   4. Serverless services are always the least expensive option.
4. What is Enterprise Architecture? (As defined in this course)
   1. Enterprise architecture is the design of systems to support change in an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs.
   2. Enterprise architecture is the design of organizational structure and hierarchy of an enterprise, including roles and responsibilities.
   3. Enterprise architecture is a sub-area of Data Architecture that is focused on optimizing how data is served throughout the enterprise.
5. This week you saw two examples of security approaches that you can take to protect your data: the “zero-trust” approach and the “hardened-perimeter” approach. What is the main difference between the two approaches?
   1. Taking a “hardened-perimeter” approach means that you block or restrict access to all external connections, while treating all internal traffic within your organization as trusted. On the other hand with “zero-trust”, no users or applications are trusted by default, whether they are internal or external to your organization’s network.
   2. Taking a “zero-trust” approach means that you block or restrict access to all external connections, while allowing all internal traffic within your organization’s network. On the other hand, “hardened-perimeter” assumes that you consider all users and applications as untrusted by default whether they are inside or outside the organization’s network.
   3. There is no difference between these two approaches as the “hardened-perimeter” approach implies building a “wall of security” around all the data resources of your system and “zero trust” means you trust no external actors with access to your data systems, which is effectively the same thing.
6. True or False: The statement “you can think of batch processing as a special case of streaming” simply means that, since data is just information about events that are happening continuously out in the world, essentially all data is streaming at its source. Therefore, streaming ingestion could be thought of as the most natural or basic approach, while batch ingestion just imposes arbitrary boundaries on that same stream of data.
   1. False
   2. True
7. When it comes to considerations around building your own solution from scratch versus using an available open source or managed service, what does it mean to “avoid undifferentiated heavy lifting”? Select all that apply.
   1. Avoiding undifferentiated heavy lifting means always choosing low-code / no code service options when available because these types of tools allow you to do less work and still get to a good result.
   2. Avoiding undifferentiated heavy lifting in some cases means choosing to pay for a managed or proprietary service instead of a free open source option because it saves your team time and effort and allows you to dedicate your time to areas that add more value for the business.
   3. Avoiding undifferentiated heavy lifting means avoiding doing work that costs significant time and resources but doesn’t add value in terms of cost savings or advantage for the business.
8. “Choose common components wisely” is one of the principles of good data architecture. Which of the following statements are true about this principle? Select all that apply.
   1. Common components can be any tool or technology that has broad applicability within an organization and that can be shared by different teams.
   2. Common components are popular components that are widely used across many organizations.
   3. Making a "wise" choice of common components is considered an act of leadership because it helps the company stay up-to date with the latest technology.
   4. Making a "wise" choice of common components is considered an act of leadership because it means that you identify the tools that help teams do their best work and work well with each other.
9. What are the key pillars of the AWS Well-Architected Framework?
   1. Plan for Failure, Architect for Scalability, Prioritize Security, Embrace FinOps
   2. Common Components, Leadership
   3. Always be Architecting, Build Loosely Coupled Systems, Make Reversible Decisions
   4. Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, Sustainability

## Answers

1. 2, 3 & 4
   1. Loosely-coupled systems allow for decisions to be reversed, because you can introduce a new/updated component without disturbing your overall system.
   2. APIs allowed teams at Amazon to communicate and serve data and functionality through a stable interface. This allowed individual teams to plug into one another, and any reconfiguration or retooling within any given team did not affect the others.
   3. With loosely coupled systems, you can easily swap out components for new ones, allowing you to always be architecting and constantly adapting to the changes in business requirements and available technology.
2. 2 & 3
   1. If you keep your workload on premises, you can have full control over your data and where it must be stored, as required by law.
   2. Since you’re expecting no fluctuations in the demand and your organization already has the needed resources, then it makes sense to run the workload on-premises.
3. 3
   1. With a serveless option, you can use services or run applications without managing the servers behind the scenes.
4. 1
5. 1
   1. In the modern era of cloud computing, where resources are typically connected over the internet, the notion of a “hardened-perimeter” has eroded, such that you should adopt a zero-trust approach in all the data systems you build.
6. 2
7. 2 & 3
   1. In some cases, a free open source solution may seem appealing but if the time and resources it would take to implement and maintain it are significant, a managed or proprietary service could be cheaper in the long run and free you up to do other work.
   2. Oftentimes using an available open source or managed service solution will actually be cheaper than paying a team to build from scratch and free you up to focus on other tasks instead of spending time deploying and maintaining a custom tool.
8. 1 & 4
   1. Common components can be anything that has broad applicability within an organization, including things like object storage, version-control systems, observability, monitoring and orchestration systems, and visualization tools.
   2. Making a wise choice of common components that will be used across teams means you are identifying opportunities, removing barriers to productivity and avoiding one-size-fits-all solutions.
9. 4
   1. You can think of these pillars as a set of principles that will help you have productive conversations about your existing solutions and help you design and operate reliable, secure, efficient, cost-effective, and sustainable systems in the cloud.
