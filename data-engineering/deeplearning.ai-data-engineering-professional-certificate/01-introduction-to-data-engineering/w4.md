- [Stakeholder Management \& Gathering Requirements](#stakeholder-management--gathering-requirements)
  - [Overview](#overview)
    - [Requirements Gathering](#requirements-gathering)
    - [Data Engineering Lifecycle](#data-engineering-lifecycle)
    - [Data Architecture Principles](#data-architecture-principles)
    - [Practical Application](#practical-application)
  - [Requirements](#requirements)
    - [Hierarchy of Needs](#hierarchy-of-needs)
      - [Functional vs. Non-Functional Requirements](#functional-vs-non-functional-requirements)
    - [Stakeholder Engagement](#stakeholder-engagement)
  - [Conversation with Matt Housley](#conversation-with-matt-housley)
    - [Background on the Book and Course](#background-on-the-book-and-course)
    - [Course and Book Synergy](#course-and-book-synergy)
    - [Breaking into Data Engineering](#breaking-into-data-engineering)
      - [**Role-Specific Focus**](#role-specific-focus)
    - [Next Steps](#next-steps)
  - [Conversation with the CTO](#conversation-with-the-cto)
    - [Business Goals \& Initiatives (E-Commerce Modernization)](#business-goals--initiatives-e-commerce-modernization)
    - [Technical Challenges](#technical-challenges)
    - [Tools \& Technologies](#tools--technologies)
      - [Streaming Infrastructure](#streaming-infrastructure)
      - [AI/ML Integration](#aiml-integration)
    - [Data Engineer’s Role](#data-engineers-role)
    - [Future Directions](#future-directions)
  - [Conversation with Marketing](#conversation-with-marketing)
    - [Current Dashboard Limitations](#current-dashboard-limitations)
      - [Real-Time Data Need](#real-time-data-need)
    - [Recommendation System Requirements](#recommendation-system-requirements)
    - [Next Steps](#next-steps-1)
  - [Breaking Down the Conversation with Marketing](#breaking-down-the-conversation-with-marketing)
    - [Business Goals](#business-goals)
    - [Stakeholder Needs](#stakeholder-needs)
      - [Analytics Dashboards](#analytics-dashboards)
      - [Recommender System](#recommender-system)
    - [Functional Requirements](#functional-requirements)
  - [Conversation with the Software Engineer](#conversation-with-the-software-engineer)
    - [Current Data Sharing Approach](#current-data-sharing-approach)
    - [Planned Improvements](#planned-improvements)
      - [Schema Changes](#schema-changes)
    - [Collaboration and Communication](#collaboration-and-communication)
  - [Documenting Non-functional Requirements](#documenting-non-functional-requirements)
    - [Conversation Takeaways](#conversation-takeaways)
    - [Documenting System Requirements](#documenting-system-requirements)
    - [Nonfunctional Requirements](#nonfunctional-requirements)
      - [Analytics Dashboards](#analytics-dashboards-1)
      - [Recommender System](#recommender-system-1)
  - [Requirements Gathering Summary](#requirements-gathering-summary)
    - [Importance of Stakeholder Alignment](#importance-of-stakeholder-alignment)
    - [The Iron Triangle](#the-iron-triangle)
      - [Breaking the Iron Triangle](#breaking-the-iron-triangle)
- [Translating Requirements to Architecture](#translating-requirements-to-architecture)
  - [Requirements Gathering Exercise](#requirements-gathering-exercise)
    - [Project Scope](#project-scope)
    - [Requirements Gathering](#requirements-gathering-1)
      - [**Quiz** on Requirements](#quiz-on-requirements)
    - [AWS Tool Selection](#aws-tool-selection)
    - [Lab Exercise](#lab-exercise)
  - [Follow-up Conversation with the Data Scientist](#follow-up-conversation-with-the-data-scientist)
    - [Recommender System Overview](#recommender-system-overview)
    - [Training Data Requirements](#training-data-requirements)
      - [Model Retraining](#model-retraining)
    - [Real-Time Recommendations](#real-time-recommendations)
    - [Scalability](#scalability)
  - [Conversation with the Data Scientist](#conversation-with-the-data-scientist)
    - [Transcript](#transcript)
    - [Conversation Take-Aways](#conversation-take-aways)
    - [Details of the Recommender System](#details-of-the-recommender-system)
  - [Reflection: Extracting Functional \& Nonfunctional Requirements](#reflection-extracting-functional--nonfunctional-requirements)
    - [Feedback](#feedback)
    - [Feedback](#feedback-1)
  - [AWS Services for Batch Pipelines](#aws-services-for-batch-pipelines)
    - [Batch Processing Tools Overview](#batch-processing-tools-overview)
    - [ETL Pipeline Components](#etl-pipeline-components)
      - [Serverless Options](#serverless-options)
    - [Storage and Serving Solutions](#storage-and-serving-solutions)
    - [Decision Factors](#decision-factors)
  - [AWS Services for Streaming Pipelines](#aws-services-for-streaming-pipelines)
  - [AWS Services to Meet Your Requirements](#aws-services-to-meet-your-requirements)
    - [AWS Service Options for Batch Pipelines](#aws-service-options-for-batch-pipelines)
    - [AWS Service Options for Streaming Pipelines](#aws-service-options-for-streaming-pipelines)
    - [Additional readings](#additional-readings)
  - [Choosing Tools and Technologies for Your Data Pipelines](#choosing-tools-and-technologies-for-your-data-pipelines)
    - [Questions](#questions)
    - [Answers](#answers)
- [Week 4 Quiz](#week-4-quiz)
  - [Questions](#questions-1)
  - [Answers](#answers-1)

# Stakeholder Management & Gathering Requirements

## Overview

### Requirements Gathering

- **Stakeholder collaboration**: Involves discussions with data scientists, marketing teams, and software engineers to identify needs.
- **Framework for system design**:
  - **Step 1**: Identify **business goals** and stakeholder needs.
  - **Step 2**: Translate needs into **system requirements** (functional and non-functional).
  - **Step 3**: Choose tools/technologies that meet requirements (e.g., prototyping, testing).
  - **Step 4**: Build, deploy, and evolve the system.

### Data Engineering Lifecycle

- **Stages and undercurrents**: Focus on components (data systems) and people (source system owners, end users).
- **Key focus areas**:
  - **Data systems**: Architecture, orchestration, security, and scalability.
  - **Stakeholder roles**: Collaboration across teams to align technical and business objectives.

### Data Architecture Principles

- **Planning for failure**: Design systems with redundancy and fault tolerance.
- **Common components**: Use standardized, reusable elements in system design for efficiency.
- **Best practices**: Emphasize operational efficiency, security, and sustainability (aligned with **AWS Well-Architected Framework**).

### Practical Application

- **Week 4 focus**: Integrate concepts from Weeks 1-3 into real-world scenarios.
  - **Requirements documentation**: Define functional/non-functional requirements.
  - **Tool selection**: Match AWS services (or alternatives) to requirements.
  - **Implementation**: Build and deploy systems on **AWS Cloud**.
- **Outcome**: Translate requirements into an architecture design and execute end-to-end implementation.

## Requirements

### Hierarchy of Needs

- **Business goals**: High-level objectives (e.g., increased revenue, market share, or user base).
- **Stakeholder needs**: Resources and tools required by employees to achieve business goals.
- **System requirements**: What data systems must do to meet stakeholder needs.

#### Functional vs. Non-Functional Requirements

- **Functional requirements**: Specific capabilities (e.g., "The system must immediately report potentially fraudulent transactions.").
- **Non-functional requirements**: Attributes or constraints (e.g., "The system must scale to handle 10,000 concurrent users.").

### Stakeholder Engagement

- **Gather requirements** from top-level leadership (CTO, CDO, or CEO) to align with overall business goals.
- **Consult end users** (e.g., data scientists) to refine system specifications.
- **Bridging the hierarchy**: Understand and connect business objectives with technical implementation.

## Conversation with Matt Housley

### Background on the Book and Course

- **Motivation**: Identified a gap in available resources—most focused on tools, not on defining data engineering or providing a strategic framework.
- **First principles**: Emphasize conceptual understanding rather than a single technology stack.
- **Audience**:
  - Primarily those transitioning into **data engineering** with a technical background.
  - Secondary audiences include product managers or professionals who need to understand data workflows.

### Course and Book Synergy

- **Hands-on application**: The course complements theoretical insights with **real-world examples** on **AWS**.
- **Practical labs**: Reinforce core concepts of the **data engineering lifecycle** and undercurrents (security, data management, etc.).

### Breaking into Data Engineering

- **Fundamental data concepts**: Start by exploring how analytics data is used (even with **Microsoft Excel**) to understand **tabular data** and basic reporting.
- **Tool diversity**: Early stages involve experimenting with various tools to build **general competence**.
- **Mental framework**: Leverage resources like the _Fundamentals of Data Engineering_ to categorize tools and principles properly.

#### **Role-Specific Focus**

- **Real-world roles**: Once employed, specialization aligns with the company’s chosen tech stack.
- **CTO perspective**: C-suite executives focus on **strategy, scalability**, and **business impact** of data initiatives.

### Next Steps

- **Mock conversation**: Demonstrates how data engineers and a **Chief Technology Officer (CTO)** discuss business objectives and technical requirements for an e-commerce company.
- **Consulting experiences**: Working with diverse companies (startups to enterprises) provides insights into **common CTO concerns** around data and engineering.

## Conversation with the CTO

### Business Goals & Initiatives (E-Commerce Modernization)

- **Key initiatives**: Expand market share, launch new product lines, and scale internationally.
- **Customer retention**: Prioritized through improved website reliability and a **recommendation engine** powered by data pipelines.

### Technical Challenges

- **Legacy systems**: Outdated code risks outages and creates complex, analytics-unfriendly schemas.
- **Software-data divide**: Disconnect between software outputs and data engineering needs increases **ETL complexity**.

### Tools & Technologies

#### Streaming Infrastructure

- Transition from batch processing to **AWS Kinesis** and **Kafka** for scalable real-time data ingestion.
- Develop proof-of-concept pipelines to validate scalability and reliability.

#### AI/ML Integration

- Build pipelines for **clickstream analysis** and **order behavior data** to train recommendation models.
- Collaborate with data scientists to engineer features for machine learning workflows.

### Data Engineer’s Role

- **Collaboration with software teams**: Advise on schema design during code refactoring to minimize post-processing.
- **Modernization focus**: Lead adoption of streaming technologies and phase out legacy systems.
- **Skill development**: Master **Kinesis/Kafka** to manage expanding streaming capabilities.

### Future Directions

- **Global scalability**: Design systems to support international expansion and high-traffic demands.
- **Proactive risk mitigation**: Implement fault-tolerant architectures to prevent outages.

## Conversation with Marketing

### Current Dashboard Limitations

- **Existing dashboards** display product sales by category, region, and time (daily/hourly).
- **Data lag**: Currently 2 days old, limiting real-time insights.
- **Desired improvement**: Reduce lag to ~1 hour for timely trend detection and action.

#### Real-Time Data Need

- **Goal**: Identify rising demand within hours (or less) to deploy targeted promotions.
- **Demand spikes**: Can last from a few hours to a couple of days, requiring immediate visibility.

### Recommendation System Requirements

- **Current approach**: Displays popular products from the previous week to all customers.
- **Desired approach**: **Personalized recommendations** based on:
  - Customer’s **purchase history**.
  - **Items in cart** at checkout.
- **Outcome**: Similar to other e-commerce sites, suggesting additional products that match user preferences.

### Next Steps

- **Action items**:
  - Implement near real-time data ingestion (goal: ~1 hour old data).
  - Expand recommendation logic to incorporate **customer behavior** and **in-cart items**.
- **Further collaboration**: More questions may arise; keep open communication channels with **Product Marketing** to refine requirements.

## Breaking Down the Conversation with Marketing

### Business Goals

- **Customer retention**: Improve loyalty and repeat purchases.
- **Expansion**: Launch new products and enter new markets.
- **Data-driven decisions**: Leverage analytics to guide strategic actions.

### Stakeholder Needs

- **Analytics Dashboards** to monitor product sales and detect demand spikes.
- **Recommender System** for personalized product suggestions.

#### Analytics Dashboards

- **Current challenge**: 2-day data lag.
- **Desired state**: Data no more than 1 hour old to catch spikes (lasting from hours to days).
- **Immediate action**: Marketing to launch targeted campaigns during high-demand periods.

#### Recommender System

- **Current approach**: Generic "popular items of the week."
- **Goal**: **Personalized recommendations** based on browsing history and items in the cart.
- **Cross-team effort**: Data engineers, data scientists, and platform team must collaborate on model training, deployment, and serving.

### Functional Requirements

- **Timely Data Delivery**: The data system must serve transformed data within 1 hour of generation for the dashboards.
- **Recommender Data Flow**: Provide up-to-date training data, ingest real-time user information, and return product recommendations to the platform.

## Conversation with the Software Engineer

### Current Data Sharing Approach

- **Daily file exports**: The sales platform team provides static files once a day.
- **Production risk**: Direct access to the production database is disallowed to avoid performance and security issues.

### Planned Improvements

- **Read replica database**: A separate copy of production data updated continuously.
- **API access**: Allows downstream teams to query data without impacting live systems.
- **Automatic notifications**: Alerts when **server downtime** or maintenance impacts data availability.

#### Schema Changes

- **Frequent updates**: New features, regions, or product lines can alter the database schema.
- **Advance notice**: Typically ~1 week before deployment. Essential for **data engineers** to adjust pipelines.
- **Consistent API**: The sales platform team can provide stable endpoints for critical data fields.

### Collaboration and Communication

- **Open lines**: Proactively discuss upcoming changes to **schemas** and timelines.
- **Automated checks**: Ingestion pipelines can validate data formats and alert stakeholders of anomalies.
- **Goal**: Minimize disruptions and ensure reliable data delivery for analytics and marketing use cases.

## Documenting Non-functional Requirements

### Conversation Takeaways

- **Read replica database and API**: Proposed solution to enable continuous data access while protecting production systems.
- **Stability measures**: Minimize breaking changes to downstream systems through schema management.
- **System notifications**: Source engineers will alert about outages or upcoming schema changes.

### Documenting System Requirements

- Follows hierarchical structure: **Business goals** → **Stakeholder needs** → **System requirements**.
- **Functional requirements**:
  - Analytic dashboards: Data must be available within 1 hour of source recording.
  - Recommender system: Real-time recommendations based on user activity.

### Nonfunctional Requirements

#### Analytics Dashboards

- **Scalability and latency**: System must handle peak data volumes without exceeding 1-hour latency.
- **Reliability**: Perform **data quality checks** to ensure ingested data conforms to expected formats.
- **Maintainability**: Ingestion/transformation stages must adapt easily to schema changes.

#### Recommender System

- **Latency**: Less than 1 second from data ingestion to recommendation delivery.
- **Scalability**: Support maximum concurrent users without performance degradation.
- **Reliability**: Fallback to serving **most popular products** if pipeline fails to ensure consistent user experience.

## Requirements Gathering Summary

### Importance of Stakeholder Alignment

- **Identify stakeholders** and understand their needs in context of **business goals**.
- **Conversations** with leadership, data scientists, and other teams ensure clarity on objectives.
- Proper **documentation** (functional and non-functional requirements) validates that the planned system meets stakeholder expectations.

### The Iron Triangle

- **Three constraints**: Timeline, scope, and cost often pull in different directions.
- **Trade-offs**: Expanding one aspect (e.g., scope) can affect the others (e.g., extend timeline, increase budget).
- Old saying: **“Good, fast, cheap—pick two.”**

#### Breaking the Iron Triangle

- **Loosely coupled architectures**: Facilitates flexible changes without large overhead.
- **Two-way door decisions**: Enables reversible choices and experimentation.
- **Thorough requirement analysis**: Minimizes surprises and rework, helping manage cost and scope effectively.

# Translating Requirements to Architecture

## Requirements Gathering Exercise

### Project Scope

- **Micro simulation**: Practice moving from requirements gathering to final system deployment in a condensed timeframe.
- **Objective**: Build and customize a **recommender system** using AWS services, based on stakeholder needs.

### Requirements Gathering

- **Focus on Recommender System**: Collect **functional** and **non-functional** requirements from a conversation between the **data engineer** and **data scientist**.
- **Transcript Review**: Analyze the discussion to extract detailed requirements (e.g., data latency, data quality, scalability).

#### **Quiz** on Requirements

- **Identify additional needs**: Combine these new findings with the previous lesson’s requirements documentation.
- **Functional vs. Non-Functional**: Ensure clarity on features (what the system does) and constraints (how the system performs).

### AWS Tool Selection

- **Next Videos**: Morgan at AWS explains **various AWS services** (e.g., compute, storage, analytics) suited to the project.
- **Second Quiz**: Match the **right AWS tools** to meet the identified requirements (e.g., low-latency storage, serverless processing).

### Lab Exercise

- **Pre-deployed infrastructure**: A **basic recommender system** scaffold is already set up.
- **Customization**: Configure services based on your **chosen AWS tools** to fulfill stakeholder requirements.
- **Hands-on Practice**: Experience an **end-to-end** data engineering workflow—from needs analysis to implementation—in a short timeframe.

## Follow-up Conversation with the Data Scientist

### Recommender System Overview

- **Content-based recommender system**: Uses vector embeddings to find similarities between user/product features.
- **Input**: User features (customer number, location, credit limit) and product features (stock, price, category).
- **Output**: List of recommended product IDs based on similarity calculations.

### Training Data Requirements

- **Data format**: Tabular structure with rows representing product purchases.
- **Key columns**:
  - **User features**: Customer number, credit limit, city, postal code, country.
  - **Product features**: Product code, stock quantity, price, product line.
  - **Rating**: User-provided score (1-5) for each product.

#### Model Retraining

- **Retraining frequency**: Initially undetermined; could range from weekly to quarterly based on performance drift.
- **Operational flexibility**: Pipeline must support easy delivery of fresh data batches and accommodate new features.

### Real-Time Recommendations

- **Latency requirement**: Recommendations must be generated within **1-2 seconds** during user browsing/checkout.
- **Model speed**: Current local implementation runs in **milliseconds** for similarity calculations.
- **Data flow**: Requires sub-second round-trip latency for ingesting user/product data, transforming features, and serving recommendations.

### Scalability

- **Current scale**: Supports spikes of **10,000+ concurrent users**.
- **Future-proofing**: Designed to accommodate growth as the company expands to new regions/product lines.
- **Performance stability**: System must maintain low latency during peak usage periods.

## Conversation with the Data Scientist

### Transcript

**Data Engineer**: The last time we spoke we talked about two projects you’re working on for the marketing team: an analytics dashboard and a recommender system. Since then I’ve talked to our product marketing manager to learn more about their needs for each of these systems and I met with one of the software engineers from the platform team to learn more about how we can best access the data they are generating on the sales platform. What I’d like to do now is learn more from you about how you are imagining the recommender system will work so that we can start to prototype the data pipeline to serve this system. Can you tell me more about your work so far on this project?

**Data Scientist**: Sure thing! So, I’ve been experimenting over the last month or so with some models and ideas for the recommender system. And for now I’ve decided to implement a content-based recommender system. The way that works is that I take a set of features containing information about each product and then also a set of features containing information about each user. I then compute a vector embedding for each user and each product and look for similarities between these embeddings in order to make product recommendations to users. So the model takes user and product features as inputs and generates a list of product ids as output.

**Data Engineer**: Ok cool. So, you’ve already got something up and running?

**Data Scientist**: Well, sort of. I’ve been experimenting with some models on my local machine. I’ve trained a content-based recommender with some of the user and product data that I had initially downloaded for the dashboards. The model seems to be working ok, and what I’d like to try next is retraining the model based on a fresh batch of data to see how stable it is and try to get a sense of how often we might want to retrain and update the model in deployment.

**Data Engineer**: Ok great, well if you can tell me more about the format and structure you need for the training data, I can start working on a plan for a data pipeline to deliver that data in batches.

**Data Scientist**: Ok, the training data I’m using is in tabular format. Each row contains data for a single product purchase, where the columns include a collection of user features, which are: “customer number”, “credit limit", "city", “postal code”, and "country". And then there’s also a set of product features, including “product code”, "quantity in stock", "buy price”, "msrp", "product line", and "product scale". In addition to these user and product features, there is a rating value from 1 to 5 that represents the user’s rating of that product.

**Data Engineer**: Got it! Now, if you had to guess, how frequently do you think you might want to retrain and update the deployed model?

**Data Scientist**: Well, the short answer is I’m not sure yet. I will plan to monitor the model’s output. In fact, it would be great if, in addition to serving recommendations on the platform, all the model output could be automatically saved for later analysis. And then I would plan to retrain the model if I notice any drift in performance or change in the input data. So, I guess I might want to retrain the model as frequently as once a week, but it might be less than that, like monthly or quarterly. So it would be great if it doesn’t require too much operational overhead for you to deliver a new batch of training data and possibly in a modified format if we have new product or user features we want to incorporate.

**Data Engineer**: OK that all makes sense. Now, can you tell me more about how you’re expecting the recommender model will be used in production?

**Data Scientist**: Yes, so what I’m thinking is that I would like to make recommendations based on both the user information as well as any information about products they have been browsing or that they have in their shopping cart. The way I imagine this could work is that the model will be set up to take user information as well as the information about any number of products they have been looking at as input. I can then find a set of products to recommend based on the user information and additional products to recommend based on the products they have been browsing or have in their cart.

**Data Engineer**: How fast will the system need to generate recommendations?

**Data Scientist**: Sure, so we would like to be able to present recommended products to the user more or less instantaneously as they are browsing through different products or during the checkout process. It usually takes a new page a second or two to render so if the recommender could work that fast it would be great.

**Data Engineer**: Ok, how long does it take to actually run the model and generate recommendations?

**Data Scientist**: Oh the model is really fast, on my local machine it only takes a few milliseconds to generate recommendations given a set of input features. The way I have set it up is that I already have all of the vector embeddings for the products in the catalog stored such that when I run the model I can quickly run the similarity calculation.

**Data Engineer**: Ok great, as I understand it the platform records data from users currently online in an event log so it should be possible to just stream events from that log with very low latency. So, assuming that the sales platform is able to both provide user and product data and then receive and render product recommendations with sub second latency, I should have maybe one second or so to complete the round trip with the data, namely, ingest, transform and serve user and product features to the model, then return the recommended product ids back to the platform.

What about scalability? Do you have a sense of how many concurrent users you would expect to be serving product recommendations to?

**Data Scientist**: Well, the company currently has around 100k customers in the database, meaning that many individual customers have bought at least one item, and many of them are repeat shoppers. We’re expecting that number to grow as the company expands to new regions and product lines. So, right now, it varies a lot. Sometimes there are only a few people shopping on the platform but we can see activity spikes of up to 10k users on the platform at the same time and we could see more in the future.

**Data Scientist**: Ok great, let me know if any more questions come up!

### Conversation Take-Aways

Here are the key takeaways from this conversation. You are tasked with building two data pipelines:

- A batch data pipeline that serves training data for the data scientists to train the recommender system;
- A streaming data pipeline that provides the top product recommendations to users based on their online activity. The streaming data pipeline makes use of the trained recommender system to provide the recommendations.

![data-pipelines](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/data-pipelines.png)

The recommendation system will recommend products to customers based on their user information and based on the products the customer has been browsing or that are in their cart.

**Notes**:

- The lab focuses on providing an end-to-end data pipeline example. Your main tasks will be to interact with the components of the batch and streaming pipelines.
- The lab does not focus on fine tuning the performance of the recommender model. Moreover the data that is used is the same one used in week 2, and may not contain good predictive user and product features. The goal of the lab is not to assess the performance of the recommender problem but to incorporate its computations inside the streaming pipeline.
- The recommendation model will compute two vectors -- the product embedding vector and the user embedding vector. It will then generate product recommendations based on these vectors. The product embedding is a vector that holds information about product characteristics, and the user embedding is a vector that holds information about user preferences for each of the product characteristics represented in the product embedding vector. To learn more about vector embeddings and how the recommendation system works, feel free to check out the next optional reading item.

### Details of the Recommender System

After you implement the batch pipeline in the lab to serve training data to the data scientist, you will be provided with a pre-trained recommender system. The development and training of the recommender system are outside the scope of this data engineering specialization. You will then use the recommender system to find products to recommend as part of the streaming pipeline.

This section briefly explains how the content-based recommender system works and how it is trained. This optional section can be read at your leisure if you’re interested in learning more about the recommender system.

![recommendation-model](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/recommendation-model.png)

Given the features of a specific user, the model predicts what ratings the user will assign to a specific product. By computing the predicted ratings for all company products, the model can provide a list of recommendations for the user (“Based on your personal profile, you may like …”). The model is trained to compute two vectors called embeddings: one describing the characteristics of a product (product embedding) and another describing the user’s preferences (user embedding). Based on these two embeddings, the model can predict if a user will like a product.

Product embeddings can also provide another set of recommendations for the user. Based on the user’s interactions with products on the company’s platform (products purchased, placed in a cart, or searched for), similar products can be found and recommended to the user (“Based on your browsing history, you may like…”). For any product the user has interacted with in a given session, the recommender model computes its embedding vector. This embedding vector is then compared to the embeddings of all company products to find similar items (based on their distances). For finding similar items, you will implement a vector database in the lab, which makes retrieving similar products faster than using the brute-force approach. In the lab, you will be given the embeddings computed by the model for all company products. You will store all the embeddings in a vector database. The vector database organizes the embeddings so that similar products are placed close to each other, accelerating the search for similar items.

The recommender model will provide a combination of recommendations based on similar items and user features.

## Reflection: Extracting Functional & Nonfunctional Requirements

Based on the conversation between the data engineer and the data scientist, what are the functional and nonfunctional requirements for the batch pipeline?

**Functional Requirements**

1. Data Ingestion
   1. Extract user and product data from the sales platform in batches.
   2. Format the data in a structured tabular format.
2. Data Processing
   1. Ensure the data contains user features (customer number, credit limit, city, postal code, country).
   2. Ensure the data contains product features (product code, quantity in stock, buy price, msrp, product line, product scale).
   3. Include a rating value (1-5) for product interactions.
3. Data Storage
   1. Store training data in a format suitable for model training (CSV, Parquet, or database).
   2. Ensure historical model output is stored for drift monitoring and analysis.
4. Model Retraining
   1. Provide fresh batches of training data at a configurable frequency (weekly, monthly, or quarterly).
   2. Support modifications in schema for new user or product features.
   3. Ensure compatibility with model retraining pipeline.

**Non-functional Requirements**

1. **Scalability**: Handle increasing amounts of user and product data as the company grows.
2. **Reliability**: Ensure high data integrity with proper validation and deduplication mechanisms.
3. **Flexibility**: Allow modifications in data format or schema with minimal operational overhead.
4. **Automation**: Enable automated scheduling for batch data extraction and processing.
5. **Performance**: Ensure efficient batch processing to deliver fresh training data within an acceptable time frame.

#### Feedback

**Functional Requirements**

When thinking about the functional requirements of the data pipeline, consider the tasks the pipeline needs to perform (i.e. what data it needs to ingest or combine, how it should process it, and what data it needs to serve).

In this case, the data pipeline needs to serve data in a format that is suitable for training the machine learning model as shown in the figure below (i.e. tabular and containing the columns specified)

![machine-learning-model](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/machine-learning-model.png)

Although not explicitly discussed in the conversation, other common considerations around functional requirements that are usually discussed for machine learning models include:

- The duration the data pipeline should retain or store the training data.
- How the data pipeline should combine the old with the new dataset. Should the old dataset be discarded? If the same customer updated their rating for a given product, should you keep the last rating or compute the average?
- The file format the training data should be served in. (e.g. CSV, Parquet, etc.)

**Nonfunctional requirements**

Nonfunctional requirements are attributes or characteristics of the data pipeline that allow it to successfully meet stakeholder needs. The data scientist mentioned that they’d like to retrain the model as soon as they notice any drift in performance or change in the input data, meaning that the data pipeline needs to perform a simple transformation task only when it is needed. So, it must not require too much operational overhead to deliver the new training set.

Although not explicitly discussed in the conversation, another common consideration around nonfunctional requirements is the cost of implementing the batch pipeline. You generally want to build a cost effective pipeline.

Here's a table that summarizes these requirements:

|                                                                                                                  Functional Requirements                                                                                                                   |                                                    Nonfunctional Requirements                                                    |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |
|                                                                                 Provide the training data for the content-based recommender model in the following format:                                                                                 |                                                                                                                                  |
|                                                                                                                       - Tabular data                                                                                                                       | The data system must be easy to maintain and update, and requires less operational overhead (Irregular / on demand run schedule) |
| Each row in the table contains the following user and item features: “customer number”, “credit limit", "city", “postal code”, and “country”, “product code”, "quantity in stock", "buy price”, "msrp", "product line", "product scale", “product rating”. |                                             The data system must be cost effective.                                              |

Based on the conversation between the data engineer and the data scientist, what are the functional and nonfunctional requirements for the streaming data pipeline?

**Functional Requirements**

1. Real-time Data Ingestion
   1. Capture live user interactions (product views, cart additions) from the event log.
   2. Extract user and product features for each interaction.
2. Feature Transformation
   1. Format the extracted data into a structure usable by the recommender model.
3. Model Inference
   1. Use precomputed vector embeddings to generate recommendations.
   2. Return a ranked list of recommended products.
4. Data Serving
   1. Deliver product recommendations with sub-second latency.
   2. Send recommendations to the sales platform for rendering.

**Non-functional Requirements**

1. **Low Latency**: The pipeline must complete the ingestion, transformation, and inference in under one second.
2. **Scalability**: Support up to 10,000 concurrent users and scale with future growth.
3. **High Availability**: Ensure the pipeline remains operational during traffic spikes.
4. **Efficiency**: Optimize real-time processing to maintain system performance.
5. **Logging and Monitoring**: Track recommendations for performance monitoring and drift detection.

#### Feedback

**Functional requirements**

The streaming data pipeline has to do the following:

- use the recommender system to find the set of products to recommend for a given user based on the user’s information and the products they interacted with online.
- stream the products to the sales platform and store them for later analysis.

**Nonfunctional requirements**

- **Latency**: The system should provide the recommended products to the user instantaneously as they are browsing through different products or during the checkout process. For a given user, the system must be able to ingest and extract user and product data, provide them to the recommender system, and finally serve the product recommendations to the sales platform, all in a few seconds.
- **Scalability & concurrency**: The system must be able to scale up to ingest, transform and serve the data volume expected with the maximum level of user activity on the platform, while staying within the latency requirements.

## AWS Services for Batch Pipelines

### Batch Processing Tools Overview

- **Key analogy**: Choosing tools = selecting vehicles based on requirements (distance, passengers, budget).
- **Focus areas**: Match technology capabilities to system needs (batch vs. streaming, scale, cost).

### ETL Pipeline Components

- **ETL paradigm**: Extract from **Amazon RDS** (common source for tabular data), transform, then load.
- **Avoid undifferentiated heavy lifting**: Prefer serverless/cloud-native tools over custom EC2-based scripts.

#### Serverless Options

- **AWS Lambda**:
  - Suitable for small-scale ETL tasks with code triggers.
  - Limitations: 15-minute runtime limit, memory/CPU constraints.
- **Amazon Glue ETL**:
  - **Automated metadata handling**: Uses crawlers to discover schemas and populate the **Glue Data Catalog**.
  - Visual ETL interface generates Spark code automatically.
- **Amazon EMR Serverless**:
  - **Big data flexibility**: Supports Apache Spark/Hadoop for petabyte-scale analytics.
  - **Control vs. Convenience**: More customizable than Glue but requires deeper configuration.

### Storage and Serving Solutions

- **Amazon S3**:
  - **Primary staging area**: Cost-effective object storage for ML training data.
  - Key features: Scalability, integration with AWS services, and bucket organization.
- **Amazon RDS**: Suitable for serving transformed tabular data in relational formats.
- **Amazon Redshift**:
  - **Data warehouse option**: Handles complex analytical queries on large datasets.
  - Higher cost compared to S3/RDS but offers advanced analytics features.

### Decision Factors

- **Downstream use case**:
  - ML training → **S3** for flexibility.
  - Analytics → **Redshift** for performance.
- **Cost vs. capability**: Balance budget constraints with technical requirements (e.g., S3 for cost efficiency vs. Redshift for speed).

## AWS Services for Streaming Pipelines

## AWS Services to Meet Your Requirements

In the previous practice quiz, you extracted the functional and nonfunctional requirements of the batch and streaming pipelines. In the quiz that's coming up, you will be given different architecture options for both pipelines. Your task is to choose the combination of AWS services that best meets the functional and nonfunctional requirements. Please make sure to check the feedback of the previous quiz because the discussion of the architectural choices relies on the requirements laid out in the feedback of the previous quiz.

In the previous two videos, Morgan walked you through some of the common AWS services that you can use when implementing batch and streaming pipelines. Feel free to also check out the following links to learn more about some of these AWS services.

### AWS Service Options for Batch Pipelines

- **AWS Glue ETL**: [features](https://aws.amazon.com/glue/features/) (check the intro and the two sections Discover (the first and last items) and Prepare (the first item)).
- **Amazon EMR**: you can quickly check the overview [here](https://aws.amazon.com/emr/?nc=sn&loc=1) and the first part in [features](https://aws.amazon.com/emr/features/?nc=sn&loc=2&dn=1).
- **Amazon S3**: you can quickly check the overview [here](https://aws.amazon.com/s3/).
- **AWS Redshift**: you can check the first three sections [here](https://www.cloudzero.com/blog/aws-redshift/).

### AWS Service Options for Streaming Pipelines

In the lab this week, you will only focus on implementing the part of the streaming pipeline that stores the recommended products.

![streaming-data-pipeline](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/streaming-data-pipeline.png)

Feel free to check out the following links to learn more about AWS services that you might choose to implement in your streaming pipeline:

- Amazon Kinesis Data Streams: check the quick overview [here](https://aws.amazon.com/kinesis/data-streams/?nc=sn&loc=1) (make sure to check the diagram on the same page) and the features [here](https://aws.amazon.com/kinesis/data-streams/features/?nc=sn&loc=2).
- Amazon Data Firehose: check the quick overview [here](https://aws.amazon.com/kinesis/data-firehose/?nc=sn&loc=2&dn=3) (make sure to check the diagram on the same page) and the features [here](https://aws.amazon.com/kinesis/data-firehose/features/?nc=sn&loc=2).
- Amazon MSK (Management Streaming for Apache Kafka): check the quick overview [here](https://aws.amazon.com/msk/) and the [features](https://aws.amazon.com/msk/features/) (the first paragraph is enough)

### Additional readings

- [A guide to choosing the right AWS streaming service: Kinesis VS MSK](https://medium.com/slalom-build/a-guide-to-choosing-the-right-streaming-solution-for-you-on-aws-57089f03e034)
- [Streaming data on AWS: Amazon Kinesis Data Streams or AWS MSK?](https://programmaticponderings.com/2023/04/23/streaming-data-on-aws-amazon-kinesis-data-streams-or-amazon-msk/)

## Choosing Tools and Technologies for Your Data Pipelines

### Questions

1. Recall the functional and nonfunctional requirements you gathered for the batch data pipeline used to serve training data for the product recommendation system. Now, let's suppose that since you are the first data engineer at this e-commerce company, you prefer to use data tools that provide more convenience and help you avoid undifferentiated heavy lifting, such as writing custom code. Which of the following combinations of AWS services would you use to implement the batch data pipeline?
   1. Amazon RDS, AWS Glue ETL, Amazon S3
   2. Run an AWS Lambda function to extract data from Amazon RDS, apply transformations to it, and store it in Amazon S3.
   3. Amazon RDS, AWS Glue ETL, Amazon Redshift
   4. Run scripts on Amazon EC2 to ingest data from Amazon RDS, transform it, and store it in Amazon S3
   5. Amazon RDS, Amazon EMR, Amazon S3
2. Recall the functional and nonfunctional requirements you gathered for the streaming pipeline that will be used to provide the product recommendations to the users. Again, let's suppose that you prefer to use services that provide more convenience because you're newer to data streaming architectures, and want to avoid undifferentiated heavy lifting, such as writing custom code. Which of the following is the best combination of AWS services to implement this streaming data pipeline?
   1. Amazon MSK, Amazon Data Firehose, Amazon S3
   2. Run an AWS Lambda function to interact with a streaming source, apply transformations to the data, and store it in Amazon S3
   3. Run scripts on Amazon EC2 to connect to a streaming source, transform the data, and store it in Amazon S3
   4. Amazon Kinesis Data Stream, Amazon Data Firehose, Amazon S3
   5. AWS Glue ETL, Amazon S3

![streaming-data-pipeline](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/01-introduction-to-data-engineering/assets/streaming-data-pipeline.png)

### Answers

1. 1
   1. **Source System**: Since you'll be ingesting tabular data, the source system can be a relational database, like Amazon Relational Database Service (RDS).
   2. **Ingestion and Transformation**: Since the batch pipeline needs to run based on demand (not continuously), and considering the fact that transforming data into training data sets is a simple task that can be executed in a short period of time, a serverless option might be best here. AWS Glue is a serverless ETL service that makes it easy to create ETL jobs. It can extract data from the source database, transform data and then load it into downstream storage. So, AWS Glue ETL is a suitable option for this batch pipeline. For simple workloads that run based on demand, AWS Glue can be reasonably priced. Feel free to check out the pricing of AWS Glue [here](https://aws.amazon.com/glue/pricing/) to learn more. Since AWS Glue ETL is serverless, it makes it easy for you to maintain and manage the data pipeline, even if you're the only data engineer at the company. Finally, as you saw in the previous video with Morgan, Glue ETL is a more convenient option than EMR, which is what you're looking for here.
   3. **Storage**: Amazon S3 is a durable, performant, and low-cost storage solution that allows you to serve data to a machine learning training process. Since the downstream data scientist is another technical data professional who’s planning to manipulate the data and use it to train the product recommendation system, S3 is a great storage option because it is flexible, scalable, and relatively cost-effective.
2. 4
   1. **Streaming System**: Amazon Kinesis Data Streams is a highly scalable streaming solution that provides low latency access to data. It offers an on-demand serverless deployment that makes it easy to set up and manage your data pipeline. It is a simpler solution than Amazon MSK, and can help you get started quickly without requiring special expertise. Kinesis Data Streams integrates well with other AWS services, and scales with increased volume in data (you can check out the pricing for this [here](https://aws.amazon.com/kinesis/data-streams/pricing/)). Amazon Data Firehose is also used here because it helps you deliver streaming data from Kinesis data streams into data stores such as S3. Moreover, in the lab, you will use Kinesis data firehose to invoke a lambda function that allows you to run the computations of the deployed model to find the products to recommend.
   2. **Storage**: Amazon S3 is a durable, performant, and low-cost storage solution that you can use to store the product recommendations for later analysis. Since the downstream data scientist is another technical data professional who’s planning to analyze the data and use it to retrain the model when needed, S3 is a great storage option because it is flexible, scalable, and relatively cost-effective.

# Week 4 Quiz

## Questions

1. In the lecture videos, system requirements are described as part of a hierarchy of needs. What is at the top of the hierarchy?
   1. Business goals and objectives
   2. Stakeholder needs
   3. Functional requirements
   4. Nonfunctional requirements
2. Stakeholders are the individual employees of the business who have specific needs and play a role in delivering on the business objectives. Which of the following best represents an example of "stakeholder needs"?
   1. The finance team needs to increase the overall profit margins of the business.
   2. The CEO needs to change the company's mission statement to reflect a new vision.
   3. The marketing director needs to expand the business to new international markets.
   4. A data analyst needs a robust data system to create monthly sales reports to track trends in monthly sales.
3. Which of the following are examples of nonfunctional requirements? Select all that apply.
   1. The system notifies users when sales metrics are outside some threshold.
   2. The system must be secure and protect the sensitive data that it stores.
   3. The data system provides raw data for exploratory purposes.
   4. The system must be able to scale up to handle data from 15,000 users simultaneously.
4. Which of the following are examples of functional requirements? Select all that apply.
   1. The data system must be available 99.9% of the time.
   2. The data system detects potentially fraudulent transactions.
   3. The data system combines sales data from several platforms and transforms it for analytical purposes.
   4. The data system must be able to access the public internet to download patches and updates but must not be accessible from the public internet from any inbound traffic.
5. According to the video this week on how to think about the tradeoffs between cost, scope, and timeline for your projects, what is the best approach when it comes to simultaneously optimizing for all three of these (i.e., breaking the "iron triangle")?
   1. Build sufficient buffer into your timelines and budgets so that if the scope of the project expands you'll still have room to maneuver.
   2. Apply the principles and processes you've learned about this week, namely, build loosely coupled systems, optimize for two-way door decisions, and understand the needs of stakeholders.
   3. Spend extra time in the planning phase of the project so that you deeply understand the scope of work before getting started and can better estimate your required timeline and budget.
6. Which of the following statements is true when comparing AWS Glue ETL and Amazon EMR as batch data processing tools?
   1. AWS Glue ETL can handle big data workloads while Amazon EMR cannot.
   2. AWS Glue ETL is serverless whereas Amazon EMR doesn’t have a serverless option.
   3. AWS Glue ETL has fewer additional features than Amazon EMR but gives you more control over what you can do with code.
   4. AWS Glue ETL can offer a more convenient experience than Amazon EMR, provided your use case fits within the features and capabilities of AWS Glue ETL.
7. Which of the following statements is true when comparing Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (MSK) as stream processing tools?
   1. Amazon Kinesis Data Streams comes with more operational overhead than Amazon Managed Streaming for Apache Kafka.
   2. Amazon Kinesis Data Streams gives you a higher degree of flexibility and control over MSK.
   3. Amazon Kinesis Data Streams is not fully managed whereas MSK is.
   4. Amazon Kinesis Data Streams gives you a more convenient experience than MSK.

## Answers

1. 1
   1. At the top of this hierarchy you have the goals and objectives of the business. The next thing down on this hierarchy is stakeholder needs. Below stakeholder needs in this hierarchy you have system requirements.
2. 4
   1. This directly reflects the needs of a data analyst to have proper resources and robust data systems so that they can do their job and contribute toward business objectives, such as increasing monthly sales.
3. 2 & 4
   1. This is a nonfunctional requirement as it pertains to the security attribute of the system, which is a characteristic of how the system functions, rather than what specific tasks it performs.
   2. This is a nonfunctional requirement as it relates to the system's scalability and concurrency attributes.
4. 2 & 3
   1. This is a functional requirement, as it specifies a particular function or capability of the system, which is in this case, detecting and reporting potentially fraudulent transactions.
   2. This is a functional requirement, as it specifies a particular function or capability of the system, which in this case is combining and transforming sales data from different platforms.
5. 2
6. 4
   1. Both tools are designed to handle big data workloads, but the difference between these two tools comes down to the tradeoffs between control vs convenience. EMR serverless gives you more control over what you can do, while Glue ETL provides a more convenient experience.
7. 4
   1. The difference between these two tools comes down to the tradeoffs between control vs convenience. MSK gives you a higher degree of flexibility and more control over what you can do, while Kinesis Data Streams is more user-friendly and reduces operational overhead, providing a more convenient experience.
