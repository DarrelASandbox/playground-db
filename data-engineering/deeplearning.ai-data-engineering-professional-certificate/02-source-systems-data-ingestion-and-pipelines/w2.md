- [Data Ingestion Overview](#data-ingestion-overview)
  - [Data Ingestion on a Continuum](#data-ingestion-on-a-continuum)
    - [Batch vs. Stream Ingestion](#batch-vs-stream-ingestion)
      - [Key Differences](#key-differences)
    - [Factors Influencing Ingestion Approach](#factors-influencing-ingestion-approach)
    - [Source System Considerations](#source-system-considerations)
      - [Databases](#databases)
      - [APIs](#apis)
      - [Files](#files)
      - [IoT/Streaming Systems](#iotstreaming-systems)
    - [Practical Applications and Trends](#practical-applications-and-trends)
  - [Batch and Streaming Tools](#batch-and-streaming-tools)
    - [Batch Ingestion Tools](#batch-ingestion-tools)
    - [Streaming Ingestion Tools](#streaming-ingestion-tools)
    - [Key Considerations for Batch vs Streaming Ingestion](#key-considerations-for-batch-vs-streaming-ingestion)
- [Batch Ingestion](#batch-ingestion)
  - [Conversation with a Marketing Analyst (Data Ingestion for Marketing Insights)](#conversation-with-a-marketing-analyst-data-ingestion-for-marketing-insights)
  - [Project Background](#project-background)
  - [Stakeholder Requirements](#stakeholder-requirements)
  - [Ingestion Strategy](#ingestion-strategy)
    - [ETL vs. ELT](#etl-vs-elt)
  - [Next Steps](#next-steps)
  - [ETL vs. ELT](#etl-vs-elt-1)
    - [Project Context](#project-context)
    - [Batch Processing Patterns](#batch-processing-patterns)
      - [**ETL (Extract, Transform, Load)**](#etl-extract-transform-load)
      - [**ELT (Extract, Load, Transform)**](#elt-extract-load-transform)
    - [Recommended Approach](#recommended-approach)
  - [Summary of the Differences: ETL vs. ELT](#summary-of-the-differences-etl-vs-elt)
  - [REST API](#rest-api)
    - [Amazon's API Mandate (2002)](#amazons-api-mandate-2002)
      - [Public-Facing Requirement](#public-facing-requirement)
    - [API Fundamentals](#api-fundamentals)
    - [REST APIs](#rest-apis)
    - [Data Engineering Applications](#data-engineering-applications)
- [Streaming Ingestion](#streaming-ingestion)
  - [Conversation with a Software Engineer (Setup for Product Recommender System)](#conversation-with-a-software-engineer-setup-for-product-recommender-system)
    - [Data Source and Ingestion Mechanism](#data-source-and-ingestion-mechanism)
    - [Data Format and Message Rate](#data-format-and-message-rate)
      - [Capacity Considerations](#capacity-considerations)
    - [Stream Configuration and Retention](#stream-configuration-and-retention)
    - [Pipeline Integration and Use Cases](#pipeline-integration-and-use-cases)
  - [Streaming Ingestion Details](#streaming-ingestion-details)
    - [Key Concepts](#key-concepts)
      - [Message Queues](#message-queues)
      - [Event Streaming Platforms](#event-streaming-platforms)
    - [Apache Kafka](#apache-kafka)
      - [Topics and Partitions](#topics-and-partitions)
      - [Consumer Groups](#consumer-groups)
    - [Amazon Kinesis](#amazon-kinesis)
    - [Additional Ingestion Approaches](#additional-ingestion-approaches)
  - [Kinesis Data Streams](#kinesis-data-streams)
    - [Core Components](#core-components)
    - [Shards and Capacity](#shards-and-capacity)
      - [Capacity Planning](#capacity-planning)
    - [Operational Modes](#operational-modes)
      - [On-Demand Mode](#on-demand-mode)
      - [Provisioned Mode](#provisioned-mode)
    - [Data Records and Partitioning](#data-records-and-partitioning)
    - [Consumer Configuration](#consumer-configuration)
    - [Use Cases and Flexibility](#use-cases-and-flexibility)
  - [What is Change Data Capture (CDC)?](#what-is-change-data-capture-cdc)
    - [What is CDC?](#what-is-cdc)
    - [Use Cases for CDC](#use-cases-for-cdc)
    - [Two approaches to CDC](#two-approaches-to-cdc)
    - [CDC Implementation Patterns](#cdc-implementation-patterns)
    - [Tools for CDC](#tools-for-cdc)
  - [Summary: General Considerations for Choosing Ingestion Tools](#summary-general-considerations-for-choosing-ingestion-tools)
    - [Characteristics of the data](#characteristics-of-the-data)
    - [Reliability and Durability](#reliability-and-durability)
- [Week 2 Quiz](#week-2-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Data Ingestion Overview

## Data Ingestion on a Continuum

### Batch vs. Stream Ingestion

- **Stream ingestion**: Processes unbounded, continuous event data (e.g., stock prices, user clicks) in real time.
- **Batch ingestion**: Imposes boundaries (size, time, or record count) on data streams to process as single units.
- **Micro-batch processing**: Bridges batch/stream approaches by processing small bounded chunks frequently.

#### Key Differences

- **Boundaries**: Batch uses explicit limits (e.g., hourly/daily chunks), while streaming handles data as it arrives.
- **Continuum concept**: Batch and streaming exist on a spectrum, with frequency determining classification.

### Factors Influencing Ingestion Approach

- **Business use case**: Determines required data freshness (e.g., real-time analytics vs. weekly reports).
- **Source system constraints**: APIs may limit call frequency; databases may require JDBC/ODBC connectors.
- **Tools**: **AWS Glue ETL** enables scheduled batch ingestion; streaming systems like **Kinesis** handle real-time data.

### Source System Considerations

#### Databases

- Use connectors (JDBC/ODBC) or serverless tools (**AWS Glue**) for scheduled or threshold-triggered ingestion.

#### APIs

- Often require custom code due to lack of universal standards.
- **Recommendation**: Use vendor-provided client libraries or managed platforms before building custom solutions.

#### Files

- May involve manual transfers via protocols like **SFTP/SCP**.
- Automated ingestion via object storage systems (e.g., Amazon S3) is preferred.

#### IoT/Streaming Systems

- Require message queues (e.g., **Kafka**) or streaming services (e.g., **Kinesis**) for real-time ingestion.

### Practical Applications and Trends

- **Industry trends**: Growth of managed data platforms simplifies API connectivity.
- **Case studies**: Batch ingestion from APIs vs. streaming from **Kinesis** demonstrates real-world implementation trade-offs.
- **Stakeholder alignment**: Requirements gathering ensures ingestion frequency matches business needs (e.g., marketing analytics).

## Batch and Streaming Tools

### Batch Ingestion Tools

- **Popular AWS Tools**:
  - **AWS Glue ETL**: this service enables you to ingest data from various sources (such as Amazon RDS, Amazon S3, Amazon Redshift, Amazon DynamoDB, and [others](https://docs.aws.amazon.com/glue/latest/dg/glue-connections.html)), transforming it and then loading it into a destination. It performs an ETL job using Apache Spark (distributed processing engine) to distribute the transformation workloads across computing nodes. AWS Glue provides a serverless environment where you can create code-based solutions for both data ingestion and transformation. To learn more about the AWS Glue environment, see [AWS Glue Components](https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html) and AWS Glue ETL guidance.
  - **Amazon EMR**: Amazon EMR is a managed cluster platform that provides a simple way to run big data frameworks such as [Apache Hadoop](https://aws.amazon.com/elasticmapreduce/details/hadoop) and [Apache Spark](https://aws.amazon.com/elasticmapreduce/details/spark). These tools are useful for ingesting vast amounts of data from a database (petabyte-scale), transforming them at scale, and loading them into AWS data stores and databases. Amazon EMR can run in a serverless mode or in a provisioned mode where you specify the computing resources that are needed for your workload. To learn more about the details of Amazon EMR, you can read the [AWS documentation](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html).
  - **What is the difference between AWS Glue ETL and Amazon EMR?** Both services can be used to perform big data processing using the Apache Spark engine, but they differ in terms of the amount of management and configuration that you need to perform, as well as cost. AWS Glue requires less configuration and is typically more convenient. On the other hand, Amazon EMR provides more control over the computing and memory resources but requires more configuration knowledge. You will learn more about the distributed framework in the upcoming courses of this specialization.
  - **AWS DMS**: AWS Glue ETL and Amazon EMR are both tools that enable you to perform transformations at scale while ingesting data. If you don’t need to perform transformations while ingesting data, you can consider using AWS DMS (Data Migration Service). This service allows you to sync data from an existing database (on-premises or hosted on the AWS cloud) to another data store that exists within your data pipeline (such as Amazon S3 or a data warehouse). You can also use this service to migrate data from one database engine to a different database engine. It is available in serverless or provisioned modes. To learn more about this service, check out [the overview page of this service](https://aws.amazon.com/dms/).
- **Other AWS ingesting services**:
  - [**AWS Snow family**](https://aws.amazon.com/snow/): if your company wants to migrate its legacy on-premise system to the cloud, you might need to transfer massive amounts of data, sometimes 100 TB or more. It would be very slow and costly to migrate this data over the internet, so you might want to consider a transfer appliance instead. At the time of creation for these courses, AWS offers transfer appliances called Snowball and Snowcone that help you move data in and out of the AWS cloud.
  - [**AWS Transfer family**](https://aws.amazon.com/aws-transfer-family/): This is a service that enables you to transfer files into and out of Amazon S3 using common file transfer protocols such as SFTP and FTP protocols.
- **Other non-AWS ingestion tools**:
  - There are other ingestion tools provided by other vendors or open-source projects that allow you to set a target and source (could be from different cloud providers) and ingest data in various ways. These tools are known as connectors because they allow you to connect a particular source to a target system. Examples of such tools include: [Airbyte](https://airbyte.com/), [Matillion](https://www.matillion.com/support) and [Fivetran](https://www.fivetran.com/?r=0).

### Streaming Ingestion Tools

- In course 1, you read about two streaming platforms: Amazon Kinesis Data Streams and Amazon Managed Streaming for Apache Kafka (MSK). To quickly refresh your memory about those services, you can check the overview service page of each ([Kinesis](https://aws.amazon.com/kinesis/data-streams/) and [MSK](https://aws.amazon.com/msk/)). Later this week, we’ll get into more detail about these streaming platforms.

### Key Considerations for Batch vs Streaming Ingestion

- **Use cases**: Ask your stakeholders: "if you get data in real time, what actions can you perform that would be an improvement to getting the data periodically in batches?"
  - **Machine learning**: batch is an excellent approach for many common use cases, such as model training. Consider if stakeholders can benefit from continuous training and online prediction.
  - **Dashboards/Reporting**: What are the benefits of having a real-time dashboard over one that is updated daily or weekly? Consider how stakeholders will act on real-time data.
- **Latency**: Do you need millisecond real-time data ingestion? Or would a micro-batch approach work, accumulating and ingesting data, say, every minute?
- **Cost**: A streaming ingestion approach is typically not as straightforward as batch ingestion, and it can carry extra costs and complexities.
  - Will your streaming-first approach cost more in terms of time, money, maintenance, downtime, and opportunity cost than simply doing batch?
  - If you're using a streaming platform: does your team have the capability to manage it? Do you have the skills to fix errors propagating in an event system?
- **Existing/Available system**:
  - **Destination system**: If you ingest data in real time, can downstream storage systems handle the rate of data flow?
  - **Source system**: Are you getting data from a live production instance? If so, what’s the impact of your ingestion process on this source system? Streaming systems are the best fit for many data source types. For instance, in IoT applications, each sensor writes events or measurements to streaming systems as they happen. While you can connect to the streaming source to directly write data into a database, you might find that it is a better fit to use a streaming ingestion platform such as Amazon Kinesis or Apache Kafka.
- **Reliability/Availability**: Are your streaming pipeline and system reliable and redundant if infrastructure fails? Streaming services require high availability of compute resources. On the other hand, batch services don't need high availability.

I suggest you adopt true real-time streaming only after identifying a business use case that justifies the trade-offs against using batch.

**Note**: There are other use cases where you might need to perform both types of ingestions (same computations done on batch and streaming). For that, you can use ingestion frameworks, such as the lambda architecture discussed in the previous course, to handle both batch and streaming ingestion patterns.

# Batch Ingestion

## Conversation with a Marketing Analyst (Data Ingestion for Marketing Insights)

## Project Background

- **E-commerce company** expanding to new markets and increasing customer retention.
- **Marketing analyst** wants to explore external factors that may influence purchasing habits.
- **Goal**: Combine **external data** with **product sales data** for deeper insights.

## Stakeholder Requirements

- **Marketing analyst** interested in customers’ emotional states and how these may affect buying behavior.
- **Proposal**: Analyze **music listening trends** by region (using the **Spotify API**) to correlate with sales.

## Ingestion Strategy

- **Third-party API**: Spotify provides public endpoints for trending artists and listening patterns.
- **Batch ingestion**: Data engineer needs to retrieve data at scheduled intervals for analysis.
- **Storage considerations**: Final data store depends on analyst needs (e.g., reporting, dashboarding).

### ETL vs. ELT

- **ETL** (Extract, Transform, Load): Transform data before loading into a destination.
- **ELT** (Extract, Load, Transform): Load raw data first, then transform as needed.
- Choice depends on volume, processing tools, and analytical requirements.

## Next Steps

- Assess **Spotify API** details (authentication, rate limits, data format).
- Define **data schema** for combining music and sales data.
- Determine **transformation** requirements and finalize **batch ingestion** schedule.

## ETL vs. ELT

### Project Context

- **Marketing analysts** want to incorporate **external data** into sales analysis.
- **Historical trends** are the primary focus (not real-time or urgent).
- **Third-party API** usage necessitates **batch ingestion** due to request limits.

### Batch Processing Patterns

#### **ETL (Extract, Transform, Load)**

- **Historical context**: Originated in the 1980s/90s when storage and compute were limited.
- **Process**: Extract raw data, **transform** it in a staging area, then **load** into a data warehouse.
- **Key advantage**: Ensures data is stored in a well-structured format for efficient queries.
- **Key downside**: Potential loss of raw data details once transformed.

#### **ELT (Extract, Load, Transform)**

- **Emergence**: Became popular with **Cloud storage** and modern **data warehouses** (e.g., **Amazon S3**, **Redshift**, **Snowflake**).
- **Process**: Extract raw data and **load** it into storage first, then **transform** it on-demand.
- **Key advantage**: Retains **raw data** for future exploration; faster initial ingestion.
- **Key downside**: Risk of creating a **data swamp** if data is dumped without organization.

### Recommended Approach

- **ELT** offers flexibility for **exploratory analysis** where transformations are uncertain.
- Ensures analysts can revisit raw data for evolving needs without losing information.

## Summary of the Differences: ETL vs. ELT

|                                     |                                                                                                                                                              **ETL**                                                                                                                                                               |                                                                                                                                                      **ELT**                                                                                                                                                       |
| :---------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|             **History**             |                                              - In the 80’s and 90’s, data warehouse cost was very expensive (millions of dollars), so engineers wanted to be very intentional about the data they were about to load into the data warehouse</br>- Data volume was still manageable.                                               |                                                                 - Cloud data warehouse reduced the cost of storing and processing data significantly (from millions of dollars to hundreds/thousands of dollars only) </br>- Data volume exploded.                                                                 |
|     Processing (transformation)     | - Data is transformed into a predetermined format before it is loaded into a data repository. So, data engineers have to carefully model the data and transform it into this format. </br> - Transformations rely on the processing power of the processing tool that is used to ingest data (unrelated to the target destination) |                             - Raw data is loaded into the target destination. Then it is transformed just before analytics (Can be used with not well-defined data requests) </br> - Transformations rely on the processing power of the data repository, such as the data warehouse.                              |
|        **Maintenance time**         |                                                                                                                            If the transformation is found to be inadequate, data needs to be re-loaded.                                                                                                                            |                                                                               The original data is intact and already loaded and can be used when necessary for additional transformation: Less time required for data maintenance.                                                                                |
| **Load Time & transformation time** |                                                      Load time: it typically takes longer as it uses a staging area and system. </br>Transformation time: it depends on the data size, the transformation complexity and the tool that is used to perform the transformation.                                                      |            Load time: there is no transformation involved, the data is directly loaded into the destination system </br> Transformation time: it is typically faster because it relies on the processing power and parallelization of modern data warehouse </br>(generally considered more efficient)             |
|    **Flexibility (data types)**     |                                                                                                                                       ETLs are typically designed to handle structured data.                                                                                                                                       |                                                                                 ELT can handle all types of data: structured, unstructured, semi-structured. Once the data is loaded into the target system, you can transform it.                                                                                 |
|              **Cost**               |                                                                                               It depends on what ETL/ELT tool is used and to what target system the data is loaded. (And of course, it depends on the data volume).                                                                                                |                                                                                       It depends on what ETL/ELT tool is used and to what target system the data is loaded. (And of course, it depends on the data volume).                                                                                        |
|           **Scalability**           |                                             Nowadays, most of the cloud tools are scalable. However the challenge here is that if you have lots of data sources and lots of targets, you would need to put in lots of effort to manage the code and handle data from multiple sources                                              |                                                                                                      ELT uses the scalable processing power of the data warehouse to enable transformation on a large scale.                                                                                                       |
|     **Data quality/ security**      |                                                                                                            It ensures data quality by cleaning it first. Transformations can also include masking personal information.                                                                                                            | The data needs to be transferred first to the target system before transformations that enhance data quality or security are applied. </br>\*There’s a sub-pattern called EtLT where small t does not refer to business modeling but to transformation with limited scope (mask sensitive data, deduplicate rows). |

## REST API

### Amazon's API Mandate (2002)

- **Jeff Bezos** issued a company-wide mandate requiring all teams to use **service interfaces (APIs)** for communication and data exchange.
- **Problem addressed**: Inefficient and inconsistent data/service sharing between teams due to lack of standardized interfaces.
- **Solution**: APIs provided stable, predictable interfaces, enabling teams to share functionality regardless of internal system complexity.

#### Public-Facing Requirement

- APIs were required to be **designed for eventual public exposure**, laying the foundation for **Amazon Web Services (AWS)**.
- This approach influenced global companies to adopt APIs for internal and external data/service sharing.

### API Fundamentals

- **Definition**: A set of rules enabling programmatic communication/data exchange between applications via code.
- **Daily use examples**: Social media apps fetching data, e-commerce transactions with payment systems, and mobile app functionalities.
- **Data engineering applications**: Extracting data from web services, cloud platforms, and third-party providers using standardized requests/responses.

### REST APIs

- **Representational State Transfer (REST)**: Most common API type using **HTTP methods** (GET, POST, etc.) for communication.
- **Analogy**: Similar to web browsing—sending HTTP requests for specific resources (e.g., web pages) and receiving structured responses.
- Supports standardized formats (e.g., JSON/XML) for data exchange.

### Data Engineering Applications

- **Common scenario**: Extracting data from third-party platforms (e.g., **Spotify API**) for analytics pipelines.
- **Key features**: APIs provide metadata, authentication, error handling, and documentation to streamline integration.
- **Lab focus**: Hands-on practice with API requests/responses to prepare for real-world data extraction tasks.

# Streaming Ingestion

## Conversation with a Software Engineer (Setup for Product Recommender System)

### Data Source and Ingestion Mechanism

- **User activity data** is separated from internal system metrics in web server logs.
- **Kinesis Data Stream** chosen for real-time ingestion instead of Kafka (based on existing pipeline compatibility).
- **Upstream stakeholder**: Software engineer owns source system and handles event routing to the stream.

### Data Format and Message Rate

- **Message format**: JSON payload containing:
  - Session ID
  - Customer location
  - Product browsing activity (views, cart additions)
- **Message size**: ~100-500 bytes per event.
- **Throughput**: Up to **1,000 events/second** during peak (10,000 concurrent users).

#### Capacity Considerations

- Estimated **<1 MB/second** data volume, well within Kinesis capabilities.
- Horizontal scaling supported for demand fluctuations.

### Stream Configuration and Retention

- **Retention period**: One day to enable stream replay for error recovery.
- **Storage estimate**: ~100 GB/day at peak usage.
- **Deletion mechanism**: Automatic removal after retention period.

### Pipeline Integration and Use Cases

- **Real-time processing**: Immediate recommendations based on user activity.
- **Data persistence**: Inputs/outputs saved separately for later analysis.
- **Fault tolerance**: Stream replay capability mitigates pipeline failures.

## Streaming Ingestion Details

### Key Concepts

- **Message queues** operate in a first-in, first-out manner, deleting messages once consumed.
- **Event streaming platforms** store messages in an append-only log, allowing replay or reprocessing.

#### Message Queues

- Designed as buffers between producers and consumers.
- **FIFO** ensures the oldest messages are delivered first.
- Once consumed, messages are removed from the queue.

#### Event Streaming Platforms

- Retain messages for a configurable period, enabling replay.
- Multiple consumers can read the same data independently.
- **Producers** push messages to the platform, **consumers** pull them.

### Apache Kafka

- [Kafka Documentation (Only check the introduction section 1.1)](https://kafka.apache.org/081/documentation.html#introduction)
- [Gentle introduction into Kafka: Gently down the stream](https://www.gentlydownthe.stream/)
- **Open-source event streaming platform** with flexible data retention.
- Messages are stored in **topics**, which are subdivided into **partitions**.

#### Topics and Partitions

- **Topic**: A category or feed name for messages (e.g., fraud alerts, customer orders).
- **Partition**: A log for ordered, immutable sequences of messages. More partitions allow higher throughput.

#### Consumer Groups

- **Consumer group**: A set of consumers that share message consumption workload.
- Each partition is consumed by exactly one consumer in the group.
- Messages are delivered to only one consumer per group.

### Amazon Kinesis

- **Kinesis data streams** function similarly to Kafka topics.
- **Shards** in Kinesis serve the same role as partitions in Kafka.
- Enables real-time processing of streaming data for analytics and ingestion pipelines.

### Additional Ingestion Approaches

- **Web server logs**: Can act as event producers; messages flow to Kinesis or Kafka.
- **Continuous change data capture (CDC)**: Monitors database logs to stream updates into data pipelines.
- Ensures pipeline data remains synchronized with source systems.

## Kinesis Data Streams

- [Kinesis Data Streams: High-level architecture](https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#high-level-architecture)
- [Quotas and limits](https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html)

### Core Components

- **Producers**: Push data to specific streams (similar to Kafka topics).
- **Consumers**: Read data from streams via managed services (AWS Lambda, Apache Flink) or custom clients (KCL).
- **Streams**: Composed of **shards** that determine capacity and scaling.

### Shards and Capacity

- **Shards**: Basic throughput units for scaling streams.
- **Write capacity per shard**: Up to **1,000 records/second** or **1 MB/second**.
- **Read capacity per shard**: Up to **5 read operations/second** or **2 MB/second** total.
- Scaling requires adding shards or using **on-demand mode** for automatic scaling.

#### Capacity Planning

- Estimate required shards based on expected data size/rate.
- Use **provisioned mode** for predictable traffic to control costs.

### Operational Modes

#### On-Demand Mode

- Automatically scales shards based on traffic.
- Pay-as-you-go pricing model.
- Ideal for variable/unpredictable workloads (e.g., e-commerce platforms).

#### Provisioned Mode

- Fixed shard count set by users.
- Cost-effective for stable, predictable traffic.
- Requires manual scaling via resharding.

### Data Records and Partitioning

- **Record structure**: Includes **partition key**, sequence number, and data blob (BLOB).
- **Partition key**: Determines shard assignment (e.g., customer ID for transaction grouping).
- **Sequence number**: Assigned by Kinesis to maintain record order within shards.

### Consumer Configuration

- **Shared fan-out**: Multiple consumers share a shard’s read capacity (default).
- **Enhanced fan-out**: Dedicated 2 MB/s read capacity per consumer (avoids contention).
- **Downstream integrations**: Send data to S3 (via Firehose), other streams, or analytics services.

### Use Cases and Flexibility

- Supports multiple independent applications reading from the same stream.
- Enables complex workflows via stream chaining (output of one stream feeds another).
- Real-time use cases: Transaction processing, user activity tracking, and live analytics.

## What is Change Data Capture (CDC)?

### What is CDC?

Suppose you extracted and loaded data from a database into your storage system. After some time, you might need to update the data stored in your storage system to ensure that it is in-sync with the data in the source system. There are two strategies for this:

- Full snapshots or full load: in this approach, every time you want to update the data stored in your system, you ingest the entire data from your source system, replacing the old stored data with the new updated data. If your data is tabular, fully loading the data means that you delete all the old data from the stored table and extract all rows from the source table every time you need to update your stored data. This is a straight-forward approach that ensures the consistency between the data in the source system and the data stored in your data pipeline. However for high-volume data, it can take a long time to run and it can require lots of processing and memory resources. It is more suitable for cases where there’s no need for frequent data updates.
- Incremental (differential) load: in this approach, you only load updates and changes since the last read from the source systems. For example, when loading updates from a source database, you might utilize a last_updated_at column to identify the rows of data that have been updated since you last read from this source database, and then only load the updated data from these identified rows. While this approach is faster than the full load approach, especially for high-volume data, it might require more complex logic to implement. When working with databases, this process is known as Change Data Capture or (CDC). According to the book Fundamentals of Data engineering, “Change data capture (CDC) is a method for extracting each change event (insert, update, delete) that occurs in a database” and making it available for downstream systems.

### Use Cases for CDC

- CDC helps you synchronize data across different databases, supporting continuous database replication. For example, you might have a source PostgreSQL system that supports an application and you want to periodically or continuously ingest table changes into a data warehouse to enable analytics based on the most recent data. Or if you work in a hybrid company, you might need to use CDC to capture changes in on-premises databases and apply those changes to on-cloud databases.
- CDC helps you capture every historical change for auditing and other business purposes. For example, certain businesses are required to maintain complete historic information of their customer purchases for regulatory purposes, or to extract insights that allow businesses to improve.
- CDC enables microservices to track any change in the source database. For example, consider a microservice that manages purchase orders. When a new order is placed, you can use CDC to relay information to shipment service and customer service.

### Two approaches to CDC

- **Push**: This approach requires you to implement some sort of logic or process to capture changes in the source database. Then it relies on the source database to push any data updates to the target system when something changes in the source system. This method allows the target systems to be updated with the latest data in near real-time, but if you don't set this up properly, you risk losing data updates if the target systems are unreachable when the source systems try to push the changes.
- **Pull**: This approach requires the target systems to continuously poll the source database to check for changes and then pull in data updates when they happen. This method typically results in a lag before the target systems pull in any new data updates because the changes are usually batched between pull requests.

### CDC Implementation Patterns

There are several methods for how CDC can extract changes from databases.

- **Batch-oriented or query-based CDC (pull-based)**: In this approach, you query the database itself to identify if there has been a change in data. In the case of relational databases, this requires that the database has an additional column labeled as updated_at, last_updated or last_modified that helps you find all updated rows past a certain specified time. This process allows you to extract changes and incrementally update a target table. However, this approach can add computational overhead to the source system because target systems have to scan each row in the table to identify the last updated values.
- **Continuous or log-based CDC (pull-based)**: Instead of running periodic queries to get the table changes as a batch, you can treat each update to the database as an event using continuous CDC. This type of CDC relies on checking the database log. A database log records every change to the database sequentially (e.g., every create, update, delete) and is used in case of a failure to restore the database state. You can read the events from this log (by writing your own code or using a CDC tool such as Debezium) and send them to a streaming platform, such as Apache Kafka. This way, you can capture data changes in real-time without incurring any computational overhead or requiring the need for an extra column in the source databases.

![cdc-tool](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/cdc-tool.png)

- **Trigger-based CDC (push-based method)**: A trigger is a stored function that you can configure to run when a specific column changes. The triggers inform the CDC of the changes in the source databases and in this way it relieves the CDC from detecting changes. However, too many triggers can negatively impact the write performance of the source database.

### Tools for CDC

Feel free to read more about some of the common tools used to implement CDC

- [Debezium](https://debezium.io/)
- [AWS DMS](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html)
- [Kafka connect API](https://limadelrey.medium.com/kafka-connect-how-to-create-a-real-time-data-pipeline-using-change-data-capture-cdc-c60e06e5306a)
- [Airbyte log-based CDC](https://airbyte.com/solutions/database-replication)

## Summary: General Considerations for Choosing Ingestion Tools

When choosing an ingestion tool for your data systems, you should consider the characteristics of the data you're ingesting, as well as the reliability and durability of the ingestion tool.

### Characteristics of the data

**Note**: In the book “Fundamentals of data engineering”, Joe and Matt refer to the characteristics of the data as the data payload, which includes data kind (type and format), shape, size, schema and data types, and metadata.

- **Data type & structure**: You learned in course 1 that data in source systems could be structured, unstructured, or semi-structured. When deciding how to ingest data and what tool to choose, you need to understand the data type and structure (e.g. an image in PNG format) so that you can identify the appropriate ingestion tool and transformations you might need to apply later on.
- **Data volume**: For data volume, you need to consider two things:
  - Data size in bytes of the existing data that you need to ingest: In case of batch ingestion, you need to consider the size of the historical data that you need to ingest. Can you ingest the entire historical data in one big chunk? Depending on the network connection between the source system and the target system, it may be possible to transfer the historical data over the network, but if you have limited bandwidth then you may need to split the massive payload into chunks, which effectively reduces the size of the payload into smaller subsections. In case of streaming ingestion, you need to consider the message size. You must ensure that the streaming ingestion tool can handle the maximum expected message size. For example, Amazon Kinesis Data Streams supports a maximum message size of 1 MB, while Kafka defaults to this maximum size but can be configured to support a maximum data size of 20 MB or more.
  - The size of the future data that you may ingest with the same pipeline: how are you expecting the data to grow? What is the daily, monthly, or yearly growth of data? Considering the actual and future size helps you understand how to configure your tool and what cost to anticipate to ensure that your ingestion system meets the demands.
  - **Latency requirements**: When designing your pipeline, one of the stakeholder requirements that you need to consider is latency: how fast do stakeholders want to operate on data? What is the acceptable delay? Do they need to extract insights from the data one day after it is ingested, or do they need near real time insights? In other words, is it a batch scenario: where data needs to be ingested once a day, a week, a month. Or does the data need to be streamed from a streaming source continuously with the lowest delay possible (for instance, in milli-seconds)? To meet the latency requirement, you need to think about how quickly you need to process the ingested data once it reaches your pipeline and also understand how quickly the source data is generated. The velocity of the data will impact the tools (batch or streaming tools) you choose to ingest and process the data.
  - **Data quality**: Is the source data in good shape for immediate downstream use? What post-processing is required to serve it? Depending on the source systems the data might be incomplete, or contain inconsistent information, duplicates, or errors. If the data is not expected to be in good shape, then you may need to check the quality of the data ingested in order to fix any issues. Some ingestion tools can help you fill in missing values or detect/fix inconsistencies or invalid entries. You’ll learn more about quality checks in the upcoming course.
  - **Changes in schema**: schema changes (e.g. adding a new column, changing a column type, creating a new table, renaming a column) frequently occur in source systems and are generally out of your control. If you’re expecting these changes to happen frequently, then you might need to consider using ingestion tools that automatically detect schema changes. However, communication between you and the upstream stakeholders is as important as the automation that checks for schema changes.

### Reliability and Durability

Reliability and durability are two important considerations in the ingestion stage. Reliability means making sure that ingestion systems are performing their intended function properly. Durability means making sure that data isn’t lost or corrupted. If you design a reliable ingestion system, you will ensure the durability of the ingested data. For example, streaming systems such as IoT devices do not retain events indefinitely, so if you don’t correctly ingest its data, the data may be lost. Make sure to understand the characteristics of the source systems and the ingestion tools.

**Advice**: Evaluate the tradeoffs between the cost of losing data vs building an appropriate level of redundancy. For more information and consideration, please check out chapter 7 of Fundamentals of Data Engineering.

# Week 2 Quiz

## Questions

1. According to this week’s videos, which of the following statements about batch and streaming ingestion is true?
   1. Batch ingestion processes data in real-time as it is generated, while streaming ingestion processes data in large chunks at scheduled intervals. 
   2. Batch ingestion can only be used with time-bound data, while streaming ingestion can only be used with size-bound data.
   3. Batch ingestion involves imposing boundaries on a continuous stream of data and ingesting all the data within those boundaries as a single unit, while streaming ingestion involves ingesting events individually as they are generated.
   4. Batch ingestion is a more modern approach that has emerged with new technologies, while stream ingestion is the traditional method of processing data as events are generated.
2. Consider the following three use cases:
   - **Use case A**: A business analyst wants to analyze sales data once a month. 
   - **Use case B**: A supply-chain manager needs new log updates from the transactional database once a minute.
   - **Use case C**: A software engineer needs processed data from IoT sensors within milliseconds after it is generated to build a customer-facing analytics dashboard.

What is the most appropriate way to order these use cases along the continuum of data ingestion frequencies (i.e. label the use cases as batch, micro-batch, and streaming)?

   1. **Use case A**: micro-batch, **Use case B**: batch, **Use case C**: streaming
   2. **Use case A**: micro-batch, **Use case B**: streaming, **Use case C**: batch
   3. **Use case A**: batch, **Use case B**: micro-batch, **Use case C**: streaming
   4. **Use case A**: streaming, **Use case B**: batch, **Use case C**: micro-batch

3. Which of the following statements is/are true about the Extract-Transform-Load (ETL) ingestion pattern? Select all that apply.
   1. Transformation is performed in an intermediate staging area.
   2. No information gets lost in the process.
   3. You can end up with what’s known as a data swamp.
   4. You transform data before loading it into the target storage destination.
   5. You don’t have to decide up front how you want to use the data.
4. Which of the following scenarios is/are appropriate for the Extract-Transform-Load (ETL)ingestion pattern? Select all that apply.
   1. Quickly providing large amounts of raw transactional data to an analyst who would like to explore the data. 
   2. Migrating data from a legacy system to a target database, where the data in the legacy system is not in a format that's compatible with the structure of the target database.
   3. Loading data into a target system, where the end users requested that the data be free of errors, duplicates, and inconsistencies.
5. What does REST in REST API stand for?
   1. Representational Symbol Transform
   2. Representational State Transform
   3. Representational Symbol Transfer
   4. Representational State Transfer
6. How can you send requests to REST APIs?
   1. Using the SELECT, ADD, REMOVE, and PATCH methods
   2. Using the CREATE, READ, UPDATE, and DELETE operations
   3. Using the POST, PUT, GET, and DELETE HTTP methods
   4. Using the SEND, RESPOND, CONNECT, and AUTHORIZE operations
7. Which of the following statements accurately describes Kafka topics and Kafka partitions? 
   1. Events are split up into Kafka partitions, where each partition has one or more Kafka topics.
   2. Events are split up and routed into topics, where each topic has one or more partitions.
   3. A Kafka topic is an ordered immutable sequence of Kafka partitions.
   4. Kafka topics and Kafka partitions are interchangeable terms.
8. Which of the following statements about Amazon Kinesis Data Streams and Apache Kafka is true?
   1. The parallel to a Kafka topic is a Kinesis stream. The parallel to a Kafka partition is a Kinesis shard.
   2. The parallel to a Kafka broker is a Kinesis stream. The parallel to a Kafka cluster is a Kinesis shard.
   3. The parallel to a Kafka cluster is a Kinesis stream. The parallel to a Kafka topic is a Kinesis shard.
   4. The parallel to a Kafka topic is a Kinesis shard. The parallel to a Kafka partition is a Kinesis stream.
9. True or False: Once a consumer reads a message from a Kafka topic, the message is deleted immediately.
   1.  True
   2.  False

## Answers

1. 3
2. 3
   1. As you go from batch to streaming ingestion along the data ingestion continuum, you are increasing the ingestion frequency.
3. 1 & 4
   1. With ETL, you transform the data in an intermediate staging area before loading it into a target storage destination.
   2. With ETL, transformation happens before loading data into a target system.
4. 2 & 3
   1. This is an ETL use case because the data needs to be transformed into a format that's compatible with the target database before it can be loaded into the target database.
   2. This is an ETL use case because the data needs to be cleaned (i.e. transformed) before it is loaded into the target system.
5. 4
   1. It means that the server returns a representation of the state of the requested object.
6. 3
   1. **GET**: Retrieves data from the server.
   2. **POST**: Submits data to the server to create a new resource.
   3. **PUT**: Updates an existing resource on the server.
   4. **DELETE**: Removes a resource from the server.
7. 2
   1. Within a Kafka cluster, message streams are split up and routed into topics. Each topic has one or more partitions, which are just ordered, immutable sequences of messages that you continually add new messages to.
8. 1
9. 2
   1. The Kafka cluster retains the message for a configurable period of time, whether or not the message was consumed. This allows consumers to replay and reprocess messages as needed.
