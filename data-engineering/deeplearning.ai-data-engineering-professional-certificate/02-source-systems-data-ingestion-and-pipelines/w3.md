- [DataOps - Automation](#dataops---automation)
  - [Overview](#overview)
    - [Three Pillars of DataOps](#three-pillars-of-dataops)
      - [Automation](#automation)
      - [Observability and Monitoring](#observability-and-monitoring)
      - [Incident Response](#incident-response)
    - [Infrastructure as Code (IaC) in Practice](#infrastructure-as-code-iac-in-practice)
    - [Industry Expert Insights](#industry-expert-insights)
    - [Course Week Structure](#course-week-structure)
  - [Conversation with Chris Bergh](#conversation-with-chris-bergh)
    - [Introduction to Chris Berg](#introduction-to-chris-berg)
    - [What is DataOps?](#what-is-dataops)
      - [Key Challenges in Data Engineering](#key-challenges-in-data-engineering)
    - [Core Principles of DataOps](#core-principles-of-dataops)
      - [Automation and Testing](#automation-and-testing)
      - [Observability and Monitoring](#observability-and-monitoring-1)
      - [Incident Response and Team Culture](#incident-response-and-team-culture)
    - [DataOps vs. DevOps](#dataops-vs-devops)
    - [The DataOps Manifesto](#the-dataops-manifesto)
    - [Lessons for Aspiring Data Engineers](#lessons-for-aspiring-data-engineers)
    - [Final Thoughts](#final-thoughts)
  - [DataOps Automation](#dataops-automation)
    - [Overview of Automation in DataOps](#overview-of-automation-in-dataops)
    - [Continuous Integration and Continuous Delivery (CI/CD)](#continuous-integration-and-continuous-delivery-cicd)
      - [Pipeline Automation Options](#pipeline-automation-options)
    - [Version Control in DataOps](#version-control-in-dataops)
    - [Infrastructure as Code (IaC)](#infrastructure-as-code-iac)
      - [Key Use Cases](#key-use-cases)
    - [Overlap with Other Data Engineering Undercurrents](#overlap-with-other-data-engineering-undercurrents)
  - [Infrastructure as Code (IaC) with Terraform and AWS](#infrastructure-as-code-iac-with-terraform-and-aws)
    - [Evolution and Key Concepts](#evolution-and-key-concepts)
    - [Key IaC Tools](#key-iac-tools)
      - [Terraform vs. CloudFormation](#terraform-vs-cloudformation)
    - [Terraform Configuration with HCL](#terraform-configuration-with-hcl)
    - [Declarative Approach and Idempotency](#declarative-approach-and-idempotency)
      - [Imperative vs. Declarative](#imperative-vs-declarative)
    - [Cross-Cloud Capabilities](#cross-cloud-capabilities)
  - [Terraform - Creating an EC2 Instance](#terraform---creating-an-ec2-instance)
    - [Terraform Workflow](#terraform-workflow)
      - [Commands](#commands)
    - [Configuration Files](#configuration-files)
      - [Terraform and Provider Blocks](#terraform-and-provider-blocks)
    - [Resource Blocks](#resource-blocks)
    - [Example EC2 Creation](#example-ec2-creation)
  - [Terraform - Defining Variables and Outputs](#terraform---defining-variables-and-outputs)
    - [Creating Input Variables](#creating-input-variables)
      - [Using .tfvars Files](#using-tfvars-files)
    - [Exporting Resource Attributes](#exporting-resource-attributes)
    - [Organizing Configuration Files](#organizing-configuration-files)
  - [Terraform - Defining Data Sources and Modules](#terraform---defining-data-sources-and-modules)
    - [Data Blocks](#data-blocks)
      - [Referencing Existing Subnets](#referencing-existing-subnets)
      - [Automating AMI Selection](#automating-ami-selection)
    - [Modules](#modules)
      - [Calling a Module](#calling-a-module)
    - [Applying Changes](#applying-changes)
  - [Additional Terraform Configuration Example - Exercise](#additional-terraform-configuration-example---exercise)
    - [Introduction](#introduction)
    - [Hints:](#hints)
    - [Solutions:](#solutions)
  - [Note regarding Practice Lab 1](#note-regarding-practice-lab-1)
  - [Lab Walkthrough - Implementing DataOps with Terraform](#lab-walkthrough---implementing-dataops-with-terraform)
    - [Bastion Host Architecture Overview](#bastion-host-architecture-overview)
      - [Security Groups](#security-groups)
    - [Lab Setup with Terraform](#lab-setup-with-terraform)
      - [Terraform Configuration Files](#terraform-configuration-files)
    - [Resource Configuration Details](#resource-configuration-details)
      - [RDS Database](#rds-database)
      - [EC2 Bastion Host](#ec2-bastion-host)
    - [Terraform State Management](#terraform-state-management)
    - [Outputs and Connectivity](#outputs-and-connectivity)
- [DataOps - Observability](#dataops---observability)
  - [Data Observability in DataOps](#data-observability-in-dataops)
    - [Importance of Observability](#importance-of-observability)
    - [Understanding Data Quality](#understanding-data-quality)
      - [Potential Failures](#potential-failures)
    - [Consequences of Low Quality Data](#consequences-of-low-quality-data)
    - [Monitoring and Detection](#monitoring-and-detection)
  - [Conversation with Barr Moses](#conversation-with-barr-moses)
    - [Introduction to Data Observability](#introduction-to-data-observability)
      - [Common Pain Points](#common-pain-points)
    - [Key Metrics](#key-metrics)
      - [Success Stories](#success-stories)
    - [Stakeholder Engagement](#stakeholder-engagement)
      - [Continuous Learning and Advice](#continuous-learning-and-advice)
  - [Conversation Takeaways \& Additional Notes](#conversation-takeaways--additional-notes)
    - [Key Takeaways From the Conversation with Barr Moses](#key-takeaways-from-the-conversation-with-barr-moses)
    - [Barr Moses 5 Pillars](#barr-moses-5-pillars)
    - [Additional Resources](#additional-resources)
  - [Monitoring Data Quality](#monitoring-data-quality)
    - [Stakeholder-Driven Focus](#stakeholder-driven-focus)
    - [Key Quality Criteria](#key-quality-criteria)
      - [Schema and Format Consistency](#schema-and-format-consistency)
    - [Tools and Best Practices](#tools-and-best-practices)
  - [Conversation with Abe Gong](#conversation-with-abe-gong)
    - [Origins of Great Expectations](#origins-of-great-expectations)
    - [Understanding Data Quality](#understanding-data-quality-1)
    - [Great Expectations in Practice](#great-expectations-in-practice)
      - [Implementation Advice](#implementation-advice)
    - [Collaboration and Scaling](#collaboration-and-scaling)
    - [Key Takeaways for Learners](#key-takeaways-for-learners)
  - [Great Expectations Workflow Overview](#great-expectations-workflow-overview)
    - [Core Components](#core-components)
      - [Data Assets and Batches](#data-assets-and-batches)
    - [Validation Process](#validation-process)
      - [Stores](#stores)
    - [Typical Workflow Steps](#typical-workflow-steps)
  - [Conversation with Chad Sanderson on Data Contracts](#conversation-with-chad-sanderson-on-data-contracts)
    - [Definition and Purpose](#definition-and-purpose)
    - [The Need for Data Contracts](#the-need-for-data-contracts)
    - [Key Components of Data Contracts](#key-components-of-data-contracts)
      - [Structural Elements](#structural-elements)
      - [Content and Enforcement](#content-and-enforcement)
    - [Implementation Strategies and Challenges](#implementation-strategies-and-challenges)
      - [Common Pitfalls](#common-pitfalls)
    - [Benefits and Recommendations](#benefits-and-recommendations)
  - [Monitoring with Amazon CloudWatch](#monitoring-with-amazon-cloudwatch)
    - [Overview of Monitoring](#overview-of-monitoring)
    - [CloudWatch Features](#cloudwatch-features)
      - [Custom Metrics](#custom-metrics)
      - [Dashboards and Visualization](#dashboards-and-visualization)
      - [Alarms and Baselines](#alarms-and-baselines)
    - [Key RDS Metrics to Monitor](#key-rds-metrics-to-monitor)
    - [Additional Considerations](#additional-considerations)
- [Incident Response](#incident-response-1)
- [Week 3 Quiz](#week-3-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# DataOps - Automation

## Overview

- **DataOps** combines practices from software engineering and data management to build robust data systems and deliver high-quality data products.
- Evolved from **DevOps**, focusing on cultural habits and efficient delivery of software/data products.
- Three core pillars: **automation**, **observability and monitoring**, and **incident response**.

### Three Pillars of DataOps

#### Automation

- Revisits **continuous integration/continuous delivery (CI/CD)** concepts.
- Emphasizes **infrastructure as code (IaC)**: Writing code to deploy data pipeline resources programmatically.
- Replaces manual resource provisioning (e.g., AWS console) with tools like **Terraform** and **AWS CloudFormation**.

#### Observability and Monitoring

- Focuses on tracking **data quality** and system performance metrics.
- Includes hands-on labs for implementing monitoring in data pipelines.
- Supported by industry tools for **data observability**.

#### Incident Response

- Primarily cultural rather than technical, involving team collaboration and rapid problem-solving.
- Less emphasized in labs due to challenges in simulating cultural practices online.

### Infrastructure as Code (IaC) in Practice

- **Key tools**: **Terraform** and **AWS CloudFormation** automate resource deployment.
- **Lab focus**: Students write Terraform code to deploy infrastructure, building on prior IaC exposure in earlier labs.
- **Benefits**: Reduces manual errors, ensures consistency, and enables scalable data pipeline setups.

### Industry Expert Insights

- Interviews with **dataops professionals** highlight real-world applications of data quality and observability.
- **Chris Berg** (CEO of Data Kitchen, **DataOps Manifesto** co-author) discusses DataOps definitions and importance.

### Course Week Structure

- **Kickoff**: Interview with Chris Berg on DataOps fundamentals.
- **Automation deep dive**: IaC implementation and CI/CD principles.
- **Observability labs**: Extending infrastructure to monitor data quality metrics.
- **Progression**: Combines theoretical concepts with practical Terraform and monitoring exercises.

## Conversation with Chris Bergh

### Introduction to Chris Berg

- **Chris Berg**: CEO of **Data Kitchen**, an expert in **DataOps** with a background in engineering and data management.
- Worked at organizations like **MIT**, **NASA**, and internet startups before focusing on data engineering.
- Developed **DataOps** practices to address challenges in delivering high-quality data products efficiently.

### What is DataOps?

- **Definition**: A methodology to deliver trusted data insights quickly and with low risk.
- **Goal**: Build a "factory" for producing reliable datasets and insights that can adapt to changes rapidly.
- Inspired by **lean manufacturing** principles, treating data pipelines as a production line.

#### Key Challenges in Data Engineering

- **Customer demands**: High expectations for speed and quality.
- **Infrastructure complexity**: Data systems are hard to manage and prone to breaking.
- **Team dynamics**: Misaligned management and team structures often lead to inefficiencies.

### Core Principles of DataOps

#### Automation and Testing

- **Automation**: Reduces manual effort and errors in data pipelines.
- **Testing**: Essential for validating data quality and ensuring reliability.
  - **Tests as a gift**: Future-proofing your work by creating reusable validation checks.

#### Observability and Monitoring

- **Observability**: Tracking data quality and system performance to identify issues early.
- **Monitoring**: Ensures that data pipelines are functioning as expected and alerts teams to problems.

#### Incident Response and Team Culture

- **Incident response**: Focuses on rapid problem-solving and collaboration.
- **Cultural habits**: Encourages teams to avoid "hero mode" and instead build sustainable systems.

### DataOps vs. DevOps

- **Similarities**: Both focus on delivering high-quality outputs quickly through automation, testing, and short delivery cycles.
- **Differences**:
  - **DataOps** specifically addresses the challenges of data pipelines and analytics.
  - **DevOps** is more general, applying to software development and deployment.

### The DataOps Manifesto

- **Origin**: Created to define and promote **DataOps** practices.
- **Purpose**: Provides a framework for managing data engineering teams effectively.
- **Key focus**: Enabling new team members to quickly identify and fix issues, and deploy changes with low risk.

### Lessons for Aspiring Data Engineers

- **Avoid heroism**: Build systems that reduce the need for constant intervention.
- **Don’t rely on hope**: Measure and validate everything to ensure reliability.
- **Focus on customer success**: Prioritize delivering value over simply completing tasks.

### Final Thoughts

- **DataOps** is about creating systems that allow teams to deliver high-quality data products efficiently.
- By adopting **DataOps** principles, data engineers can reduce stress, improve job satisfaction, and avoid burnout.

## DataOps Automation

### Overview of Automation in DataOps

- **DataOps** borrows heavily from **DevOps** in terms of automation practices.
- Focuses on delivering high-quality data products through automated processes.
- Key concepts include **continuous integration/continuous delivery (CI/CD)**, **version control**, and **infrastructure as code (IaC)**.

### Continuous Integration and Continuous Delivery (CI/CD)

- **CI/CD**: Automates the review, testing, and deployment of code and data in pipelines.
- **Application in DataOps**:
  - Applies to code for data transformations, database population, and the data itself.
  - Ensures data pipelines are reliable and can be updated efficiently.

#### Pipeline Automation Options

- **Manual execution**: Running pipeline processes without automation (not recommended).
- **Scheduled execution**: Running pipeline stages at predefined intervals.
- **Orchestration**: Using tools like **Airflow** to define pipelines as **directed acyclic graphs (DAGs)** for automated execution.

### Version Control in DataOps

- **Version control**: Tracks changes in code and data, enabling rollbacks to previous versions if issues arise.
- **Data versioning**: Allows tracking changes in data pipelines, similar to code versioning.
- **Tools**: Platforms like **GitHub** are used for version control in both code and data.

### Infrastructure as Code (IaC)

- **Definition**: Programmatically defining and managing infrastructure using code.
- **Benefits**:
  - Enables version control for infrastructure.
  - Simplifies deployment and updates by running code to deploy or modify infrastructure.
  - Facilitates rollbacks to previous infrastructure versions if needed.

#### Key Use Cases

- **Cloud platform resources**: Managing cloud infrastructure for data pipelines.
- **Automation**: Deploying and updating infrastructure without manual intervention.

### Overlap with Other Data Engineering Undercurrents

- **Software engineering**: DataOps practices like CI/CD and IaC are borrowed from DevOps.
- **Data management**: Version control for data enhances data lifecycle management and value delivery.

## Infrastructure as Code (IaC) with Terraform and AWS

- [Terraform Documentation](https://developer.hashicorp.com/terraform/docs)
- [AWS CloudFormation (User Guide)](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html)

### Evolution and Key Concepts

- **Infrastructure as Code (IaC)** enables programmatic definition and deployment of cloud resources (networking, security, storage, etc.).
- Roots trace back to 1970s configuration management using scripts like BASH.
- **Amazon EC2 (2006)** popularized on-demand cloud computing, driving need for scalable IaC tools.

### Key IaC Tools

- **Terraform**: Cross-cloud tool using **HCL (HashiCorp Configuration Language)** for declarative infrastructure management.
- **AWS CloudFormation**: Native AWS service for provisioning resources via JSON/YAML templates.
- Both used in labs to automate **Amazon VPC**, **RDS databases**, **S3 buckets**, and **Glue ETL jobs**.

#### Terraform vs. CloudFormation

- **Terraform**: Cloud-agnostic, supports multiple providers (AWS, GCP, Azure).
- **CloudFormation**: AWS-specific, tightly integrated with AWS services.

### Terraform Configuration with HCL

- **Resource blocks** define components (e.g., `resource "aws_s3_bucket" "data_lake"`).
- Follows pattern: `resource "[provider]_[resource_type]" "[name]" { configurations }`.
- **Example**: S3 bucket creation with unique naming and public access settings.

### Declarative Approach and Idempotency

- **Declarative language**: Define **desired end state**; Terraform handles execution steps.
- **Idempotent operations**:
  - Creates missing resources, updates mismatched ones, or leaves existing matches unchanged.
  - Prevents duplicate resources (e.g., multiple EC2 instances) when re-run.

#### Imperative vs. Declarative

- **Imperative (BASH scripts)**: Require explicit commands for each step.
- **Declarative (HCL)**: Focus on outcome, not process.

### Cross-Cloud Capabilities

- **Terraform** supports multi-cloud configurations (e.g., AWS VPC and GCP Compute Engine in same file).
- Syntax consistency across providers simplifies multi-cloud management.

## Terraform - Creating an EC2 Instance

### Terraform Workflow

- **Infrastructure as Code**: Write configuration files to define resources.
- **Consistent Process**: **Write** configs → **Initialize** workspace → **Plan** changes → **Apply** to create, update, or destroy resources.
- **Default VPC**: Generally used for quick experimentation, but custom VPCs are recommended for production.

#### Commands

- **Terraform init**: Installs required provider plugins (e.g., **AWS** provider).
- **Terraform plan**: Generates an execution plan showing what will be created, updated, or destroyed.
- **Terraform apply**: Provisions the infrastructure; prompts for approval before execution.

### Configuration Files

- **Five Sections**: **Terraform settings**, **providers**, **resources**, optional **input variables**, and optional **output values**.
- **Any .tf file**: Recognized by **Terraform** as a configuration file.
- **IDE Choice**: You can use VS Code or any editor; ensure **Terraform** is installed and **AWS** credentials are set.

#### Terraform and Provider Blocks

- **Terraform block**: Specifies **required providers** and **required version** of **Terraform**.
- **Provider block**: Configures details like **AWS region** (`provider "aws" { region = "us-east-1" }`).
- **AWS provider**: A **plugin** that enables Terraform to interact with **Amazon Web Services** resources.

### Resource Blocks

- **resource "aws_instance" "web_server"**: Creates an **Amazon EC2** instance.
- **Arguments**: **AMI** (e.g., Linux-based AMI) and **instance_type** (e.g., `t2.micro`) are required.
- **Optional Fields**: Tags, subnet, security groups, etc.
- **Reference Resources**: Use `aws_instance.web_server` in other parts of the config.

### Example EC2 Creation

- **Configuration**: Minimal example includes **terraform**, **provider**, and **resource** blocks.
- **Tags**: Provide metadata like `Name = "example server"`.
- **Check**: Use **Terraform plan** to see planned changes before **apply**.

## Terraform - Defining Variables and Outputs

### Creating Input Variables

- **Parametrize configuration**: Replace hard-coded values (e.g., AWS region, server name) with **input variables** for flexibility.
- **Variable declaration**: Use the `variable "name" {}` block.
- **Optional arguments**: **description**, **type**, and **default**.
- **Reference variables**: Access using `var.<variable_name>` in other blocks.

#### Using .tfvars Files

- **Avoid prompts**: Store variable values in a file ending with **.tfvars** (e.g., `terraform.tfvars`).
- **Command line flag**: Alternatively, use `--var "variable_name=value"` to pass values directly.

### Exporting Resource Attributes

- **Resource attributes**: Each **EC2 instance** or other resource has attributes like **id**, **ARN** (Amazon Resource Name), and **public IP**.
- **Output values**: Declare with `output "identifier" { value = <resource_attribute> }`.
- **Terraform output**: Retrieves all or specific output values after **apply**.

### Organizing Configuration Files

- **Multiple .tf files**: Split main configuration into `variables.tf`, `outputs.tf`, `providers.tf`, and resource files to improve maintainability.
- **Automatic concatenation**: **Terraform** merges all `.tf` files in the same directory.
- **Better practice**: Keep related configuration blocks (variables, outputs, providers, resources) logically separated to simplify updates and collaboration.

## Terraform - Defining Data Sources and Modules

### Data Blocks

- **Data sources**: Let Terraform read external or pre-existing resources (e.g., VPCs, subnets, AMIs).
- **Syntax**: `data "<provider_resource_type>" "<data_source_name>" { ... }`.
- **Attributes**: Access using `data.<provider_resource_type>.<data_source_name>.<attribute>`.

#### Referencing Existing Subnets

- **Example**: `data "aws_subnet" "selected_subnet"` with `id` specified.
- **Usage**: In an **Amazon EC2** resource block, set `subnet_id` to `data.aws_subnet.selected_subnet.id`.

#### Automating AMI Selection

- **Data source**: Searches for the latest **AWS** Linux AMI owned by **Amazon**.
- **Usage**: Set the EC2 `AMI` to `data.aws_ami.latest_linux.id` instead of a hard-coded ID.

### Modules

- **Definition**: A **module** is a subdirectory grouping related resources (e.g., a website module for server resources).
- **Files**: Like the root directory, it includes **Terraform** settings, variables, and outputs.

#### Calling a Module

- **Module block**: Declared in the root directory with `module "<module_name>" { source = "./<module_directory>" ... }`.
- **Inputs**: Pass root variables to module variables (e.g., `server_name`).
- **Outputs**: To export module outputs to the root directory, reference them via `module.<module_name>.<output_name>`.

### Applying Changes

- **Before running**: Perform `terraform init` when adding or modifying **module blocks**.
- **After changes**: Use `terraform apply`, type **yes**, and wait for resource updates or creation.

## Additional Terraform Configuration Example - Exercise

### Introduction

You learned how you can create an EC2 instance using Terraform. In this optional exercise, you’ll create a database instance in Terraform. The goal of this exercise is for you to practice using Terraform and its documentation.
You are given the ID of a VPC created in the us-east-1 region. You will need to create a MySQL RDS database instance inside a subnet of the given VPC. The VPC contains two private subnets; the IDs of these subnets are not given to you.

Here are the specifications of the database:

- **username**: should be defined as a variable with a default value of “admin_user”
- **password**: should be defined as a variable. Its value is specified in the tfvars file.
- **port number**: 3306
- **database instance class**: db.t3.micro (this is to determine the [memory and computation capacity of the database](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html))
- **allocated storage**: 10 GiB

From this database instance, you want to return the database hostname, username, password and port number as output values.

Here are some useful links:

- Resource [aws_db_instance](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_instance), list of its [arguments](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_instance#argument-reference) and [attributes](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_instance#attribute-reference).
- Resource [aws_db_subnet_group](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_subnet_group), list of its [arguments](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_subnet_group#argument-reference) and [attributes](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/db_subnet_group#argument-reference).
- Data source [aws_subnets](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/subnets), list of its [attributes](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/subnets#attribute-reference).

How should you replace the question marks in these configuration files?

![terraform-config-files-question](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/terraform-config-files-question.png)

### Hints:

- If you look at the arguments of aws_db_instance, you will notice that you need to specify a name for a subnet group when you don’t want the database to be launched in the default VPC. In Terraform, you can create the subnet group as another resource block in the main file and then use its attributes in the database resource block. The subnet group should consist of at least two subnet ids, and each subnet should reside in a different availability zone. This is an AWS requirement (for more info, check [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html#Overview.RDSVPC.Create)). You can assume that the given VPC contains two subnets where each subnet belongs to a different availability zone.
- In the main file, the first data block (of type aws_subnets) gets you the subnet IDs of the given VPC.

### Solutions:

When you're ready, check out the solution provided below

![terraform-config-files-answer](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/terraform-config-files-answer.png)

## Note regarding Practice Lab 1

In the previous lab, you learned that you can connect to the database from an external environment through the bastion host using an SSH connection. But since the current lab environment does not support an SSH connection, you connected directly to the bastion host using the console. This optional note is to explain to you how the SSH connection would look like in this case. Please note that running these commands in the provided lab environment does not work.

An SSH tunnel is a secure way to transfer data between two devices over an encrypted connection. It works by establishing an SSH connection and then creating a secure conduit for transmitting data. There are two types: local port forwarding and remote port forwarding. SSH tunnels are used for secure access to services, bypassing firewalls, and secure browsing. Here's how the command looks like:

`ssh -i de-c2w3lab1-bastion-host-key.pem -L 5432:<RDS-HOST>:<DATABASE-PORT>  ec2-user@<BASTION-HOST-DNS> -N -f`

- The `ssh` command uses the private key to authenticate the connection to the bastion host;
- The `-i` tag means `identity_file` and it specifies the file that contains the SSH private key;
- The `L` options means local port forwarding (`-L LOCAL_PORT:DESTINATION_HOSTNAME:DESTINATION_PORT USER@SERVER_IP`): this means that you're forwarding the connection from the bastion host to the RDS instance.
- The `-f` option means the command can be run in the background and `-N` means "do not execute a remote command

Once the tunnel is configured, the RDS can be accessed as follows:

`psql -h localhost -U postgres_admin -p 5432 -d postgres --password`

If you'd like to learn more about SSH tunneling, you can check this [article](https://linuxize.com/post/how-to-setup-ssh-tunneling/#remote-port-forwarding).

## Lab Walkthrough - Implementing DataOps with Terraform

### Bastion Host Architecture Overview

- **Bastion host (jump server)**: Bridges public internet to private network resources via **SSH connections**.
- **Key components**:
  - **RDS database** in private subnet.
  - **EC2 instance** in public subnet (bastion host).
  - **VPC** with pre-configured public/private subnets (created via **CloudFormation**).

#### Security Groups

- **Bastion host security group**: Allows SSH traffic from public internet.
- **RDS security group**: Restricts traffic to only the bastion host.

### Lab Setup with Terraform

- **Pre-existing resources**: VPC and subnets are imported via **data blocks** in Terraform.
- **SSH key pair**:
  - **Public key** stored in EC2 instance.
  - **Private key** saved locally for authentication (simulated due to lab constraints).

#### Terraform Configuration Files

- **Module structure**: Organized into `variables`, `outputs`, `providers`, and resource-specific files (EC2, RDS, network).
- **Utility providers**:
  - **Random**: Generates database password.
  - **TLS**: Creates SSH key pair.
  - **Local**: Stores private key file.
- **Variables**: Define AWS region, VPC/subnet IDs, and database credentials (assigned via `tf.vars`).

### Resource Configuration Details

#### RDS Database

- Requires **two private subnets** for potential multi-AZ deployment.
- Configuration includes engine type (PostgreSQL), instance class, security groups, and auto-generated credentials.

#### EC2 Bastion Host

- Uses **Amazon Machine Image (AMI)** and predefined instance type.
- Associates SSH key pair via `aws_key_pair` resource.

### Terraform State Management

- **State file (`terraform.tfstate`)**: Tracks infrastructure state locally (lab setup) or remotely (e.g., S3 for team collaboration).
- **Backend configuration**: Defined in `backend` file for state storage path.

### Outputs and Connectivity

- **Exported outputs**: Database host, port, credentials, and bastion host DNS (used for connection).
- **SSH syntax**: Provided for external environments (simulated in lab).

# DataOps - Observability

## Data Observability in DataOps

- [What is data observability?, by Andy Petrella [book]](https://learning.oreilly.com/library/view/what-is-data/9781098120993/ch02.html#characteristics_of_a_data_incident)
- [Driving data quality with data contracts, by Andrew Jones [book]](https://learning.oreilly.com/library/view/driving-data-quality/9781837635009/B19790_02.xhtml#_idParaDest-34)
- [Data Contracts: Why Thought Leaders Opt for Ounces Over Pounds, by Chad Sanderson ](https://www.gable.ai/blog/data-contracts)

### Importance of Observability

- **DataOps** incorporates **observability and monitoring** practices originally developed for distributed software systems.
- Common **observability tools** can track metrics like CPU usage and response times, helping prevent downtime and ensuring reliability.
- In **data engineering**, observability extends beyond system performance to include **data quality**.

### Understanding Data Quality

- **High Quality Data**: Accurate, complete, discoverable, and timely; matches well-defined schemas and stakeholder expectations.
- **Low Quality Data**: Inaccurate, incomplete, or otherwise unusable. Worse than no data at all because it can lead to wrong decisions.

#### Potential Failures

- **Obvious failures**: System crashes or 404 errors are visible, prompting immediate alerts.
- **Subtle failures**: Data pipelines may continue running but produce **incorrect** outputs (e.g., currency changes causing unexpected revenue shifts).

### Consequences of Low Quality Data

- **Business Impact**: Incorrect analytics or dashboards can mislead decision-makers, harming trust in the **data team**.
- **Organizational Cost**: Wasted time and resources on emergency responses and stakeholder confusion.
- **Shared Responsibility**: Even if changes originate from upstream systems, data engineers must **mitigate** disruptions.

### Monitoring and Detection

- **Data Observability**: Enables early detection of anomalies, preventing hidden data errors.
- **Immediate Notification**: Alerts ensure teams address quality issues before they significantly impact the business.
- **Next Steps**: Further exploration of data observability tools can enhance data reliability and stakeholder confidence.

## Conversation with Barr Moses

### Introduction to Data Observability

- **Data Observability**: Ensures visibility into the health and reliability of data systems.
- **Data Downtime**: Periods when data is inaccurate, delayed, or otherwise unusable.
- **Corollary to Software Observability**: Similar to tracking uptime and reliability in software products.

#### Common Pain Points

- **Wrong Data in Dashboards**: Surprises business stakeholders and disrupts decision-making.
- **Root Cause Complexity**: Ingested data, transformation code, or infrastructure failures can cause issues.
- **High Stakes**: Inaccurate data can impact revenue, brand reputation, and operational efficiency.

### Key Metrics

- **Number of Incidents**: Frequency of data quality issues (e.g., daily or weekly).
- **Time to Detection**: How quickly the team identifies and acknowledges a data problem.
- **Time to Resolution**: Speed of diagnosing and fixing the root cause.

#### Success Stories

- **JetBlue**: Boosted internal trust and improved team NPS by implementing observability, ensuring reliable data for airline operations and customer support.

### Stakeholder Engagement

- **Collaboration**: Regularly talk to stakeholders (internal customers, executives) to understand data needs.
- **Validate Solutions**: Test new tools or processes directly with end-users to gather feedback.
- **Build Trust**: Reliable data fosters confidence and drives broader data adoption within the organization.

#### Continuous Learning and Advice

- **Stay Curious**: The data landscape changes rapidly (e.g., **generative AI**). Keep updating skills.

## Conversation Takeaways & Additional Notes

### Key Takeaways From the Conversation with Barr Moses

Data issues are inevitable and they could occur at any stage of your data pipeline. The earlier you are able to detect them, the less damage to the organization it will cause. To detect data issues, you need to first choose metrics that assess the data quality, similarly to how software teams monitor metrics that assess the health of their software's infrastructure.

In her book ([Data Quality Fundamentals, by Barr Moses, Lior Gavish, Molly Vorweck [book]](https://www.oreilly.com/library/view/data-quality-fundamentals/9781098112035/ch02.html#what_are_data_quality_metricsquestion_m)), Barr Moses suggests to start with the following questions:

- Is the data up-to-date?
- Is the data complete?
- Are fields within expected ranges?
- Is the null rate higher or lower than it should be?
- Has the schema changed?

She formulated these questions into 5 pillars for data observability, which aim to fully describe the state of the data.

### [Barr Moses 5 Pillars](https://www.montecarlodata.com/blog-what-is-data-observability/)

1. **Distribution/ Internal Quality**: The quality pillar refers to the internal characteristics of the data, and checks metrics such as the percentage of NULL elements, percentage of unique elements, summary statistics and if your data is within the expected range. It helps you ensure that your data is trusted based on your data expectation.
2. **Freshness**: Data freshness refers to how “fresh” or “up-to-date” the data is within the final asset (table, BI report), i.e., when the data was last updated, and how frequently it is updated. Stale data results in wasted time and money.
3. **Volume**: Data volume refers to checking the amount of data ingested and looking for unexpected spikes or drops. Sudden drops in data volume can indicate issues like lost data or system outages, and sudden increases may indicate unexpected surges in usage.
4. **Lineage**: According to [Barr](https://towardsdatascience.com/introducing-the-five-pillars-of-data-observability-e73734b263d5), “When data breaks, the first question is always “where?” Data lineage helps you trace the data journey from its source to its destination, visualizing how data was transformed and where it was stored. This way, you can identify the source of errors or anomalies.
5. **Schema**: Data schema refers to monitoring changes in data structure or types. This pillar helps avoid the failure of the data pipeline.

### Additional Resources

If you'd like to learn more about data observability, you can check the following additional resources.

- [Data quality fundamentals](https://learning.oreilly.com/library/view/data-quality-fundamentals/9781098112035/), by Barr Moses, Lior Gavish, Molly Vorweck [book]
- [The rise of data downtime](https://towardsdatascience.com/the-rise-of-data-downtime-841650cedfd5), by Barr Moses [article]
- [What is data observability?](https://learning.oreilly.com/library/view/what-is-data/9781098120993/ch02.html#characteristics_of_a_data_incident), by Andy Petrella [book]

## Monitoring Data Quality

### Stakeholder-Driven Focus

- **Identify critical metrics** by first understanding what stakeholders care about most.
- **Avoid alert fatigue**: Monitoring every possible metric can create noise and confusion.
- **Accuracy** and **completeness** are universally important, but prioritize which data fields matter most for each project.

### Key Quality Criteria

- **Freshness**: Ensure data is no more than 24 hours old (or meets other stakeholder-defined time windows).
- **Accuracy**: Verify crucial values (e.g., purchase amounts) match expectations.
- **Completeness**: Confirm all records are ingested (e.g., no missing sales data).
- **Null checks**: Identify unexpected null values to catch ingest or transformation issues.

#### Schema and Format Consistency

- **Before ingestion**: Include tests for schema changes and data types.
- **Early detection**: Spot format shifts before incorrect data propagates through downstream systems.

### Tools and Best Practices

- Start with basic or **custom checks** for quick prototyping.
- Consider dedicated **data quality tools** (e.g., **Great Expectations**) to automate checks and reduce manual overhead.

## Conversation with Abe Gong

### Origins of Great Expectations

- **Inspiration**: Born from repeated data quality challenges in healthcare data consulting and data science collaborations.
- **Core idea**: Enable testing data pipelines similarly to software testing (adopted by co-founders Abe Gong and James Campbell).
- **Healthcare context**: Highlighted need for reliability due to high-stakes regulatory and health implications.

### Understanding Data Quality

- **Definition**: **Fit for purpose** - Data's ability to reliably support specific use cases (e.g., dashboards, ML models).
- **Key considerations**:
  - Accuracy, completeness, and timeliness for operational metrics.
  - Distributional integrity and bias mitigation for machine learning.

### Great Expectations in Practice

- **Functionality**: Creates assertions (**expectations**) about data properties at pipeline stages.
- **Flexibility**: Adaptable to diverse use cases but requires clear documentation for specific implementations.

#### Implementation Advice

- **Stakeholder analysis**: Determine who defines/maintains expectations (e.g., engineers vs. non-technical teams).
- **Anti-pattern avoidance**: Avoid overly rigid expectations that don’t evolve with business needs.

### Collaboration and Scaling

- **Challenges**: Non-technical stakeholders (e.g., Excel users) may struggle with code-based expectations.
- **Solution**: **Great Expectations Cloud** (upcoming) simplifies collaboration and expectation management for mixed teams.

### Key Takeaways for Learners

- **Proactive approach**: Address data quality early to prevent downstream failures.
- **Lab focus**: Practice creating expectations tied to specific data use cases.
- **Evolution**: Tools like Great Expectations bridge technical and business needs in data reliability.

## Great Expectations Workflow Overview

- [Great Expectations Documentation](https://docs.greatexpectations.io/docs/core/introduction/)

### Core Components

- **Data Sources**: Defines where your data resides (e.g., SQL database, local file system, **Amazon S3**, or pandas DataFrame).
- **Expectations**: Assertions or checks (e.g., `expect_column_values_to_be_unique`) to ensure data meets defined quality conditions.
- **Checkpoints**: Automate validation by combining **batch requests** with **expectation suites** and generating validation results.

#### Data Assets and Batches

- **Data Asset**: Collection of records within a data source (e.g., a table, a file, or a query).
- **Partitioning**: Divide a data asset into smaller **batches** (e.g., monthly slices, store IDs).
- **Batch Request**: Primary way to retrieve these subsets for validation.

### Validation Process

- **Expectation Suite**: Groups multiple expectations relevant to a data asset.
- **Validator Object**: Accepts a **batch request** and an **expectation suite** to perform manual checks.
- **Checkpoint**: Automates validation by providing the required parameters (batch request, expectation suite) and handling the result generation.

#### Stores

- **Expectation Store**: Holds **expectation suites** definitions.
- **Validation Store**: Stores validation outcomes and related metadata.
- **Checkpoint Store**: Saves configurations for checkpoints.
- **Data Docs Store**: Generates accessible reports on expectations, checkpoints, and validation results.

### Typical Workflow Steps

- **Instantiate** the **data context** to set up configurations and metadata.
- **Configure** a **data source** and define **data assets** to specify which records to validate.
- **Partition** data into **batches** using batch requests.
- **Create** or **reuse** an **expectation suite** to define quality checks.
- **Validate** data via a **validator** object or a **checkpoint** for automated execution.
- **Review** validation results and reports in the respective **stores** to track data quality status.

## Conversation with Chad Sanderson on Data Contracts

- [The rise of Data contracts, by Chad Sanderson [blog]](https://dataproducts.substack.com/p/the-rise-of-data-contracts)

### Definition and Purpose

- **Data contracts**: Serve as interfaces/APIs for data between producers and consumers.
- Prevent data quality issues by establishing expectations for schema, content, and SLAs.
- Address lack of communication between data producers (e.g., software engineers) and consumers (e.g., data teams).

### The Need for Data Contracts

- **Modern data challenges**: Federated data ingestion in cloud environments leads to unmanaged dependencies.
- **Downstream impacts**: Data quality issues require time-consuming root-cause analysis and backfilling by data engineers.
- **Ownership gap**: Producers often lack awareness of how their data is used in dashboards, ML models, or business decisions.

### Key Components of Data Contracts

#### Structural Elements

- **Schema**: Column names, data types, and business logic defining data structure.
- **Schema evolution rules**: Guidelines for managing changes over time.

#### Content and Enforcement

- **Data quality rules**: Constraints on data validity (e.g., value ranges, uniqueness).
- **SLAs**: Performance and freshness guarantees.
- **Enforcement mechanisms**: Integration tests, alerts, or automated validation in DevOps workflows.

### Implementation Strategies and Challenges

- **Start small**: Focus on critical pipelines with clear business impact (e.g., CFO dashboards).
- **Technology integration**: Embed contracts into existing workflows (e.g., CI/CD pipelines) to minimize producer burden.
- **Collaboration**: Educate producers on downstream data usage to foster ownership.

#### Common Pitfalls

- Avoid overly complex contracts; prioritize actionable/enforceable constraints.
- **Producer resistance**: Software engineers may perceive contracts as slowdowns without clear ROI.

### Benefits and Recommendations

- **Reduced data engineering burden**: Minimizes reactive firefighting and backfilling.
- **Improved cross-team communication**: Producers gain visibility into data usage and impact.
- **Actionable enforcement**: Combine paper agreements with automated checks for scalability.
- **Selective adoption**: Target high-value data sources first to demonstrate success.

## Monitoring with Amazon CloudWatch

### Overview of Monitoring

- **Monitoring purpose**: Ensures components in data systems operate as expected and alerts teams to issues proactively.
- **Amazon CloudWatch**: Central service for collecting and analyzing metrics from AWS resources.
- **System-level metrics**: Automatically tracked (e.g., **CPU utilization**, **disk IO**, **network traffic**, **memory usage**).

### CloudWatch Features

#### Custom Metrics

- Track application-specific data (e.g., **transactions processed**, **API response times**, **active users**).
- Extend monitoring beyond default system metrics.

#### Dashboards and Visualization

- **CloudWatch dashboards**: Aggregate metrics from multiple systems into unified views.
- Identify patterns, anomalies, and real-time issues through visualizations.

#### Alarms and Baselines

- **CloudWatch alarms**: Trigger alerts/actions when metric thresholds (e.g., CPU > 90%) are breached.
- **Baseline establishment**: Measure system performance under varied loads using CloudWatch’s 15-month data retention.

### Key RDS Metrics to Monitor

- **CPU utilization**: Consistently high values (>80-90%) indicate scaling needs or query optimization requirements.
- **RAM consumption**: High usage may necessitate upgrading instance types.
- **Disk space**: Values above 85% require data archiving/deletion.
- **Database connections**: Approaching max limits can cause errors; manage via connection pooling or scaling.

### Additional Considerations

- **Third-party tools**: Services like **Data Dog** or **Splunk** complement CloudWatch for monitoring.
- **Use-case dependency**: Metrics vary by AWS service (e.g., RDS vs. EC2) and application requirements.

# Incident Response

- [article 1](https://medium.com/@mikldd/incident-management-for-data-teams-5a14acd4e3d8)
- [article 2](https://cloud.google.com/docs/security/incident-response)

# Week 3 Quiz

## Questions

1. Which of the following statements best differentiates DataOps observability from DevOps observability?
   1. DataOps observability focuses on monitoring the accuracy of data, while DevOps observability focuses on ensuing that the data is discoverable.
   2. DataOps observability relies solely on manual data checks, while DevOps observability uses tools developed specifically for software applications.
   3. DataOps observability focuses on monitoring the health of data through data quality metrics, while DevOps observability focuses on monitoring the health of systems through system performance metrics.
2. According to this week’s videos, why is it important to communicate with source system owners in the context of data observability and monitoring?
   1. To reduce the reliance on data quality testing tools like Great Expectations.
   2. To eliminate the need for monitoring data freshness and accuracy.
   3. To anticipate and mitigate potential changes in data that might affect its quality.
3. True or False: It is a best practice to monitor every aspect of your data to ensure that the data is high quality for many stakeholders.
   1. True
   2. False
4. Which of the following statements is/are true about version control within DataOps?
   1. With version control, you can track changes in your infrastructure.
   2. With version control, you can track changes in your code and your data.
   3. With version control, you cannot roll back to a previous version of the infrastructure.
   4. With version control, you can roll back to a previous version of the data.
5. Other than Terraform, what other infrastructure as code tools were mentioned in this week’s videos?
   1. AWS CloudFormation
   2. Bash
   3. Airflow
   4. Ansible
6. Which of the following statements is true about Terraform?
   1. You can use Terraform to create only AWS resources. You cannot provision resources from other cloud providers.
   2. With Terraform, you use a declarative language to specify the desired end-state of the infrastructure.
   3. If you repeatedly run a Terraform configuration file that creates two EC2 instances, then you will create two new EC2 instances each time, regardless of whether they already exist.
   4. You write the Terraform configuration files in Python.
7. Which of the following statements is TRUE about the use of Infrastructure as Code in cloud infrastructure management?
   1. Infrastructure as Code helps you automatically run your data pipelines.
   2. Infrastructure as Code allows for the automatic creation of python scripts used to ingest and transform your data.
   3. Infrastructure as Code allows for the automation of resource provisioning using code-based configuration files.
   4. Infrastructure as Code allows for the automation of new code testing using code-based testing files.
8. According to this week’s videos, which of the following is/are data quality metrics you could monitor? Select all that apply.
   1. The freshness of data
   2. The number of null values
   3. RAM consumption
   4. CPU utilization
   5. The range of values in a particular column

## Answers

1. 3
2. 3
   1. Communication with source system owners to understand upcoming changes that could impact data quality is as important as building in checks or tests in your data monitoring.
3. 2
   1. You want to identify the most important metrics that the stakeholders care about and only monitor those to avoid causing confusion and “alert fatigue”
4. 1, 2 & 4
5. 1 & 4
   1. We are using CloudFormation in this specialization to create the resources that are needed to set up the labs. To learn more about CloudFormation, you can check out its overview page [here](https://aws.amazon.com/cloudformation/).
6. 2
   1. You write Terraform configuration files in a declarative language called HCL, or HashiCorp Configuration Language. You just have to declare what you want the end-state of the infrastructure to look like and Terraform will figure out the exact steps needed to achieve the desired end-state.
7. 3
   1. Infrastructure as Code, through tools like Terraform and CloudFormation, enables the automation of creation, configuration, and management of cloud infrastructure resources with code-based configuration files.
8. 1, 2 & 5
