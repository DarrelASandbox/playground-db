- [Orchestration Overview](#orchestration-overview)
  - [Overview](#overview)
    - [Importance of Orchestration](#importance-of-orchestration)
    - [Evolution of Orchestration](#evolution-of-orchestration)
    - [Airflow as a Leading Tool](#airflow-as-a-leading-tool)
  - [Before Orchestration Tools](#before-orchestration-tools)
    - [Cron-Based Pipeline Automation](#cron-based-pipeline-automation)
      - [Cron Job Structure](#cron-job-structure)
    - [Example Pipeline Implementation](#example-pipeline-implementation)
    - [Limitations of Cron-Based Pipelines](#limitations-of-cron-based-pipelines)
    - [Use Cases for Cron](#use-cases-for-cron)
  - [Evolution of Orchestration Tools](#evolution-of-orchestration-tools)
    - [Historical Background](#historical-background)
    - [Airflow](#airflow)
    - [Emerging Orchestration Tools](#emerging-orchestration-tools)
    - [Recommendation](#recommendation)
  - [Orchestration Basics](#orchestration-basics)
    - [Directed Acyclic Graphs (DAGs)](#directed-acyclic-graphs-dags)
    - [Dependencies and Scheduling](#dependencies-and-scheduling)
      - [Implementation in Airflow](#implementation-in-airflow)
    - [Monitoring and Alerting](#monitoring-and-alerting)
- [Airflow](#airflow-1)
  - [Airflow Core Components](#airflow-core-components)
    - [Core Components](#core-components)
      - [Task Flow](#task-flow)
    - [Managed Airflow Services](#managed-airflow-services)
  - [Airflow - The Airflow UI](#airflow---the-airflow-ui)
    - [DAG View](#dag-view)
    - [Grid View](#grid-view)
      - [Details Tab](#details-tab)
      - [Graph Tab](#graph-tab)
      - [Gantt Chart Tab](#gantt-chart-tab)
      - [Code Tab](#code-tab)
    - [Task Troubleshooting](#task-troubleshooting)
    - [Key UI Purposes](#key-ui-purposes)
  - [Airflow - Creating a DAG](#airflow---creating-a-dag)
    - [Defining a DAG](#defining-a-dag)
      - [Example Structure](#example-structure)
    - [Tasks and Operators](#tasks-and-operators)
      - [Defining Python Tasks](#defining-python-tasks)
    - [Establishing Dependencies](#establishing-dependencies)
  - [Additional Notes About Airflow Basic Concepts](#additional-notes-about-airflow-basic-concepts)
    - [Scheduling Your DAG \& Other DAG Parameters](#scheduling-your-dag--other-dag-parameters)
      - [Check out these links if you want to learn more about scheduling in Airflow:](#check-out-these-links-if-you-want-to-learn-more-about-scheduling-in-airflow)
    - [Airflow Operators](#airflow-operators)
    - [Defining Dependencies](#defining-dependencies)
    - [Additional References](#additional-references)
  - [Airflow - XCom and Variables](#airflow---xcom-and-variables)
    - [XCom (Cross-Communication)](#xcom-cross-communication)
      - [Implementation Example](#implementation-example)
    - [Global Variables in Airflow](#global-variables-in-airflow)
      - [UI Variable Configuration](#ui-variable-configuration)
      - [Code Integration](#code-integration)
    - [Best Practices](#best-practices)
  - [Best Practices for Writing Airflow DAGs](#best-practices-for-writing-airflow-dags)
    - [1](#1)
    - [2](#2)
    - [3](#3)
    - [4](#4)
    - [5](#5)
    - [Additional References (if you’d like to learn more)](#additional-references-if-youd-like-to-learn-more)
  - [Airflow TaskFlow API](#airflow-taskflow-api)
    - [Introduction to TaskFlow API](#introduction-to-taskflow-api)
    - [Traditional vs. TaskFlow Paradigm](#traditional-vs-taskflow-paradigm)
      - [Traditional Paradigm](#traditional-paradigm)
      - [TaskFlow API](#taskflow-api)
    - [XCom Integration with TaskFlow](#xcom-integration-with-taskflow)
    - [Combining Paradigms](#combining-paradigms)
  - [Example on Branching in Airflow](#example-on-branching-in-airflow)
    - [Traditional Paradigm](#traditional-paradigm-1)
    - [TaskFlow API](#taskflow-api-1)
  - [Orchestration on AWS](#orchestration-on-aws)
    - [Apache Airflow on AWS](#apache-airflow-on-aws)
      - [Managed Workflows for Apache Airflow (MWAA)](#managed-workflows-for-apache-airflow-mwaa)
    - [Other AWS Orchestration Services](#other-aws-orchestration-services)
      - [AWS Glue Workflows](#aws-glue-workflows)
      - [AWS Step Functions](#aws-step-functions)
    - [Choosing the Right Orchestration Tool](#choosing-the-right-orchestration-tool)
- [Week 4 Quiz](#week-4-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Orchestration Overview

## Overview

- **Focus of the final week**: Understanding orchestration in data pipelines.
- **Relation to DataOps**: Ties closely to **automation** and **observability** pillars.
- Builds on previous work with **Terraform** (infrastructure as code), **Great Expectations**, and **CloudWatch** for data quality and monitoring.

### Importance of Orchestration

- **Definition**: Automates and coordinates data pipeline tasks.
- **Part of DataOps**: Enhances workflow automation and observability.
- **Undercurrent in Data Engineering**: Linked to data management and software engineering.
- **Standalone undercurrent**: Emphasized in the Data Engineering Lifecycle due to its central role.

### Evolution of Orchestration

- **Before orchestration**: Data pipelines were managed via scripts or manual processes.
- **Modern tools**: Emerged to handle increasing complexity and scale.
- Future direction involves more **tool-agnostic** orchestration frameworks.

### Airflow as a Leading Tool

- **Industry standard**: Most widely used orchestration platform today.
- **Pipelines as code**: Shifts from infrastructure as code to managing entire pipelines in code.
- Integrates with data quality checks and monitoring (e.g., **Great Expectations**, **CloudWatch**).
- **Practical hands-on experience**: Focus of this week's lab.

## Before Orchestration Tools

### Cron-Based Pipeline Automation

- **Cron jobs** were the primary method for automating data pipelines before orchestration tools.
- **Pure scheduling approach**: Tasks are timed sequentially without dependency management.

#### Cron Job Structure

- **Syntax**: `[minute] [hour] [day] [month] [day_of_week] [command]`.
- **Asterisks (\*)**: Indicate "no restriction" for a time field.
- **Example**: `0 0 1 1 * echo "Happy New Year"` executes annually at midnight on January 1.

### Example Pipeline Implementation

- **Data ingestion**:
  - `0 0 * * * python ingest_from_rest_api.py` runs daily at midnight.
  - `0 0 * * * python ingest_from_database.py` synchronizes database data nightly.
- **Data transformation**:
  - `0 1 * * * python transform_api_data.py` processes API data at 1 AM.
- **Data combination**:
  - `0 2 * * * python combine_api_and_database.py` merges datasets at 2 AM.

### Limitations of Cron-Based Pipelines

- **Failure risks**:
  - No automatic handling of task failures or unexpected delays.
  - Downstream tasks proceed regardless of upstream success.
- **Lack of observability**:
  - No built-in monitoring, alerts, or failure notifications.
  - Failures may go undetected until stakeholders report issues.

### Use Cases for Cron

- **Independent tasks**: Suitable for simple, isolated processes (e.g., scheduled data downloads).
- **Prototyping**: Useful for testing pipeline components before adopting orchestration tools.

## Evolution of Orchestration Tools

### Historical Background

- **Before open source**: Orchestration was accessible primarily to large companies due to the high cost of building in-house solutions.
- **Facebook’s Data Swarm**: An internal tool still in use today.
- **Apache Oozie**: Became popular in the 2010s but was closely tied to Hadoop, limiting flexibility in diverse environments.
- **Airbnb’s Airflow (2014)**: Inspired by early tools like Data Swarm, later became the industry standard.

### Airflow

- **Developed by**: Maxime Beauchemin and collaborators at Airbnb.
- **Open source foundation**: Designed for Airbnb’s internal needs but made publicly available from the start.
- **Apache project status**: Became an incubator project in 2016 and a full Apache-sponsored project in 2019.
- **Advantages**:
  - Written in Python, allowing broad extensibility.
  - Active open source community with frequent commits and quick bug/security fixes.
  - Available as a managed service on **AWS**, **GCP**, and **astronomer.io**.
- **Shortcomings**: Has gaps in scalability, data integrity, and support for streaming pipelines.

### Emerging Orchestration Tools

- **Luigi** and **Conductor**: Early open source alternatives.
- **Prefect**, **Dagster**, and **Mage**: Aim to improve on Airflow’s design by offering enhanced scalability, data quality testing, or built-in data transformations.
- Some tools focus on **streaming pipelines**, addressing Airflow’s current limitations.

### Recommendation

- **Learn Airflow**: Still the most in-demand orchestration tool in industry.
- **Stay current**: Keep an eye on newer tools as the orchestration landscape continues to evolve.

## Orchestration Basics

### Directed Acyclic Graphs (DAGs)

- **DAG**: Represents tasks (nodes) in a data pipeline, connected by edges showing data flow in one direction.
- **Acyclic**: No circular dependencies, ensuring tasks flow from start to finish without looping.
- Provides a clear, visual representation of the data pipeline.

### Dependencies and Scheduling

- **Dependencies**: Ensure one task completes before the next begins, preventing conflicts and broken pipelines.
- **Scheduling**: Can be time-based (e.g., daily at midnight) or event-based (e.g., new file upload triggers a run).
- **Sensors**: Specialized tasks that wait for external events (e.g., **S3 file upload**) before allowing downstream tasks to proceed.
- Allows for **fallback plans** if tasks fail or run longer than expected.

#### Implementation in Airflow

- **Python-based**: DAGs are defined programmatically in Python.
- **Airflow UI**: Visualizes DAGs, monitors task statuses, and offers debugging/troubleshooting features.
- Widely adopted, but concepts apply to most orchestration platforms.

### Monitoring and Alerting

- **Monitoring tasks**: Track runtimes, resource usage, and overall health of the pipeline.
- **Alerts**: Notify stakeholders if tasks fail or exceed expected run times.
- **Data quality checks**: Validate schemas, check for null values, or verify value ranges to ensure data integrity.

# Airflow

## Airflow Core Components

### Core Components

- **Web Server**: Hosts the Airflow UI, allowing you to visualize, monitor, and manually trigger DAGs.
- **Scheduler**: Periodically checks tasks, ensuring schedules and dependencies are met. Tasks move from **scheduled** to **queued** to **running** and finally **success** or **failed**.
- **Workers**: Execute tasks passed from the scheduler.
- **Metadata Database**: Stores task statuses and DAG states. The web server reads from this database to display information in the UI.
- **DAG Directory**: Holds Python scripts defining each DAG. Any DAG placed here appears automatically in the UI.

#### Task Flow

- **Dependencies**: Scheduler verifies if all prerequisites are met before triggering a task.
- **Status Updates**: Progression of a task’s status is recorded in the metadata database and shown in the UI.

### Managed Airflow Services

- **Amazon Managed Workflows for Apache Airflow (MWAA)**: Automatically configures and manages core components in AWS.
  - Uses **Amazon S3** as the DAG directory.
  - Relies on **Aurora PostgreSQL** as the metadata database.
  - Integrates with AWS networking and security services for logs and monitoring.

## Airflow - The Airflow UI

### DAG View

- **Initial landing page**: Displays all DAGs in the DAG directory with key metadata.
- **Key metadata shown**: **DAG ID**, owner, schedule, last run time, and status (queued/running/success/failed).
- **Interaction features**:
  - **Pause/unpause**: Toggle to disable/enable DAG execution.
  - **Manual triggers**: Start DAG runs on demand.
  - **Filtering**: Sort DAGs by status or custom tags.

### Grid View

- **Access**: Clicking a DAG ID opens detailed insights into its runs and tasks.
- **Left panel**:
  - **Bar chart**: Visualizes historical DAG runs with color-coded statuses (green=success, red=failure).
  - **Run duration**: Height of bars indicates execution time.
- **Right panel tabs**: Provide deeper analysis tools.

#### Details Tab

- **Historical metrics**: Total runs, success/failure counts, and duration statistics (min/mean/max).
- **Run-specific data**: Select a bar to view details for individual DAG runs.

#### Graph Tab

- **DAG visualization**: Shows task dependencies and structure.
- **Run-specific statuses**: Tasks color-coded based on their state in selected runs.

#### Gantt Chart Tab

- **Task timing analysis**: Displays queue/run durations per task to identify bottlenecks.

#### Code Tab

- **Code synchronization**: Displays DAG code to verify UI matches local files (read-only).

### Task Troubleshooting

- **Log access**: Click failed tasks to view error messages in the **Logs tab**.
- **Retry mechanism**: Use **Clear task** to rerun failed tasks and resume downstream dependencies.

### Key UI Purposes

- **Monitoring**: Track real-time and historical pipeline health.
- **Debugging**: Diagnose failures via logs and task statuses.
- **Validation**: Ensure DAG structure and code align with expectations.

## Airflow - Creating a DAG

### Defining a DAG

- **DAG class**: Represents the workflow, identified by a unique **DAG_ID**.
- **Schedule**: Can use **cron expressions** (e.g., `0 8 * * *`), **cron presets** (e.g., `@daily`), or **time delta** (e.g., `timedelta(days=3)`).
- **Start date**: Sets the initial run date (e.g., `datetime(2023, 1, 1)`).
- **Catch up**: Determines whether to run missed intervals when the DAG is paused (`True` by default).

#### Example Structure

- **DAG parameters**: `dag_id`, `description`, `tags`, `schedule`, `start_date`, `catchup`.
- Defined within a **with** statement (context manager) to group all tasks.

### Tasks and Operators

- **Operators**: Python classes that define how tasks are processed.
  - **PythonOperator**: Executes Python functions.
  - **BashOperator**: Runs Bash commands.
  - **EmptyOperator**: Marks start/end or organizes the DAG.
  - **EmailOperator**: Sends email notifications.
  - **Sensors**: Wait for external events (e.g., file in **Amazon S3**).

#### Defining Python Tasks

- **task_id**: Unique name for referencing in the Airflow UI.
- **python_callable**: Points to the Python function containing task logic.
- Simple example: each task uses a print statement for demonstration.

### Establishing Dependencies

- **Bit-shift operator** (`>>`): Sets task execution order.
  - `extract >> transform >> load`
- Ensures **sequential workflow**: Tasks cannot start until preceding tasks finish.

## Additional Notes About Airflow Basic Concepts

This reading item contains additional notes about scheduling in Airflow, using Airflow Operators and defining dependencies between tasks.

### Scheduling Your DAG & Other DAG Parameters

When instantiating the DAG in the previous video, I specified the following parameters: `dag_id`, `tags`, `description`, `schedule`, `start_date` and `catchup`. You can also specify other DAG parameters. Check out the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#airflow.models.dag.DAG) to learn more.

In this reading item, we'll take a closer look at what the `start_date` parameter does. When you orchestrate your pipeline in Airflow, you may encounter the terms "data interval " and "logical date" in the Airflow UI or in the Airflow documentation. Each DAG run is associated with a data interval that represents the time range it operates in. Let’s say you instantiated a DAG to run daily using the cron preset `@daily` and the start date is March 1. As shown in the following figure, each DAG run operates in a data interval that starts each day at midnight (00:00) and ends at midnight (24:00).

![airflow-timeline](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/airflow-timeline.png)

The “logical date” is a term associated with a specific DAG run, and it denotes the start of the data interval.

![airflow-logical-date](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/airflow-logical-date.png)

The `start_date` argument for the DAG marks the "logical date" or the start of the first "data interval".

Given a data interval, the DAG is executed at the end of the data interval, not the beginning. This is because Airflow was developed as a solution for ETL needs, where you typically need to aggregate data collected over a time interval. So if you want to analyze the data for March 1, you would need to wait till March 2 midnight after all data for March 1 becomes available. This is why a DAG is always executed at the end of the data interval, and the logical date of a DAG run (start of the data interval) represents the date for which the DAG run is executed, not when the DAG is actually executed. So the first DAG run will only be scheduled one interval after start_date.

#### Check out these links if you want to learn more about scheduling in Airflow:

- [Data-interval](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dag-run.html#data-interval)
- [What does execution date mean?](https://airflow.apache.org/docs/apache-airflow/stable/faq.html#what-does-execution-date-mean)
- You can customize your DAG scheduling using [timetables](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html). In addition to scheduling DAGs, you can make your DAG data-aware, meaning that it is triggered when a data object is updated in another task. Here's an [example](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html) of this.

### Airflow Operators

You learned about some of the Airflow operators such as EmptyOperator, PythonOperator, BashOperator, EmailOperator. You can learn more about these operators by checking out the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html). In addition to those [core operators](https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html) provided by Airflow, there’s a [list of other operators](https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html) that are released independently of the Airflow core that allows you to connect to external systems. For example, [this link](https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html) shows all the possible operators that you can use to interact with each AWS service, and [this link](https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/software.html#transfers) includes the operators you can use to copy data, for example, from a database to S3. It is generally recommended to use the available operators instead of writing your own code from scratch.

In the previous video, the two parameters that were specified in the PythonOperator were `task_id` and `python_callable`. You can always review the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator) to see what other parameters you can specify for PythonOperator. [Here](https://airflow.apache.org/docs/apache-airflow/1.10.13/_api/airflow/operators/index.html#package-contents) is another set of parameters that you can pass to any operators, it includes the following parameters:

- **`email (str or list[str])`**: the ‘to’ email address(es) used in email alerts.
- **`email_on_retry (bool)`**: indicates whether email alerts should be sent when a task is retried
- **`email_on_failure (bool)`**: indicates whether email alerts should be sent when a task failed
- **`retries (int)`**: the number of retries that should be performed before failing the task

Check out this [link](https://docs.astronomer.io/learn/what-is-a-sensor) if you'd like to learn more about Airflow sensors, another special kind of operator.

### Defining Dependencies

You learned that you can use the bit-shift operator (>>) to specify the dependencies between tasks. Here are some examples:

![airflow-bit-shift-operator](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/airflow-bit-shift-operator.png)

### Additional References

- [An introduction to the Airflow UI](https://www.astronomer.io/docs/learn/airflow-ui)
- [Airflow UI - screenshots](https://airflow.apache.org/docs/apache-airflow/stable/ui.html#)

## Airflow - XCom and Variables

### XCom (Cross-Communication)

- **Purpose**: Share small data/metadata between tasks (e.g., metrics, dates, simple values).
- **Key components**:
  - **XCom variables** store data in Airflow's metadata database.
  - Use **xcom_push** to store data and **xcom_pull** to retrieve it.
  - Includes metadata: **key**, value, timestamp, DAG ID, and task ID of origin.
- **Limitation**: Not suitable for large datasets (use intermediate storage like **Amazon S3** instead).

#### Implementation Example

- **Extract task**:
  - Calls API, computes metrics, and uses `context['ti'].xcom_push(key='ratio', value=0.25)`.
  - Requires passing **Airflow context** to access the task instance (`ti`).
- **Print task**:
  - Retrieves data with `context['ti'].xcom_pull(task_ids='extract_metric', key='ratio')`.
  - Accessed via Admin > XComs in the Airflow UI.

### Global Variables in Airflow

- **Use case**: Replace hard-coded values (e.g., API parameters) with reusable variables.
- **Benefits**: Reduces code duplication, enables dynamic configuration, and simplifies updates.

#### UI Variable Configuration

- **Creation**:
  - Navigate to **Admin > Variables** in the Airflow UI.
  - Add variables via key-value pairs (e.g., `locations: ["US", "UK"]`, `num_posts: 20`).
- **JSON handling**: Store complex data as JSON and deserialize when retrieving.

#### Code Integration

- **Retrieval**: Use `Variable.get("num_posts")` or `Variable.get("locations", deserialize_json=True)`.
- **Best practice**: Avoid hard-coding values; use variables for configurability and reproducibility.

### Best Practices

- **XComs**: Reserve for small data transfers; use external storage for large datasets.
- **Variables**: Store frequently used or dynamic parameters in Airflow UI variables.
- **Code structure**: Ensure DAGs are efficient, readable, and leverage Airflow's built-in features.

## Best Practices for Writing Airflow DAGs

When writing DAGs, there are some best practices that help ensure your code is efficient, readable, idempotent and reproducible (like with any code). Let’s go through some of these basic practices.

|                                                                    **Best practices**                                                                     | **Explanation/Example of a bad code** |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------: |
|                                                               Keep tasks simple and atomic                                                                |                   1                   |
|                                                                   Avoid top-level code                                                                    |                   2                   |
| Use variables (user-created variables, Airflow [built-in variables and macros](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)) |                   3                   |
|                                                                        Task groups                                                                        |                   4                   |
|                                               Other practices (Airflow is an orchestrator not an executor)                                                |                   5                   |

### 1

When you prepare your pipeline for orchestration, you need to identify the tasks or steps of your pipeline. Keep your tasks simple such that each task represents one operation. You don’t want to end up with one task that does everything, otherwise you'll lose visibility into your data pipeline and reduce the readability of your code, which does not support idempotency.

For example, in an ETL or ELT process, you would need to create at least three tasks: extract, transform, load, instead of creating just one task that handles the entire process.

### 2

In the following code,

```py
call_some_function()
perform_computation()

with DAG(dag_id="example_xcom", start_date=datetime(2024, 3, 13), schedule='@daily',catchup=False):
        task_1 = PythonOperator(task_id='extract',python_callable=extract_api)
        task_2 = PythonOperator(task_id='load_data',python_callable=load)
        task_1 >> task_2
```

`call_some_function()` and `perform_computation()` are both high-level codes. In general any code that isn’t part of your DAG or operator instantiations is considered to be top-level code. This type of code will be executed at the time when the DAG is parsed by the scheduler. On the other hand, any code that is part of an operator is executed when the task runs, not when the DAG is parsed. Top-level code can cause performance issues because the scheduler checks the DAG directory and parses the DAG files every 30 seconds. So it may not be efficient to execute the high-level code this frequently especially if the code makes some requests to an API or a database.

### 3

**User-created variables**: Including hard-coded values directly in your code is generally not a good practice in software development. This is because they make your code less readable and more error-prone -- you may need to use the same value in multiple places and updating the same value in multiple places can be error-prone. The same principle also applies to when you write code to define your pipelines. Instead of including hard-coded values within your DAG or task definitions, you can store these values by creating variables in the Airflow UI or creating environmental variables and use these variables dynamically inside your code.

[Recommendations from Airflow documentation regarding using Variables](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/variables.html): “Variables are global, and should only be used for overall configuration that covers the entire installation; to pass data from one Task/Operator to another, you should use XComs instead. We also recommend that you try to keep most of your settings and configuration in your DAG files, so it can be versioned using source control; Variables are really only for values that are truly runtime-dependent."

**Airflow built-in Variables**: You learned that Airflow has a set of built-in variables that contain information about the currently running DAG and its tasks, such as the logical date of the DAG run and task instance (for a list of such variables, check [here](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)). You learned that you can access these variables within a task function by passing the context dictionary as an argument to the function. You can also pass these variables directly to the PythonOperator using a syntax known as Jinja templating, which looks like this : “`{{ds}}`”. You use double curly brackets and inside the brackets you specify the variable you’d like to access. In this example, ds represents the logical date of the DAG run.

Let’s see an example: Assume that your `python_callable` is a function that expects the name of a file. For example, this function loads some data to an s3 bucket and requires that you pass the file name. And let’s say you want to include the logical date in the `file_name`.

```py
def load_to_s3(file_name):
    #code that loads data
    print(file_name)
```

So you can specify this information in the PythonOperator as follows:

```py
task_load_s3 = PythonOperator(task_id="load_to_d3",
         python_callable=load_to_s3,
         op_kwargs={'file_name': "data/created{{ds}}/file.csv"})
```

The parameter `op_kwargs` allows you to specify the arguments that you need to pass to the function `load_to_s3`. Note how the logical date was included in the file name using templating (“`{{ds}}`”).

### 4

In the Airflow UI, you can group tasks using Task Groups to organize your DAGs and make them more readable. Inside the task group, you can define tasks and their dependencies using the bit-shift operators `<<` and `>>.` You can create a Task Group using the "`with`" statement, as shown in the following example.

```py
from airflow.utils.task_group import TaskGroup

with DAG(...):
    start = DummyOperator(...)
    with TaskGroup('task_group')as task_group:
       task_a = PythonOperator(...)
       task_b = PythonOperator(...)
       task_a >> task_b

    end = DummyOperator(...)
    start >> task_group >> end
```

### 5

- Heavy processing should be assumed by execution frameworks (e.g. Spark) not Airflow
- For large datasets, don’t use XComs (push dataframes). Use intermediary data storage instead.
- Including code that is not part of your DAG or operator makes your DAG hard to maintain and read: consider keeping any extra code that is needed for your tasks in a separate file.

### Additional References (if you’d like to learn more)

- [Airflow best practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#communication)
- [Other best practices](https://docs.astronomer.io/learn/dag-best-practices)
- [Functional-data-engineering-a-modern-paradigm-for-batch-data-processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) by Maxime Beauchemin

## Airflow TaskFlow API

### Introduction to TaskFlow API

- Introduced in **Airflow 2.0** to simplify DAG creation, especially for Python-heavy workflows.
- Uses **decorators** (`@dag`, `@task`) to reduce boilerplate code while maintaining functionality.
- **Key goal**: Improve code conciseness without replacing traditional operators/paradigms.

### Traditional vs. TaskFlow Paradigm

#### Traditional Paradigm

- Requires explicit instantiation of `DAG` object and operators (e.g., `PythonOperator`).
- Manually track task IDs, function names, and task variables.

#### TaskFlow API

- **@dag decorator**: Replaces explicit DAG constructor calls; function name becomes DAG ID.
- **@task decorator**: Automatically wraps Python functions into tasks (implicit `PythonOperator` use).
- **Dependencies**: Defined using bit-shift operator (`>>`), with tasks represented as function calls.

### XCom Integration with TaskFlow

- **Return statements** implicitly push data to XCom (replaces `xcom_push`).
- **Function parameters** allow pulling XCom values from upstream tasks (simplifies `xcom_pull`).
- Example:
  - **Task 1**: `return processed_data` stores data in XCom.
  - **Task 2**: Define input parameter (e.g., `def transform(data):`) to retrieve XCom data.

### Combining Paradigms

- **Mixed workflows**: TaskFlow can coexist with traditional operators (e.g., `BashOperator`).
- **Explicit XCom usage**: Still possible via `context` parameter in decorated tasks.
- Flexibility to use TaskFlow for Python functions and traditional operators for non-Python tasks.

## Example on Branching in Airflow

Branch operators in Airflow dynamically direct task flow, deciding which subsequent task to execute next based on a specified condition. For instance, consider the following DAG:

![airflow-dag-branch-operators](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/airflow-dag-branch-operators.png)

In the first task, you extract data from an API and compute a certain ratio. In the second task , you check the value of the ratio: if this value is greater than half, you execute the task ‘`print_greater`’, otherwise you execute the task ‘`print_less`’. And finally, you execute the last task ‘`do_nothing`’ regardless of which task was previously executed.

Let’s see this example in code form. We'll take a look at the code written based on the traditional paradigm and the code written with TaskFlow API.

### Traditional Paradigm

First you define the DAG and its tasks as usual:

```py
with DAG(dag_id="branching", start_date=datetime(2024, 3, 13), schedule='@daily', catchup=False):
    task_1 = PythonOperator(task_id='extract_data', python_callable=extract_from_api)
    task_2 = BranchPythonOperator(task_id='check_ratio', python_callable=check_ratio)
    task_3 = PythonOperator(task_id='print_greater', python_callable=print_case_greater_half)
    task_4 = PythonOperator(task_id='print_less', python_callable=print_case_less_half)
    task_5 = EmptyOperator(task_id='do_nothing', trigger_rule = 'none_failed_min_one_success')

    task_1 >> task_2 >> [task_3, task_4] >> task_5
```

Note that for the last task (‘`do_nothing`’), we needed to specify the parameter `trigger_rule` as follows: `trigger_rule =’none_failed_min_one_success’`. This is because we want this task to execute regardless of which previous task was executed, otherwise it will be skipped.

Now, let’s check out the function of each task.

This is the first function: `extract_from_api`.

```py
def extract_from_api(**context):
   import requests
   number_posts = 40
   location = "usa"
   url_link = "https://jobicy.com/api/v2/remote-jobs"
   response = requests.get(url_link, params={"count": number_posts,
                                             "geo": location,
                                             "industry": "engineering",
                                             "tag": "data engineer"}).json()
   count = 0
   for job in response['jobs']:
       if job['jobLevel'] == 'Senior':
           count += 1
   ratio = count / len(response['jobs'])
   context['ti'].xcom_push(key='ratio_us', value=ratio)
```

Now let’s check out the function `check_ratio` that corresponds to the `BranchPythonOperator`:

```py
def check_ratio(**context):
   if float(context['ti'].xcom_pull(key='ratio_us', task_ids='extract_data'))>0.5:
       return 'print_greater' #task_id of the greater than case
   return 'print_less' #task_id of the less than case
```

You can see that it's a regular if statement, but it returns the id of the task that should be executed in each case.

And finally, let's check out the functions of the remaining tasks:

```py
def print_case_greater_half(**context):
   print("The ratio is greater than half: " + str(context['ti'].xcom_pull(key= 'ratio_us', task_ids='extract_data')))

def print_case_less_half(**context):
   print("The ratio is less than half: " + str(context['ti'].xcom_pull(key= 'ratio_us', task_ids='extract_data')))
```

### TaskFlow API

Here's the equivalent code written with TaskFlow API

```py
from airflow import DAG
from datetime import datetime
from airflow.decorators import dag, task

@ dag(start_date=datetime(2024, 3, 13),schedule='@daily', catchup=False)
def example_branching():
    @task
    def extract_from_api():
        import requests
        number_posts = 40
        location = "usa"
        url_link = "https://jobicy.com/api/v2/remote-jobs"
        response = requests.get(url_link,
                    params={"count": number_posts,
                            "geo": location,
                            "industry": "engineering",
                            "tag": "data engineer"}).json()
        count = 0
        for job in response['jobs']:
            if job['jobLevel'] == 'Senior':
                count += 1
        ratio = count / len(response['jobs'])
        return ratio

    @task.branch()
    def check_ratio(ti=None):
        if float(ti.xcom_pull(task_ids='extract_from_api')) > 0.5:
            return 'print_case_greater_half' # task_id of the greater than case
        return 'print_case_less_half'  # task_id of the less than case

    @task
    def print_case_greater_half(ti=None):
        print( "The ratio is greater than half: " +
                str(ti.xcom_pull(key='ratio_us', task_ids='extract_data')))

    @task
    def print_case_less_half(ti=None):
        print("The ratio is less than half: " +
                str(ti.xcom_pull(key='ratio_us', task_ids='extract_data')))

    @task(trigger_rule='none_failed_min_one_success')
    def empty_task():
        pass

    extract_from_api() >> check_ratio() >> [print_case_greater_half(), print_case_less_half()] >> empty_task()


example_branching()
```

Note the use of **decorator**: `@task.branch()` , which is the decorated version of the BranchPythonOperator. Also note that for the empty task, we used the operator @task and we defined a Python function that does nothing. Also to specify the `trigger_rule` for this task, we passed them to the task decorator: `@task(trigger_rule='none_failed_min_one_success')`

Also note how the task instance is accessed when calling xcom_pull: in the example you saw in the previous video, you saw that you can pass in the entire Airflow context dictionary to the task function (**context), and then access the task instance as follows: `context[‘ti’]`. Instead of passing the entire dictionary, you could just pass the task instance as follows: `def check_ratio(ti=None)` (instead of `def check_ratio(**context)`)

## Orchestration on AWS

### Apache Airflow on AWS

- **Hosting options**: Run open-source Airflow on **Amazon EC2 instances** or containers for full configuration control.
- **Trade-offs**: Requires managing infrastructure vs. optimizing for convenience with managed services like **MWAA**.
- **Lab setup**: Used open-source Airflow in containers running on EC2 for hands-on configuration experience.

#### Managed Workflows for Apache Airflow (MWAA)

- **Managed service**: Automates provisioning, scaling, and infrastructure management for Airflow environments.
- **Architecture**:
  - **AWS Fargate** hosts Airflow schedulers and workers in containers.
  - Uses **Amazon Aurora** for metadata storage and **Amazon S3** for DAG directory.
  - Integrates with **Amazon CloudWatch** (logs/metrics) and **AWS KMS** (data encryption).

### Other AWS Orchestration Services

#### AWS Glue Workflows

- **Purpose**: Designed for ETL processes, coordinating **Glue jobs**, crawlers, and triggers.
- **Workflow creation**: Built visually via the AWS Glue console to define dependencies between components.
- **Triggers**: Supports scheduled, on-demand, or event-driven (via **Amazon EventBridge**) execution.

#### AWS Step Functions

- **State machines**: Orchestrate workflows involving **AWS services** (e.g., Lambda, Glue, ECS tasks).
- **Key features**:
  - Tasks execute serverless functions, jobs, or containerized applications.
  - States can make decisions, process inputs, and pass outputs.
  - Ideal for AWS-centric workflows requiring native service integration.

### Choosing the Right Orchestration Tool

- **Airflow**: Best for **Python-based DAGs** and complex workflows with plug-in flexibility.
- **Step Functions**: Favor serverless architectures requiring **native AWS service coordination**.
- **Glue Workflows**: Optimized for **ETL-centric pipelines** in serverless environments.
- **General advice**: Stay updated on evolving tools and align choices with use-case requirements (e.g., control vs. convenience).

# Week 4 Quiz

## Questions

1. What are the benefits of Orchestration over simple Cron scheduling?
   1. Orchestration allows you to set up dependencies between data pipeline tasks, monitor tasks, receive alerts, and create fallback plans, compared to scheduling tasks with Cron.
   2. Orchestration simplifies the task of scheduling by eliminating the need for programming or scripting that is needed for scheduling data pipeline tasks with Cron.
   3. With Orchestration, you can exclusively use the user interface (UI) to configure, define, and trigger your DAG.
   4. Orchestration reduces the operational overhead compared to scheduling data pipeline tasks with Cron.
2. Some of the orchestration engines require that you define your data pipeline as a Directed Acyclic Graph (DAG). What is a DAG?
   1. A DAG is a graph used to visualize your data pipeline, where each node represents data and the tasks that need to be applied to the data are illustrated as edges.
   2. A DAG is a graph used to visualize your data pipeline, where each task is a node and dependencies are illustrated as edges, which flow in one direction and can point to a previous task.
   3. A DAG is a graph used to visualize your data pipeline, where each task is a node and dependencies are illustrated as edges indicating that data flows only in one direction and doesn't form any cycles.
3. One of the Airflow’s core components is the scheduler. What is the role of the scheduler?
   1. The scheduler is where you can monitor, manually trigger, and troubleshoot the behavior of your overall DAGs and the tasks within each DAG.
   2. The scheduler is responsible for directly executing the tasks and updating their status in the UI.
   3. The scheduler monitors all your DAGs and their tasks, and triggers the tasks based on the schedule or when their dependencies are complete.
   4. The scheduler stores the Python scripts that define your DAGs and executes them on a schedule.
4. Which of the following tasks can you do in the Airflow UI? Select all that apply.
   1. Create a global variable.
   2. Check the logs of any task to understand any execution errors in your DAG.
   3. Update the Python script that contains the DAG definition.
   4. Manually trigger a DAG.
5. When you create a DAG instance in Airflow, which parameter do you need to specify if you don’t want the scheduler to start the DAG runs for any missed interval when you unpause the DAG?
   1. tags
   2. catchup
   3. start_date
   4. description
6. XCom is an Airflow feature for sharing data between tasks. According to this week’s videos, for which of the following use cases should XCom be used? Select all that apply.
   1. To pass a single value accuracy metric computed in an upstream task to a downstream task.
   2. To pass large DataFrames between tasks.
   3. To pass information about dates between tasks.
   4. To pass metadata between tasks.
7. Consider the following DAG. Which of the following statements correctly represents the DAG structure?

   ![c2-w4-quiz-q7-dag](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/c2-w4-quiz-q7-dag.png)

   1. A >> [ B>> C, D] >> E
   2. A >> D >> [B, C] >> E
   3. A >> [B, C] >> D >> E

8. Which of the following statements is true about TaskFlow API?
   1. TaskFlow API is an old programming paradigm for defining tasks and their dependencies in a given data pipeline.
   2. TaskFlow API is a programming paradigm that allows you to write your DAGs in an easier and more concise way using decorators.
   3. TaskFlow API represents a REST API that allows external applications to interact with your DAG.
   4. TaskFlow API is a new programming paradigm that relies on the explicit use of Airflow operators.
9. According to this week’s lecture materials, which of the following is/are best practices for writing Airflow DAGs? Select all that apply.
   1. Include top-level code before the DAG definition in the DAG file.
   2. Use variables to store values that are used more than once or that may need to be updated
   3. Group related operations into a single task.
10. Consider the following code. Which of the following statements correctly defines the dependencies?

    ![c2-w4-quiz-q10-code](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/02-source-systems-data-ingestion-and-pipelines/assets/c2-w4-quiz-q10-code.png)

    1. extract_data >> transform_data >> load_data
    2. task_1 >> task_2 >> task_3
    3. “extract” >> “transform” >> “load”

## Answers

1. 1
2. 3
3. 3
4. 1, 2 & 4
5. 2
   1. If this parameter is set to true, when you unpause the DAG, the scheduler will kick off a DAG run for any missed interval when the DAG was paused. If you don’t want this to happen, then you need to set catchup to False.
6. 1, 3 & 4
   1. XCom is designed to pass small amounts of information, like a single value metric, between tasks.
   2. XCom is designed to pass small amounts of information, like dates, between tasks.
   3. XCom is designed to pass small amounts of information, like metadata, between tasks.
7. 1
8. 2
9. 2
   1. Avoid hard-coded values directly in your code to make your code more readable and less error-prone.
10. 2
