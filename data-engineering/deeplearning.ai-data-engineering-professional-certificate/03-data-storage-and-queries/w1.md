- [Data Storage Deep Dive](#data-storage-deep-dive)
  - [Storage Raw Ingredients- Physical Components of Data Storage](#storage-raw-ingredients--physical-components-of-data-storage)
    - [Persistent Storage](#persistent-storage)
      - [Magnetic Disks (HDDs)](#magnetic-disks-hdds)
      - [Solid-State Drives (SSDs)](#solid-state-drives-ssds)
    - [Volatile Memory](#volatile-memory)
      - [Random Access Memory (RAM)](#random-access-memory-ram)
      - [CPU Cache](#cpu-cache)
    - [Performance \& Cost Trade-offs](#performance--cost-trade-offs)
    - [Scalability Techniques](#scalability-techniques)
  - [Storage Raw Ingredients - Processes Required for Data Storage](#storage-raw-ingredients---processes-required-for-data-storage)
    - [Distributed Storage \& Networking](#distributed-storage--networking)
    - [Data Representation \& Serialization](#data-representation--serialization)
      - [Serialization Basics](#serialization-basics)
      - [Row vs Column-Based Serialization](#row-vs-column-based-serialization)
      - [Common Serialization Formats](#common-serialization-formats)
    - [Data Compression](#data-compression)
      - [Benefits of Compression](#benefits-of-compression)
  - [Compression Algorithms](#compression-algorithms)
    - [Compression Overview](#compression-overview)
    - [Compression in Column-Based Formats](#compression-in-column-based-formats)
    - [Examples of Compression Algorithms Used on Column-Based Formats](#examples-of-compression-algorithms-used-on-column-based-formats)
      - [Run-length Encoding](#run-length-encoding)
      - [Bit-Vector Encoding (or bitmap encoding)](#bit-vector-encoding-or-bitmap-encoding)
    - [Optional Resources](#optional-resources)
  - [Cloud Storage Options: Block, Object and File storage](#cloud-storage-options-block-object-and-file-storage)
    - [File Storage](#file-storage)
    - [Block Storage](#block-storage)
      - [Mechanism \& Performance](#mechanism--performance)
      - [Cloud Implementation](#cloud-implementation)
    - [Object Storage](#object-storage)
      - [Structure \& Scalability](#structure--scalability)
      - [Use Cases](#use-cases)
    - [Comparison \& Use Cases](#comparison--use-cases)
      - [File Storage](#file-storage-1)
      - [Block Storage](#block-storage-1)
      - [Object Storage](#object-storage-1)
    - [Storage Tiers](#storage-tiers)
  - [Storage Tiers - Hot, Warm, \& Cold Data](#storage-tiers---hot-warm--cold-data)
    - [Hot Data](#hot-data)
    - [Warm Data](#warm-data)
    - [Cold Data](#cold-data)
    - [Cost vs. Access Trade-offs](#cost-vs-access-trade-offs)
    - [Design Considerations](#design-considerations)
  - [Distributed Storage Systems](#distributed-storage-systems)
    - [Core Components of Distributed Storage Systems](#core-components-of-distributed-storage-systems)
      - [Scaling Benefits](#scaling-benefits)
    - [Key Advantages](#key-advantages)
      - [Fault Tolerance \& Availability](#fault-tolerance--availability)
      - [Performance Optimization](#performance-optimization)
    - [Data Distribution Methods](#data-distribution-methods)
      - [Replication](#replication)
      - [Partitioning (Sharding)](#partitioning-sharding)
    - [Consistency vs. Availability Trade-offs (CAP Theorem)](#consistency-vs-availability-trade-offs-cap-theorem)
    - [Practical Implementation Example: Amazon RDS Aurora](#practical-implementation-example-amazon-rds-aurora)
    - [Applications \& Technologies](#applications--technologies)
  - [Database Partitioning/Sharding Methods](#database-partitioningsharding-methods)
    - [Common Sharding Methods](#common-sharding-methods)
      - [Range-based sharding](#range-based-sharding)
      - [Hashed sharding](#hashed-sharding)
      - [Geo sharding](#geo-sharding)
      - [Other methods](#other-methods)
      - [Resources](#resources)
- [Storage in Databases](#storage-in-databases)
  - [How Databases Store Data](#how-databases-store-data)
    - [Database Fundamentals](#database-fundamentals)
      - [Storage Engine Role](#storage-engine-role)
    - [Indexing for Efficient Data Retrieval](#indexing-for-efficient-data-retrieval)
    - [In-Memory Databases](#in-memory-databases)
      - [Key Technologies](#key-technologies)
  - [Row vs Column Storage](#row-vs-column-storage)
    - [Storage Patterns and Use Cases](#storage-patterns-and-use-cases)
    - [Row-Oriented Storage Performance](#row-oriented-storage-performance)
    - [Column-Oriented Storage Performance](#column-oriented-storage-performance)
  - [The Parquet Format](#the-parquet-format)
    - [Overview](#overview)
      - [Hybrid approach](#hybrid-approach)
    - [A bit more detail about Parquet](#a-bit-more-detail-about-parquet)
    - [Resources](#resources-1)
  - [Wide-Column Databases](#wide-column-databases)
    - [Resources](#resources-2)
  - [Graph Databases](#graph-databases)
    - [Graph Database Fundamentals](#graph-database-fundamentals)
    - [Comparison with Relational Databases](#comparison-with-relational-databases)
      - [Query Complexity Example](#query-complexity-example)
    - [Key Use Cases](#key-use-cases)
    - [Technologies \& Tools](#technologies--tools)
      - [Lab Focus: Neo4j \& Cypher](#lab-focus-neo4j--cypher)
    - [Emerging Trends](#emerging-trends)
  - [Vector Databases](#vector-databases)
    - [Overview of Vector Databases](#overview-of-vector-databases)
    - [Vector Embeddings](#vector-embeddings)
    - [Similarity Search Algorithms](#similarity-search-algorithms)
      - [K-nearest neighbors (KNN)](#k-nearest-neighbors-knn)
      - [Approximate Nearest Neighbors (ANN)](#approximate-nearest-neighbors-ann)
    - [Example ANN Algorithm: HNSW](#example-ann-algorithm-hnsw)
  - [ANN Algorithm: Hierarchical Navigable Small World (HNSW)](#ann-algorithm-hierarchical-navigable-small-world-hnsw)
    - [Resources](#resources-3)
    - [Vector databases](#vector-databases-1)
  - [Neo4j and Cypher Query Language](#neo4j-and-cypher-query-language)
    - [Introduction to Graph Databases](#introduction-to-graph-databases)
    - [Property Graph Model](#property-graph-model)
      - [Nodes and Labels](#nodes-and-labels)
      - [Relationships and Types](#relationships-and-types)
      - [Example Data Model](#example-data-model)
    - [Neo4j Database Setup](#neo4j-database-setup)
    - [Cypher Query Language](#cypher-query-language)
    - [Lab Preparation](#lab-preparation)
    - [Basics of the **MATCH** Statement](#basics-of-the-match-statement)
    - [Node Queries](#node-queries)
      - [Retrieving Basic Node Information](#retrieving-basic-node-information)
      - [Exploring Node Properties](#exploring-node-properties)
    - [Relationship Queries](#relationship-queries)
      - [Retrieving Relationships](#retrieving-relationships)
      - [Aggregations and Analysis](#aggregations-and-analysis)
    - [Filtering and Aggregation](#filtering-and-aggregation)
      - [**WHERE** Clauses](#where-clauses)
      - [Advanced Aggregations](#advanced-aggregations)
    - [Advanced Traversal Queries](#advanced-traversal-queries)
      - [Chain Relationships](#chain-relationships)
      - [Counting Products per Order](#counting-products-per-order)
    - [Lab and Practical Use](#lab-and-practical-use)
  - [Links to Data and Cypher Instructions](#links-to-data-and-cypher-instructions)
  - [Conversation with Juan Sequeda](#conversation-with-juan-sequeda)
    - [**Graph Basics**](#graph-basics)
    - [**Graph Models**](#graph-models)
      - [**Relational vs. Graph Databases**](#relational-vs-graph-databases)
      - [**RDF (Resource Description Framework)**](#rdf-resource-description-framework)
      - [**Property Graphs**](#property-graphs)
    - [**Knowledge Graphs**](#knowledge-graphs)
    - [**Graphs \& Large Language Models (LLMs)**](#graphs--large-language-models-llms)
    - [**Advice for Learners**](#advice-for-learners)
- [Week 1 Quiz](#week-1-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Data Storage Deep Dive

## Storage Raw Ingredients- Physical Components of Data Storage

### Persistent Storage

#### Magnetic Disks (HDDs)

- **Mechanism**: Uses rotating magnetic platters and read/write heads; data stored via magnetic orientation.
- **Latency**: ~4ms (seek time + rotational latency) due to mechanical operations.
- **IOPS**: Limited to several hundred operations per second.
- **Transfer Speed**: Up to 300 MB/s, scalable via distributed storage clusters.
- **Cost**: 2-3x cheaper than SSDs per unit.

#### Solid-State Drives (SSDs)

- **Mechanism**: Stores data electronically in flash memory cells via electrical charges.
- **Latency**: ~0.1ms with no moving parts.
- **IOPS**: Tens of thousands per second, ideal for random access.
- **Transfer Speed**: Over 10x faster than HDDs (e.g., 3+ GB/s with parallel processing).

### Volatile Memory

#### Random Access Memory (RAM)

- **Speed**: ~100 GB/s transfer speed and 0.1µs latency; supports millions of IOPS.
- **Volatility**: Data lost on power failure; suitable for transient data/code execution.
- **Cost**: 30-50x more expensive than SSDs per unit.
- **Use Cases**: Real-time processing, caching, and indexing.

#### CPU Cache

- **Speed**: Fastest access (~1 TB/s transfer, ~1ns latency) due to on-chip placement.
- **Use Cases**: Stores frequently accessed data for immediate CPU retrieval.

### Performance & Cost Trade-offs

- **HDDs vs SSDs**:
  - **HDDs**: Preferred for bulk storage, infrequent access, and large block operations (≥1MB).
  - **SSDs**: Optimal for OLTP systems, high-frequency transactions, and low-latency needs.
- **RAM Limitations**: High cost and volatility restrict usage to non-persistent, high-speed tasks.

### Scalability Techniques

- **Distributed Storage**:
  - **HDDs**: Parallel reads across clusters offset mechanical limits (network-bound).
  - **SSDs**: Partitioning and parallel controllers enable multi-GB/s throughput.
- **Key Consideration**: Storage choice depends on workload demands (speed, cost, persistence).

## Storage Raw Ingredients - Processes Required for Data Storage

### Distributed Storage & Networking

- **Distributed systems**: Split, replicate, and spread data across servers for **improved performance**, **durability**, and **availability**.
- **Networking & CPU**: Critical for handling read/write operations, aggregating results, and distributing requests across servers.
- **Cloud era focus**: Storage solutions rely on network efficiency and CPU coordination to manage distributed data.

### Data Representation & Serialization

#### Serialization Basics

- **Purpose**: Transform in-memory data structures (CPU-optimized) into bytes for **persistent storage** or **network transmission**.
- **Deserialization**: Reconstruct original data from serialized bytes.
- **Key impact**: Serialization choices directly affect query performance and storage efficiency.

#### Row vs Column-Based Serialization

- **Row-based**: Encodes data record-by-record (e.g., entire rows/documents stored sequentially).
  - Ideal for **transactional operations** requiring full-row access.
- **Column-based**: Encodes data column-by-column (e.g., values for one key stored together).
  - Optimized for **analytical queries** targeting specific columns.

#### Common Serialization Formats

- **Human-readable formats**:
  - **CSV**: Row-based but lacks schema enforcement, error-prone, and manual handling for structural changes.
  - **JSON**: Replaced XML; standard for APIs and semi-structured data storage.
  - **XML**: Legacy format with slow serialization, largely phased out.
- **Binary formats**:
  - **Parquet**: Column-based, optimized for big data processing and storage efficiency.
  - **Avro**: Row-based with schema evolution support.
- **Performance example**: Switching from CSV to Parquet improved query speed by **100x** in a case study.

### Data Compression

- **Purpose**: Reduce data size to save storage and **accelerate I/O operations** during queries.
- **Traditional algorithms**: Use frequency-based encoding (e.g., shorter bits for common characters).
- **Compression ratio**: Ratio of compressed to original file size; impacts storage costs and transfer speed.
- **Modern algorithms**: Prioritize **speed** and **CPU efficiency** over max compression (e.g., used in data lakes, columnar databases).

#### Benefits of Compression

- **Storage efficiency**: Smaller files reduce disk usage.
- **Query performance**: Faster data loading from disk to memory.
- **Network optimization**: Compressed data transmits more quickly over networks.

## Compression Algorithms

### Compression Overview

To encode data into a sequence of bits, you can use raw encoding which relies on the data type (boolean, integer, double, character) to map the data item into a sequence of bits of fixed size. This is the raw uncompressed method of encoding. Compression algorithms look for redundancy and repetition in the data values, then re-encode data to reduce the overall number of bits that represent data in storage systems. For example, one way to compress textual data is to map the most frequent characters to codes that use less number of bits than the codes mapped to the less frequent characters. In this way, the total number of bits representing a text could be less than the total number of bits used in raw encoding.

Compression algorithms utilize more sophisticated mathematical techniques to identify and remove redundancy; they can often realize compression ratios of 10:1 on text data. Note that we’re talking about lossless compression algorithms. Decompressing data encoded with a lossless algorithm recovers a bit-for-bit exact copy of the original data. Lossy compression algorithms for audio, images, and video aim for sensory fidelity; decompression recovers something that sounds like or looks like the original but is not an exact copy. Data engineers might deal with lossy compression algorithms in media processing pipelines but not in serialization for analytics, where exact data fidelity is required.

Traditional compression engines such as gzip and bzip2 compress text data extremely well; they are frequently applied to JSON, XML, CSV, and other text-based data formats. In recent years, engineers have created a new generation of compression algorithms that prioritize speed and CPU efficiency over compression ratio. Major examples are Snappy, Zstandard, LZFSE, and LZ4.

### Compression in Column-Based Formats

Some algorithms are generic and can be used in both row-stores and column-stores to compress data using a general-purpose algorithm: LZO (1996), LZ4 (2011), Snappy (2011), Brotli (2013), Oracle OZIP (2014), and Zstd (2015) - ([source](https://15721.courses.cs.cmu.edu/spring2023/slides/05-compression.pdf)). However, some algorithms are specific to column-stores since they use the fact that consecutive values from the same column are stored together on disk. Compression algorithms benefit from repetition and redundancy in data, and values from the same column can have this characteristic.

Consider the following table:

| **Product SKU** | **Quantity** | **Price** | **Customer ID** | **Store ID** | **State** |
| :-------------: | :----------: | :-------: | :-------------: | :----------: | :-------: |
|       34        |      1       |    10     |       67t       |      3       |    CA     |
|       34        |      3       |    30     |       56t       |      3       |    CA     |
|       34        |      1       |    20     |       87q       |      3       |    CA     |
|       63        |      5       |    34     |       98q       |      1       |    IL     |
|       32        |      2       |    44     |       98q       |      1       |    IL     |
|       32        |      1       |    22     |       67t       |      1       |    IL     |
|       67        |      1       |    30     |       56u       |      2       |    NY     |
|       67        |      2       |    60     |       78y       |      3       |    TX     |
|       67        |      2       |    60     |       13t       |      3       |    TX     |

If data is stored in rows, it means that you need to store the values of product sku, quantity, price, customer id, store ID, and state all together. Since each value represents a different feature, the algorithm might not encounter a lot of repetitions. On the other hand, if data is stored in columns, then the product SKUs are all stored together, same thing for store ID and state. Each column can have lots of repeated values: let’s say you have millions of rows where each row represents an order, then the number of distinct values in the product column will be much less than the total number of rows: maybe 10,000 distinct products. This column characteristic allows the compression algorithm to detect the common patterns in data easier, as well as represent the data more efficiently.

In addition to reducing disk space, compression also improves database performance, meaning it helps the database process queries faster since less data is read from disk into memory, and from memory to CPU.

### Examples of Compression Algorithms Used on Column-Based Formats

#### Run-length Encoding

Run-length encoding (RLE) compresses a run of the same values in a column to a more compact representation. Each run is replaced with a tuple that has 3 elements -- (value, start position, runLength), where each element is represented with a fixed number of bits.

For example, here's the product sku column data from the table mentioned earlier:

_34 34 34 63 32 32 32 67 67 67_

With RLE, you'll get this result:

_(34, 1, 3), (63, 4, 1,), (32, 5, 3), (67, 8, 3)_

RLE can be used in column-oriented systems where the columns have few distinct values, meaning that you'll likely have runs of the same value stored together.

#### Bit-Vector Encoding (or bitmap encoding)

With this algorithm, each distinct value is associated with a sequence of bits where the length of the sequence is the same as the number of records/rows in the column: a ‘1’ in the i-th position means that the distinct value appears in the i-th row of the column, and ‘0’ otherwise.

For example, here's the product sku column data again:

_34 34 34 63 32 32 32 67 67 67_

With this algorithm, this data would be represented by four sequences of ten bits (i.e. the number of rows):

- bit-string for value 34: _1110000000_
- bit-string for value 63: _0001000000_
- bit-string for value 32: _0000111000_
- bit-string for value 67: _0000000111_

Bit-vector encoding is most useful when columns have a limited number of unique values (such as states in the US, store ID, product ID). However, it can be used even for columns with a large number of distinct values, especially if it is combined with another compression such as RLE (to further compress it).

### Optional Resources

- Other compression algorithms: [The Design and Implementation of Modern Column-Oriented Database Systems](https://stratos.seas.harvard.edu/sites/g/files/omnuum4611/files/stratos/files/columnstoresfntdbs.pdf)
- [Compression encodings supported in AWS](https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html)
- There is an additional file format (Avro) that you may encounter as a data engineer, especially when working with streaming systems such as Kafka. Avro is a row oriented binary file format that encodes semi-structured data in a way that is more efficient than Binary Json. To learn more about Avro, check the following two resources:
  - [Schema evolution](https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)
  - [Parquet vs Avro](https://airbyte.com/data-engineering-resources/parquet-vs-avro)

## Cloud Storage Options: Block, Object and File storage

### File Storage

- **Structure**: Hierarchical directory trees (folders/subfolders) with metadata (names, permissions, modification dates).
- **Managed Services**: **Amazon Elastic File System (EFS)** provides shared network access without managing infrastructure.
- **Use Cases**: Centralized file sharing for users/applications with simple management.
- **Limitations**: Lower read/write performance due to tracking file hierarchies.

### Block Storage

#### Mechanism & Performance

- **Divides files** into fixed-size blocks (each with a unique ID) stored on HDDs/SSDs.
- **High-speed access**: Ideal for **low-latency transactional workloads** (e.g., OLTP databases, VM storage).
- **Distributed architecture**: Enhances scalability and data durability.

#### Cloud Implementation

- **Amazon Elastic Block Store (EBS)**: Attached to EC2 instances; offers SSD (performance) and HDD (cost-efficient) volume types.
- **Scalability limit**: Capped at terabytes due to compute instance dependencies.

### Object Storage

#### Structure & Scalability

- **Flat, immutable objects** in buckets (e.g., **Amazon S3**) with unique keys (e.g., `s3://bucketName/dataExample.json`).
- **Data immutability**: Requires rewriting entire objects for updates but simplifies synchronization and scaling.
- **Horizontal scaling**: Supports petabytes with parallel reads/writes across distributed nodes.

#### Use Cases

- **Analytical workloads**: OLAP systems, data lakes, and cloud data warehouses.
- **Big data & ML**: Stores unstructured data (text, images) for cost-efficient, large-scale processing.
- **Ephemeral compute**: Decouples storage from compute, enabling on-demand cluster scaling.

### Comparison & Use Cases

#### File Storage

- **Best for**: Simple sharing, low-performance needs, and hierarchical file management.

#### Block Storage

- **Best for**: High-frequency transactional operations (e.g., databases, VMs) requiring **low latency**.

#### Object Storage

- **Best for**: Read-heavy analytics, massive datasets (petabytes), and unstructured data storage.

### Storage Tiers

- **Provider tiers**: Classify data as **hot** (frequent access), **warm** (less frequent), or **cold** (archival) to optimize cost and performance.

## Storage Tiers - Hot, Warm, & Cold Data

### Hot Data

- **Definition**: Frequently accessed data requiring **fast retrieval** (e.g., real-time transactions, cached results).
- **Storage Mediums**: **SSDs** or memory-optimized systems for low-latency access.
- **Cost & Performance**:
  - **Higher storage cost** due to high-speed infrastructure.
  - **Lower retrieval time/compute resources** needed.
- **Example**: Store product catalogs, user purchase histories, or query results for recommendation systems.
- **Amazon S3 Tiers**: **S3 Express One Zone** or **S3 Standard** for high-frequency access.

### Warm Data

- **Definition**: Accessed less frequently than hot data but must remain available (e.g., weekly/monthly reports).
- **Storage Mediums**: Lower-cost **magnetic disks** or hybrid systems.
- **Cost & Performance**:
  - **Moderate storage cost** (cheaper than hot).
  - **Longer retrieval time and higher compute use** compared to hot data.
- **Example**: Historical data for periodic analysis or model fine-tuning.
- **Amazon S3 Tiers**: **S3 Standard-Infrequent Access (IA)** or **S3 One Zone-IA**.

### Cold Data

- **Definition**: Rarely accessed, archived data (e.g., compliance docs, old emails).
- **Storage Mediums**: Low-cost **magnetic disks** with minimal compute overhead.
- **Cost & Performance**:
  - **Lowest storage cost**.
  - **Longest retrieval time** and highest compute requirements.
- **Example**: Archived project documentation or legacy records.
- **Amazon S3 Tiers**: **S3 Glacier** (Flexible Retrieval, Deep Archive) for archival use.

### Cost vs. Access Trade-offs

- **General trend**: Storage cost decreases as access speed slows (**Hot > Warm > Cold**).
- **Balanced strategy**:
  - **Risk of overusing hot tiers**: High costs despite rapid access.
  - **Overusing cold tiers**: Savings offset by slow retrieval and computation.
- **Best practice**: Combine tiers based on **access frequency** and use case needs.

### Design Considerations

- **Beyond access frequency**: Evaluate **scalability**, **durability**, and **data lifecycle**.
- **Amazon S3 tiering example**:
  - Hot data → **S3 Standard/Express One Zone**.
  - Warm data → **S3 Standard-IA/One Zone-IA**.
  - Cold data → **S3 Glacier** tiers for long-term archiving.

## Distributed Storage Systems

### Core Components of Distributed Storage Systems

- **Distributed systems** spread data across multiple **nodes** (servers) connected via networks.
- **Clusters**: Groups of nodes that collectively form the storage system.
- Each node contains storage mediums (disks/SSDs) and handles data management, replication, and access control.

#### Scaling Benefits

- **Horizontal scaling**: Achieved by adding nodes to clusters, enabling storage expansion and workload management.
- Avoids limitations of **vertical scaling** (upgrading a single server).

### Key Advantages

#### Fault Tolerance & Availability

- **Replication** ensures data persists even during node/network failures.
- High availability: Accessible via alternate nodes (e.g., in different geographical locations).

#### Performance Optimization

- Tasks split into subtasks processed in parallel across nodes.
- Read requests served from nearest/low-congestion replica nodes for faster access.

### Data Distribution Methods

#### Replication

- Copies of data stored on multiple nodes (potentially across regions).
- **Benefits**: Redundancy, improved availability, and performance.

#### Partitioning (Sharding)

- Splits datasets into smaller **partitions/shards** distributed across nodes.
- Often combined with replication for redundancy.

### Consistency vs. Availability Trade-offs (CAP Theorem)

- **CAP Theorem**: Distributed systems guarantee only two of three properties:
  - **Consistency**: All reads reflect the latest write.
  - **Availability**: Every request receives a response (may not be current data).
  - **Partition-tolerance**: System operates despite network disruptions.
- **ACID vs. BASE**:
  - **ACID** (RDBMS): Prioritizes consistency and reliability (e.g., strong consistency in updates).
  - **BASE** (NoSQL): Focuses on **eventual consistency** with high availability.

### Practical Implementation Example: Amazon RDS Aurora

- **Use case**: Serving sales data via read replicas for analytics dashboards.
- **Main database instance**: Enforces **strong consistency** (strict read-after-write).
- **Read replicas**: Provide **eventual consistency** for faster access to less recent data.
- Engineers configure queries to prioritize consistency (main instance) or availability (replica nodes).

### Applications & Technologies

- Common in **object storage**, cloud data warehouses, **Hadoop Distributed File System (HDFS)**, and Apache Spark.
- Enables scalable solutions for complex data access patterns in cloud environments.

## Database Partitioning/Sharding Methods

Let’s take a closer look at one approach for implementing distributed storage, specifically for databases, known as database sharding.

Say you want to distribute the following dataset across multiple nodes.

| **Customer ID** | **Name** | **Country** |
| :-------------: | :------: | :---------: |
|      10023      |  Sanjay  |     IND     |
|      27181      |   Jane   |     USA     |
|      98221      |    Mo    |     IND     |
|      10134      |  Abdul   |     CAN     |
|      33410      |   Mina   |     USA     |
|      30191      |   Sam    |     USA     |
|                 |          |             |

You need to split the dataset into partitions or shards, where each shard contains unique rows of data and the shards will collectively make up the whole dataset. Then you can distribute these shards across the nodes in your system. You can use a database sharding method or rule to construct a shard key that indicates how the data will be partitioned.

### Common Sharding Methods

#### Range-based sharding

This method splits the rows based on a range of values. For example, let’s say you want to group the rows based on the first letter of the customer's name. One shard might hold customers whose names start with A through J, another shard might hold names starting with K through R, then a third shard could hold names starting with S through Z. Then the shard key you see in the following tables tells the database which node to distribute each row of data to. This is a straight-forward method but can result in unbalanced shards, meaning unbalanced nodes.

|      **Name**      | **Shard Key** |
| :----------------: | :-----------: |
| Starts with A to I |       A       |
| Starts with J to S |       B       |
| Starts with T to Z |       C       |

#### Hashed sharding

This method uses a mathematical formula called a hash function to determine how to partition your data. For example, you can simply assign alternating hash values of 1 or 2 to each row to separate the rows into two shards. Then this shard key tells the database where to distribute the data for each row. This method can result in a more even distribution of data across nodes.

#### Geo sharding

This method partitions data based on geographical location. Then you can store the customer's information in nodes that are physically located in that location. By reducing the physical distance between the shard and the customer, you retrieve data faster.

#### Other methods

There are also other methods that split the data based on meaningful attributes, for instance, the customer’s occupation or favorite color.

#### Resources

- [What is database sharding?](https://aws.amazon.com/what-is/database-sharding/)
- [Designing data intensive applications](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?adgrpid=1344703291324157&dib=eyJ2IjoiMSJ9.i1bUGZK7N-KyWM2sQR7-B8KYS_yn_vgEDIPgCZKZEqrD3_kYv1WLMRNg2a_cyMTZkenScKZLD1xQT6PoxGtZjpfYLwagMBcOcvqwyg12Ux6vvPPHgXX1vMZoOg1vTM_pc7M5GoJOYAWtL-UQU8rix049vlX-qOUnpYLTJ2MrssfiHjzXSj62mtpldPZ9F8sSVwb2QyjkabDuQFUBKt8wljiPffjJIMY5B8rR7JfDvO8.HDmJ7Lu7-7fnydSo4DSG8hxechXwbUNz0baNI01HWH0&dib_tag=se&hvadid=84044027549737&hvbmt=be&hvdev=c&hvlocphy=44152&hvnetw=o&hvqmt=e&hvtargid=kwd-84044312865379%3Aloc-190&hydadcr=16438_10463512&keywords=designing+data+intensive+applications&qid=1713574421&sr=8-1) - Chapters 5 and 6 (Replication and partitioning)

# Storage in Databases

## How Databases Store Data

### Database Fundamentals

- **Database Management System (DBMS)**: Manages interactions with databases (relational, graph, vector, etc.).
- Components of DBMS: Includes transport system, query processor, execution engine, and **storage engine** (handles physical data storage tasks).

#### Storage Engine Role

- Manages **serialization**, data arrangement on disk, **indexing**, and storage structures.
- Modern engines optimize for SSDs and support **columnar storage** (ideal for analytics on large datasets).

### Indexing for Efficient Data Retrieval

- **Indexing**: Accelerates queries by creating metadata structures (e.g., sorted reference tables).
- **Example**: Filtering by "country=USA" uses a sorted index to reduce search time from linear ($O(n)$) to logarithmic ($O(log n)$) complexity.
- Common use cases: Primary/foreign keys, application-specific columns.

### In-Memory Databases

- **In-memory databases**: Store data primarily in RAM for ultra-fast access (e.g., caching, real-time bidding).
- **Volatility trade-off**: Data is lost on system restart unless persistence mechanisms are enabled.

#### Key Technologies

- **Memcached**: Volatile key-value store for temporary caching (e.g., query/API result caching).
- **Redis**: Supports complex data types and persistence (snapshotting, journaling) for high-performance applications tolerating minimal data loss.
- Use cases: Gaming leaderboards, real-time analytics, session management.

## Row vs Column Storage

- [The Design and Implementation of Modern Column-Oriented Database Systems](https://stratos.seas.harvard.edu/sites/g/files/omnuum4611/files/stratos/files/columnstoresfntdbs.pdf)

### Storage Patterns and Use Cases

- **Row-oriented storage**: Stores data row-by-row (ideal for transactional workloads).
  - Used in **OLTP systems** (efficient for reading/writing entire records).
  - Example: Retrieving user profiles by ID in a production database.
- **Column-oriented storage**: Stores data column-by-column (optimized for analytical workloads).
  - Used in **OLAP systems** (efficient for aggregating columnar data).
  - Example: Calculating total revenue across billions of rows.

### Row-Oriented Storage Performance

- **Strengths**:
  - **Fast record retrieval**: Entire row stored sequentially (e.g., user ID lookups).
  - Low latency for transactional operations (inserts, updates, deletes).
- **Weaknesses**:
  - **Inefficient for analytical queries**: Requires reading entire rows to extract columns.
  - Example: Summing a "price" column in a 1-billion-row dataset takes **4 hours** (3TB transfer at 200MB/s).

### Column-Oriented Storage Performance

- **Strengths**:
  - **Efficient columnar reads**: Transfers only relevant

## The Parquet Format

### Overview

Column-oriented storage is suitable for analytical workloads where you want to apply aggregating operations on columns. But it is not suitable for reading or writing/updating rows. On the other hand, row-oriented storage is suitable for transactional workloads that require read and write to be performed with low latency. But it is not suitable for efficient analytical workloads.

Parquet and ORC (optimized row columnar) are file formats that combine both approaches by following a hybrid approach that tries to get the best from both worlds. The hybrid approach relies on partitioning rows into groups where each row group is stored in a column-wise format.

| **Product SKU** | **Price** | **Quantity** | **Customer ID** |
| :-------------: | :-------: | :----------: | :-------------: |
|     045865      |    40     |      1       |       67t       |
|     902348      |    23     |      4       |       56t       |
|     125893      |    45     |      2       |       87q       |
|     456829      |    50     |      3       |       98q       |

#### Hybrid approach

| 045865 | 902348 | 40  | 23  |  1  |  4  | 67t | 56t |
| :----: | :----: | :-: | :-: | :-: | :-: | :-: | :-: |
| 125893 | 456829 | 45  | 50  |  2  |  3  | 87q | 98q |

Although they are similar, ORC is generally less popular than Parquet (ORC was very popular for use with Apache Hive - a data warehouse), and it enjoys somewhat less support in modern cloud ecosystem tools. So let’s focus on Parquet.

### A bit more detail about Parquet

With a Parquet file, the data is horizontally partitioned into row groups, where each row group has a default size of 128 megabytes. A row group consists of a column chunk for each column in the dataset. Each column chunk is divided up into pages, where each page contains the encoded values for that column chunk, metadata like the minimum, maximum and count of the values, along with other data (repetition and definition levels) used to reconstruct the nested structure of the data. Parquet can be used to store tabular data as well as nested (i.e., semi-structured data like json) data.

Another advantage of Parquet is its portability. So you’ll get better performance with Parquet when interoperating with external tools, unlike proprietary cloud data warehouse columnar formats, that require deserialization and re-serialization for compatibility.

To learn more about the Parquet format, feel free to check out [this video](https://www.youtube.com/watch?v=1j8SdS7s_NY&t=643s) by Databricks.

### Resources

- [Parquet, ORC and Avro](https://www.upsolver.com/blog/the-file-format-fundamentals-of-big-data)
- [Parquet documentation](https://parquet.apache.org/docs/file-format/)

## Wide-Column Databases

A wide-Column database is a type of NoSQL database that's like a combination of a relational database and a document store. It structures the value part of a key-value pair into more key-value pairs.

In a key-value database, the data is represented as follows:

```
 -------      ---------
| Key 1 | -> | Value 1 |
 -------      ---------

 -------      ---------
| Key 2 | -> | Value 2 |
 -------      ---------
```

On the other hand, a wide-column database stores data in tables that are like two-dimensional key-value maps:

```
                  ---------------------------------------------------------
                 | --------------      --------------      --------------   |
                 || Column Key 1 | -> | Column Key 2 | -> | Column Key 3 |  |
 -----------     | --------------      --------------      --------------   |
| Row Key 1 | -> |                                                          |
 -----------     |   ---------           ---------           ---------      |
                 |  | Value 1 |    ->   | Value 2 |    ->   | Value 3 |     |
                 |   ---------           ---------           ---------      |
                  ---------------------------------------------------------

                  ---------------------------------------------------------
                 | --------------      --------------      --------------   |
                 || Column Key 1 | -> | Column Key 2 | -> | Column Key 3 |  |
 -----------     | --------------      --------------      --------------   |
| Row Key 2 | -> |                                                          |
 -----------     |   ---------           ---------           ---------      |
                 |  | Value 1 |    ->   | Value 2 |    ->   | Value 3 |     |
                 |   ---------           ---------           ---------      |
                  ---------------------------------------------------------
```

Each row usually describes a single entity, and is identified by its row key. Within a row, data that’s related is modeled into column families that contain columns with unique column names. The actual data is stored within cells, which are uniquely identified by the combination of row key, column family, and column name (e.g. row key 1, column family 4, column key 3).

For example, let’s suppose you want to store customer and purchase information in a wide-column database like this:

![wide-column-database-customer-purchase-info](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/wide-column-database-customer-purchase-info.png)

You could use the customer_id 12345 as the row key that points to two column families -- customer information with 3 columns (first_name, last_name, and phone_num) and purchase information with 2 columns (invoice_num and store_id).

Each cell value is versioned and uniquely identified by its version number, typically its timestamp. Let’s say this customer changed their phone number or made more than one purchase, then these cells will contain multiple values, each identified by their timestamp.

![wide-column-database-customer-purchase-info-timestamp](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/wide-column-database-customer-purchase-info-timestamp.png)

Now, let’s say you have another customer, with customer_id 45678, who you don’t know the phone number for and who hasn’t made any purchases. Then their row key will point to the customer information column family that contains only the customer’s first_name and last_name columns without the phone_num column. Since this customer hasn’t made any purchases, you don’t have to point to a purchase information column family. When this information becomes available, you can add that to the wide-column database. Since the wide-column database doesn’t enforce a strict table schema, adding columns becomes very flexible, and a column is only written if there’s data for it.

A wide-column database typically stores column families separately on disk. So it will store the customer information separately from the purchase information. Then the data within a column family is stored in a row-oriented fashion. So, for the purchase information, you would store all the data for a specific row key next to each other on disk, storing all the invoice_num values together, then the store_id values together, before moving on to the next row key, and so on.

Examples of wide-column databases include [HBase](https://databass.dev/links/118), [BigTable](https://databass.dev/links/117), [Apache Cassandra](https://cassandra.apache.org/_/index.html), and [Amazon keyspaces](https://aws.amazon.com/keyspaces/) (for Apache Cassandra)

### Resources

- [Wide-column Databases](https://scaleyourapp.com/wide-column-and-column-oriented-databases/)
- [ScyllaDB - Wide-column Database](https://www.scylladb.com/glossary/wide-column-database/)
- [Cassandra: The Definitive Guide](https://www.amazon.com/Cassandra-Definitive-Guide-Distributed-Scale/dp/1491933666)
- [Introduction to HBase schema design](http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf)
- [Apache Cassandra VS Apache HBase](https://aws.amazon.com/compare/the-difference-between-cassandra-and-hbase/)

## Graph Databases

### Graph Database Fundamentals

- **Nodes**: Represent entities (e.g., users, products, locations).
- **Edges**: Define relationships between nodes (e.g., purchases, friendships).
- Treats **relationships as first-class citizens**, enabling efficient traversal of connections.

### Comparison with Relational Databases

- **Relational databases** require complex JOIN operations to analyze relationships (e.g., finding products purchased by a user’s friends).
- **Graph databases** simplify relationship traversal (e.g., traversing "friend" edges to recommend products via direct path navigation).

#### Query Complexity Example

- **Recommendation systems**: Graph databases avoid multi-table JOINs needed in relational systems for friend-of-friend queries.

### Key Use Cases

- **Product recommendations**: Traverse user-friend-product relationships to suggest items.
- **Fraud detection**: Model connections between users, credit cards, and IP addresses to flag suspicious activity.
- **Knowledge graphs**: Integrate data from disparate sources (e.g., products, customers, shipping) for applications like **retrieval-augmented generation (RAG)** in chatbots.
- **Data lineage & supply chain logistics**: Map dependencies and workflows.

### Technologies & Tools

- **Popular graph databases**: Neo4j, ArangoDB, Amazon Neptune.
- **Query languages**:
  - **Cypher** (Neo4j), **Gremlin**, **SPARQL**.
  - **Neo4j features**: Combines graph capabilities with **vector search** (used in AI/ML applications).

#### Lab Focus: Neo4j & Cypher

- Hands-on practice querying graph data with Neo4j and the Cypher language.

### Emerging Trends

- **Vector databases**: Support vector search for similarity-based queries (covered in the next video).
- **Generative AI integration**: Enhances LLMs with contextual data via knowledge graphs (e.g., RAG).

## Vector Databases

### Overview of Vector Databases

- Enable **similarity search** to find semantically similar data for applications like **recommendation systems**, **anomaly detection**, and text generation.
- Optimized for storing/processing **vector data** (numerical arrays), including sensor readings, image data (RGB values), or vector embeddings.
- **Key advantage**: Faster similarity comparisons than raw data analysis.

### Vector Embeddings

- **Vector embeddings**: Numerical representations capturing semantic content of data (text, images) via machine learning models.
- **Process**: Original data (e.g., text) → passed through trained ML model → converted to embeddings → stored in a vector database.
- **Benefits**: Efficient retrieval of similar items by comparing vector distances in high-dimensional space.

### Similarity Search Algorithms

- **Distance metrics**:
  - **Euclidean distance**: Straight-line distance between vectors.
  - **Cosine distance**: Angle between vectors.
  - **Manhattan distance**: Axis-aligned path distance.

#### K-nearest neighbors (KNN)

- Exhaustively computes distances between all vectors and the query item.
- Inefficient for large datasets due to computational complexity.
- Challenged by the **curse of dimensionality** (sparse high-dimensional space reduces accuracy).

#### Approximate Nearest Neighbors (ANN)

- **ANN algorithms**: Prioritize speed over precision by estimating nearest neighbors.
- **Use in vector databases**: Employ optimized data structures for fast retrieval of approximate matches.
- Balances **efficiency** (scalability) with slight accuracy trade-offs.

### Example ANN Algorithm: HNSW

- **Hierarchical Navigable Small World (HNSW)**: Popular ANN method using layered graph structures for rapid traversal.
- Enables vector databases to return approximate nearest neighbors efficiently.

## ANN Algorithm: Hierarchical Navigable Small World (HNSW)

Hierarchical Navigable Small World (HNSW) is a popular ANN algorithm that underpins many vector databases and is considered to be among the best performing ANN algorithms.

![vector-databases-HNSW-ANN-algorithms](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/vector-databases-HNSW-ANN-algorithms.png)

This algorithm relies on building a hierarchical graph representation of the embeddings: each layer consists of a graph representation of the data, where each node represents an embedding, and each edge represents the degree of similarity between two nodes. The layers are constructed in a way so that the top layer contains more of the longest links and the bottom layer contains more of the shortest links. Moreover as you move up from the lowest layers to the highest layer, the number of nodes decreases.

Given a query point (e.g. the green node in the image above), the algorithm starts at the entry node of the top layer (i.e. the red node) and navigates through the graph of that layer, each time choosing the neighboring node that is closest to the query point. It stops at the node that does not have any neighboring nodes closer to the query point. At this point, the algorithm shifts to the current node in the next lower layer and begins searching again. It repeats the process until it finds the nearest node at the bottom layer.

The way this algorithm organizes the data reduces the time and computational resources needed for these searches when compared to the k-nearest neighbors algorithm.

### Resources

- [What is similarity search?](https://www.pinecone.io/learn/what-is-similarity-search/)
- [Use cases of similarity search](https://hyper-space.hashnode.dev/what-is-similarity-search-definition-and-use-cases)
- [What is a vector database?](https://www.pinecone.io/learn/vector-database/)
- [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/)
- Paper - [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs](https://arxiv.org/pdf/1603.09320.pdf)

### Vector databases

- [Neptune analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/algorithms.html)
- [Pinecone](https://www.pinecone.io/) (Short Course - [Building Applications with Vector Databases](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/))
- [Weaviate](https://weaviate.io/) (Short course - [Vector Databases: from Embeddings to Applications](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/))

## Neo4j and Cypher Query Language

### Introduction to Graph Databases

- **Graph databases** like **Neo4j** enable data modeling as **nodes** (entities) and **relationships** (connections).
- Similar interaction paradigm to relational databases but optimized for graph-based queries.
- **Cypher query language** is used to interact with data in Neo4j (analogous to SQL in relational databases).

### Property Graph Model

#### Nodes and Labels

- **Nodes** represent entities (e.g., `Customer`, `Product`) and carry **labels** to categorize them (e.g., `:Customer`, `:Supplier`).
- Labels define node types (e.g., `Category` nodes labeled `:Category`).
- **Node properties**: Key-value pairs (e.g., `customerID`, `address`, `name`) stored with nodes.

#### Relationships and Types

- **Relationships** connect nodes, are directional, and have **types** (e.g., `ORDERS`, `SUPPLIES` in the example model).
- **Relationship properties**: Attributes like `quantity` or `discount` stored with relationships.

#### Example Data Model

- Nodes: `Customer`, `Order`, `Supplier`, `Product`, `Category`.
- Relationships: `PURCHASED` (Customer→Order), `ORDERS` (Order→Product), `SUPPLIES` (Supplier→Product), etc.
- **Real-world use**: Track orders, product categories, suppliers, and customer details with interconnected nodes.

### Neo4j Database Setup

- **Graph creation**: Define nodes, labels, relationships, and properties using Cypher commands.
- Data sources: Import from **CSV files** or directly specify node/relationship properties.
- **Steps**:
  - Define node labels and properties (e.g., `Customer` with `customerID`, `contactName`).
  - Specify relationship types and properties (e.g., `ORDERS` with `discount`, `unitPrice`).
  - Map data sources (e.g., CSV files) to populate nodes and relationships.

### Cypher Query Language

- **Purpose**: Create, read, update, and delete (**CRUD**) graph data.
- **Key syntax**:
  - `MATCH` to query nodes/relationships.
  - `CREATE` to add nodes or relationships.
  - `RETURN` to visualize results.
- Example: Retrieve a customer’s orders and products using a traversal query.

### Lab Preparation

- **Neo4j Desktop Browser**: Interface for interacting with the graph database.
- **Lab tasks**: Practice CRUD operations (e.g., creating/deleting nodes).
- **Provided resources**:
  - Pre-built database example based on the property graph model.
  - CSV files and Cypher scripts for database setup.
  - Optional reading on advanced graph queries.

### Basics of the **MATCH** Statement

- **MATCH pattern RETURN result**: Core syntax for retrieving data, similar to SQL `SELECT`.
- **Nodes**: Represented with parentheses `(n)` (e.g., `MATCH (n) RETURN n` retrieves all nodes).
- **Relationship paths**: Use `(source)-[r]->(target)` (e.g., `MATCH (c)-[r]->(o) RETURN r`).

### Node Queries

#### Retrieving Basic Node Information

- **Total nodes**: `MATCH (n) RETURN COUNT(n)`.
- **Labels**: `MATCH (n) RETURN DISTINCT LABELS(n)` to list unique node labels.
- **Specific labels**: `MATCH (n:Order) RETURN COUNT(n)` counts `Order`-labeled nodes.

#### Exploring Node Properties

- **Properties**: `MATCH (n:Order) RETURN PROPERTIES(n)` retrieves all properties of `Order` nodes.
- **Limit results**: Add `LIMIT 1` to return only the first matching entry.

### Relationship Queries

#### Retrieving Relationships

- **Relationships syntax**: Denoted with `[ ]` (e.g., `MATCH ()-[r]->() RETURN COUNT(r)` counts all relationships).
- **Relationship types**: `MATCH ()-[r:ORDERS]->() RETURN DISTINCT TYPE(r)` lists unique relationship types.

#### Aggregations and Analysis

- **Averages**: Calculate metrics like `AVG(r.quantity * r.unitPrice) AS average_price`.
- **Grouping**: Use `MATCH` with chained relationships and `RETURN c.categoryName, average_price` to group by category.

### Filtering and Aggregation

#### **WHERE** Clauses

- **Basic filtering**: `MATCH (p:Product)-[:PART_OF]->(c:Category) WHERE c.categoryName = 'Meat/Poultry' RETURN p.productName, p.unitPrice`.
- **Inline filters**: Specify properties directly in nodes (e.g., `MATCH (c:Category {categoryName: 'Meat/Poultry'})`).

#### Advanced Aggregations

- **WITH statements**: Temporarily store results for conditional filtering (e.g., `WITH id, COUNT(prod) AS countProd WHERE countProd <= 2 RETURN id`).

### Advanced Traversal Queries

#### Chain Relationships

- **Multi-hop queries**: Traverse paths like `MATCH (c1:Customer {customerID: 'QUEDE'})-[:PURCHASED]->()-[:ORDERS]->(p:Product)<-[:ORDERS]-()-[:PURCHASED]->(c2:Customer) RETURN c2.customerID`.
  - Finds customers who ordered the same products as `QUEDE`.

#### Counting Products per Order

- **Grouping and filtering**: `MATCH (o:Order)-[r:ORDERS]->(p:Product) WITH o.orderID AS id, COUNT(p) AS countProd WHERE countProd <= 2 RETURN id`.

### Lab and Practical Use

- **Neo4j Desktop Browser**: Visualize graph data.
- **JupyterLab integration**: Execute Cypher queries in Python code cells.
- **Combined use cases**: Learn to use Neo4j as both **graph** and **vector database** in labs.
- **CRUD operations**: Practice creating/deleting nodes and relationships.

## Links to Data and Cypher Instructions

- [neo4j - northwind datasets](https://github.com/neo4j-documentation/developer-resources/tree/gh-pages/data/northwind)
- [Tutorial: Import data from a relational database into Neo4j](https://neo4j.com/docs/getting-started/appendix/tutorials/guide-import-relational-and-etl/)

## Conversation with Juan Sequeda

### **Graph Basics**

- **Nodes and edges**: Fundamental units representing entities (nodes) and their relationships (edges).
- **Flexibility**: Graphs adapt to evolving data without rigid schemas, unlike relational databases.
- **Query focus**: Optimized for **path traversal**, **centrality analysis**, and **connected data queries** (e.g., recommendations, fraud detection).

### **Graph Models**

#### **Relational vs. Graph Databases**

- **Relational databases**: Fixed schemas, ideal for stable applications with defined requirements.
- **Graph databases**: Store data as **nodes** (entities) and **edges** (relationships), enabling dynamic data integration.

#### **RDF (Resource Description Framework)**

- **Triple structure**: Subject-predicate-object (e.g., "Document - writtenBy - Joe").
- **Web-centric**: Designed for semantic web data, standardized via **SPARQL** query language.
- **Linked data**: Focuses on meaning and metadata (e.g., `rdfs`, `owl` for ontologies).

#### **Property Graphs**

- **Key-value properties**: Attach metadata to nodes/edges (e.g., `Customer {name: "Alice"}`).
- **Database-first**: Popularized by **Neo4j** with **Cypher** query language.
- **Standardization**: Emerging **GQL** (ISO standard) for property graph querying.

### **Knowledge Graphs**

- **Semantic layering**: Adds context (taxonomies, hierarchies) to raw graph data.
- **Use cases**:
  - Integrate diverse data sources (e.g., product taxonomies, geographies).
  - Enable reasoning (e.g., "Lipstick is a subclass of Cosmetics").
- **Organizational value**: Bridges the "data meaning gap" by aligning with user-domain thinking.

### **Graphs & Large Language Models (LLMs)**

- **Context enhancement**: Knowledge graphs provide structured semantics to improve LLM accuracy.
  - Example: Translating queries to SPARQL/Cypher is **3x more accurate** than SQL alone.
- **Hybrid approach**: Combines graph-contextualized prompts with retrieval-augmented generation (RAG).

### **Advice for Learners**

- **Adopt graph thinking**: Move beyond tables to nodes/edges for interconnected data challenges.
- **Leverage existing work**: Study historical approaches (e.g., RDF, ontologies) rather than reinventing solutions.
- **Focus on integration**: Use graphs to unify structured/unstructured data in AI applications.

# Week 1 Quiz

## Questions

1. Which of the following physical components of data storage has the highest data transfer speed?
   1. Random Access Memory (RAM)
   2. CPU Cache
   3. Magnetic Disk
   4. Solid State Drive (SSD)
2. What is data serialization?
   1. Serialization is a way to reduce the number of bits needed to represent the data.
   2. Serialization is the process of converting data stored in a human-readable textual format into a binary format.
   3. Serialization is the process of applying compression on data stored on disk.
   4. Serialization is the process of translating data from its in-memory representation into a standard format, usually sequence of bytes, that can be efficiently stored or shared over a network.
3. Consider the following three use cases:

   - Case A: Building a transactional database that supports low-latency read and write operations.
   - Case B: Building a repository for file sharing and collaboration with no need for high performance.
   - Case C: Building a storage system to store petabytes of unstructured data for training machine learning models.

   Which cloud storage option should you choose for each use case?

   1. Case A: Object Storage, Case B: File Storage, Case C: Block Storage
   2. Case A: Block Storage, Case B: Object Storage, Case C: File Storage
   3. Case A: Block Storage, Case B: File Storage, Case C: Object Storage
   4. Case A: File Storage, Case B: Block Storage, Case C: Object Storage

4. How are ACID-compliant databases different from the databases that are designed around the BASE principles?
   1. ACID-compliant databases guarantee partition tolerance, while databases designed around the BASE principles do not.
   2. ACID-compliant databases prioritize consistency over availability, while databases designed around the BASE principles prioritize availability over consistency.
   3. ACID-compliant databases are eventually consistent, while databases designed around the BASE principles require strong consistency.
5. Which of the following statements is/are true about columnar storage? Select all that apply.
   1. Columnar storage is more efficient for performing analytical queries than row storage.
   2. Columnar storage is the best storage option for transactional workloads.
   3. It’s faster to update a row in a columnar storage than in row storage.
   4. Columnar storage is well-suited for OLAP systems but not OLTP systems.
6. Consider the following graph model:

   ![c3-w1-quiz-q6-graph-source](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/c3-w1-quiz-q6-graph-source.png)

   Let’s say you want to find, for each user, the total number of questions they commented on. Which of the following is the correct Cypher statement that returns the desired result?

   1. `Match (user:User) -[:COMMENTED]->(:Comment)-[:COMMENTED_ON]->(q:Question) return user, count(q)`
   2. `Match (user:User)-[:COMMENTED]->(:Comment)-[:COMMENTED_ON]->(q:Question) return user, count(distinct q)`
   3. `Match (user:User) -[:asked]->(q:Question) return user, count(q)`
   4. `Match (user:User) -[]->()-[]->(q:Question) return user, count( q)`

7. Which of the following statements best describes the primary function of a vector database as described in this week's videos?
   1. A vector database is designed to store and retrieve large volumes of unstructured text documents.
   2. A vector database enables efficient querying of data based on semantic similarities by storing and processing vector embeddings.
   3. A vector database is primarily used for storing time-series data, such as daily weather measurements.
8. Which of the following statements is true about indexes?
   1. You define indexes based on the primary key of a table.
   2. An index is a separate data structure used to keep some metadata on the side to help locate the data you want more efficiently.
   3. Indexes are created and used by the query processor.

## Answers

1. 2
   1. At the time of the creation of these courses, CPU cache has a high data transfer speed of about 1 TB/s because it is located directly on the CPU processing chip.
2. 4
3. 3
4. 2
5. 1 & 4
   1. With columnar storage, the data from each column is stored next to each other on disk, making it more efficient for performing analytical queries on entire columns of data.
   2. Columnar storage is well-suited for OLAP systems, while row storage is well-suited for OLTP systems
6. 2
7. 2
8. 2
