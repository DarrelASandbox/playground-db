- [Data Warehouse \& Data Lakes](#data-warehouse--data-lakes)
  - [Conversation with Bill Immon](#conversation-with-bill-immon)
    - [Background](#background)
    - [Data Warehouse Fundamentals](#data-warehouse-fundamentals)
    - [ETL Technology](#etl-technology)
    - [Challenges from IBM](#challenges-from-ibm)
    - [Motivation and Innovations](#motivation-and-innovations)
      - [Legacy of Structured Design](#legacy-of-structured-design)
    - [Industry Recognition and Reflection](#industry-recognition-and-reflection)
    - [Key Takeaways](#key-takeaways)
  - [Data Warehouse - Key Architecture Ideas](#data-warehouse---key-architecture-ideas)
    - [Overview of Data Warehouses](#overview-of-data-warehouses)
    - [ETL Pipeline \& Data Ingestion](#etl-pipeline--data-ingestion)
      - [Data Marts](#data-marts)
    - [Evolution of Data Warehouses](#evolution-of-data-warehouses)
  - [Modern Cloud Data Warehouses](#modern-cloud-data-warehouses)
    - [Key Features of Modern Cloud Data Warehouses](#key-features-of-modern-cloud-data-warehouses)
      - [Amazon Redshift MPP Architecture Example](#amazon-redshift-mpp-architecture-example)
    - [ELT Pattern Support](#elt-pattern-support)
    - [Evolution Beyond Traditional Data Warehouses](#evolution-beyond-traditional-data-warehouses)
    - [Performance and Cost Benefits](#performance-and-cost-benefits)
  - [Data Lakes - Key Architectural Ideas](#data-lakes---key-architectural-ideas)
    - [Data Lake Fundamentals](#data-lake-fundamentals)
    - [Data Lake 1.0: Implementation \& Shortcomings](#data-lake-10-implementation--shortcomings)
      - [Core Components](#core-components)
      - [Key Challenges](#key-challenges)
      - [Partial Success Cases](#partial-success-cases)
    - [Transition to Next-Generation Data Lakes](#transition-to-next-generation-data-lakes)
  - [Next-Generation Data Lakes](#next-generation-data-lakes)
    - [Data Lake Organization](#data-lake-organization)
      - [Data Zones](#data-zones)
      - [Partitioning](#partitioning)
      - [Data Catalog](#data-catalog)
    - [Challenges with Data Lakes + Warehouses](#challenges-with-data-lakes--warehouses)
    - [Data Lakehouse Architecture](#data-lakehouse-architecture)
  - [Lab Walkthrough - Simple Data Lake with AWS Glue](#lab-walkthrough---simple-data-lake-with-aws-glue)
    - [Lab Overview](#lab-overview)
    - [Data Processing Workflow](#data-processing-workflow)
      - [Product Reviews Transformation](#product-reviews-transformation)
      - [Metadata Transformation](#metadata-transformation)
    - [Terraform Configuration](#terraform-configuration)
      - [IAM Roles and Policies](#iam-roles-and-policies)
      - [Glue Jobs Setup](#glue-jobs-setup)
    - [Querying and Outputs](#querying-and-outputs)
    - [Compression and Partitioning Exploration](#compression-and-partitioning-exploration)
    - [Data Transformation Process](#data-transformation-process)
      - [Review Transformation Script](#review-transformation-script)
      - [Metadata Transformation](#metadata-transformation-1)
    - [Terraform and Resource Setup](#terraform-and-resource-setup)
    - [Glue Job Execution and Data Cataloging](#glue-job-execution-and-data-cataloging)
    - [Experiment Overview](#experiment-overview)
      - [Experiment 1: Compression Impact (Uncompressed vs. Snappy)](#experiment-1-compression-impact-uncompressed-vs-snappy)
      - [Experiment 2: Compression Algorithms (Snappy vs. Gzip)](#experiment-2-compression-algorithms-snappy-vs-gzip)
      - [Experiment 3: Partitioning Efficiency (Year/Month vs. None)](#experiment-3-partitioning-efficiency-yearmonth-vs-none)
      - [Experiment 4: Poor Partitioning (ASIN Column)](#experiment-4-poor-partitioning-asin-column)
    - [Results and Best Practices](#results-and-best-practices)
- [Data Lakehouses](#data-lakehouses)
  - [The Data Lakehouse Architecture](#the-data-lakehouse-architecture)
    - [Core Components](#core-components-1)
    - [Key Features](#key-features)
      - [Data Management](#data-management)
      - [Governance and Security](#governance-and-security)
    - [Unified Analytics](#unified-analytics)
    - [Industry Adoption](#industry-adoption)
  - [Data Lakehouse Implementation](#data-lakehouse-implementation)
    - [Converged Architectures: Cloud Warehouses vs. Data Lakes](#converged-architectures-cloud-warehouses-vs-data-lakes)
    - [Open Table Formats for Lakehouse Implementation](#open-table-formats-for-lakehouse-implementation)
    - [Inside Apache Iceberg](#inside-apache-iceberg)
      - [Metadata Architecture](#metadata-architecture)
      - [Query Workflow](#query-workflow)
    - [Choosing Between Warehouses, Lakes, or Lakehouses](#choosing-between-warehouses-lakes-or-lakehouses)
    - [The Future of Unified Data Platforms](#the-future-of-unified-data-platforms)
  - [Lakehouse Architecture on AWS](#lakehouse-architecture-on-aws)
    - [Core Components](#core-components-2)
      - [AWS Lake Formation](#aws-lake-formation)
      - [Integration with Existing Tools](#integration-with-existing-tools)
    - [Architectural Layers](#architectural-layers)
      - [Storage Layer](#storage-layer)
      - [Processing Layer](#processing-layer)
      - [Catalog Layer](#catalog-layer)
      - [Consumption Layer](#consumption-layer)
    - [Key Advantages](#key-advantages)
  - [Implementing a Lakehouse on AWS](#implementing-a-lakehouse-on-aws)
    - [Storage Layer](#storage-layer-1)
    - [Catalog Layer](#catalog-layer-1)
      - [Apache Iceberg Tables](#apache-iceberg-tables)
    - [Consumption Layer](#consumption-layer-1)
      - [Redshift Spectrum](#redshift-spectrum)
      - [Amazon Athena](#amazon-athena)
    - [Key Advantages](#key-advantages-1)
  - [Lab Walkthrough - Building a Data Lakehouse with AWS Lake Formation and Apache Iceberg](#lab-walkthrough---building-a-data-lakehouse-with-aws-lake-formation-and-apache-iceberg)
    - [Data Lake Architecture](#data-lake-architecture)
    - [Data Ingestion (Landing Zone)](#data-ingestion-landing-zone)
    - [Data Processing (Curated Zone)](#data-processing-curated-zone)
      - [CSV Processing (Transformation 1)](#csv-processing-transformation-1)
      - [ML Data Preparation (Transformation 2)](#ml-data-preparation-transformation-2)
      - [Ratings Update (Transformation 3)](#ratings-update-transformation-3)
    - [Data Storage Formats](#data-storage-formats)
    - [Catalog Integration \& Querying](#catalog-integration--querying)
    - [Governance \& Tooling](#governance--tooling)
    - [Apache Iceberg File Structure](#apache-iceberg-file-structure)
      - [Catalog Layer](#catalog-layer-2)
    - [Schema Evolution \& Time Travel](#schema-evolution--time-travel)
    - [AWS Lake Formation Governance](#aws-lake-formation-governance)
      - [Lab Implementation Example](#lab-implementation-example)
    - [Optional Lab Features](#optional-lab-features)
- [Week 2 Quiz](#week-2-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Data Warehouse & Data Lakes

## Conversation with Bill Immon

- [Bill Inmon - Data Warehousing in 2022 (Monday morning chat)](https://www.youtube.com/watch?v=-ckvw6I9KKw)
- [Building the data lakehouse, by Bill Inmon](https://www.databricks.com/resources/ebook/building-the-data-lakehouse)
- [Building the data warehouse, by Bill Inmon](https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445)

### Background

- **Bill Inmon**: Credited as the creator of the **data warehouse** and a pioneer in the modern data industry.
- **Early career**: Wrote his first program in 1965 at White Sands Missile Range, sparking a lifelong passion for computing.

### Data Warehouse Fundamentals

- **Definition**: A **data warehouse** is corporate data that provides a unified view across an organization.
- **Purpose**: Solves the limitations of siloed applications, enabling cross-departmental analysis (e.g., marketing, finance).
- **Transformation**: Involves converting application-specific data into a corporate format, facilitated by **ETL (extract, transform, load)**.

### ETL Technology

- **Origins**: Initially required manual coding (COBOL, PL/I) for repetitive data transformation tasks.
- **Automation**: Developed with Prism Solutions to streamline ETL, reducing drudgery and redundancy.
- **Key innovation**: Replaced custom programs with reusable tools for data extraction and integration.

### Challenges from IBM

- **IBM's initial dominance**: Fiercely opposed data warehousing, advocating only for transaction processing use cases.
- **Controversial article**: Inmon’s proposition that data could be used beyond transactions sparked backlash (e.g., accusations of "anarchy" and professional censorship).
- **Early adoption**: Marketing departments drove data warehouse adoption despite skepticism from technical leaders.

### Motivation and Innovations

- **Driving force**: Identifying gaps in industry practices, such as underutilized **textual data**, led to innovations like **textual ETL**.
- **Ed Yourdon’s legacy**: Highlighted the neglect of pioneers like **Ed Yourdon**, who revolutionized software design with **structured design and analysis**.

#### Legacy of Structured Design

- **Pre-Yourdon era**: Development was chaotic, with coding starting immediately without systematic planning.
- **Ed Yourdon’s contribution**: Introduced **structured design and analysis**, creating a methodical framework for application development still used today.

### Industry Recognition and Reflection

- **Human element**: Emphasizes the importance of remembering foundational contributors like Yourdon to understand industry evolution.
- **Overcoming resistance**: Persistence in challenging norms (e.g., IBM’s opposition) enabled transformative technologies like data warehousing.

### Key Takeaways

- **Data warehousing**: Emerged to solve fragmented data views, enabling cross-functional corporate insights.
- **Innovation through frustration**: Automation of ETL and textual ETL arose from repetitive manual processes.
- **Historical context**: Acknowledges pioneers like Ed Yourdon, whose work underpins modern software practices.

## Data Warehouse - Key Architecture Ideas

### Overview of Data Warehouses

- **OLTP systems** are inefficient for analytical queries due to row-based transactional data structures.
- **Bill Inmon's definition**: A data warehouse is **subject-oriented**, **integrated**, **non-volatile**, and **time-variant** for decision-making support.
  - **Subject-oriented**: Organizes data around business domains (e.g., customers, sales).
  - **Integrated**: Combines data from multiple sources into a consistent schema.
  - **Non-volatile**: Data is read-only, preserving historical records.
  - **Time-variant**: Stores current and historical data for trend analysis.

### ETL Pipeline & Data Ingestion

- **ETL (Extract, Transform, Load)**:
  - **Extract**: Pull data from OLTP systems (e.g., production databases).
  - **Transform**: Clean, standardize, and model data in a staging area (e.g., cloud storage).
  - **Load**: Transfer transformed data into the data warehouse.
- **Change Data Capture (CDC)**: Captures incremental changes (inserts, updates, deletions) to minimize source system impact.

#### Data Marts

- Subsets of the data warehouse tailored for specific departments (e.g., sales, finance).
- Use simpler, denormalized schemas for focused analytical queries.
- May include additional transformations to optimize query performance.

### Evolution of Data Warehouses

- **Traditional Limitations**: Early monolithic servers struggled with scalability and performance.
- **MPP (Massively Parallel Processing)**: Enabled parallel data scanning for faster analytics but required complex maintenance.
- **Modern Cloud Data Warehouses** (e.g., **Amazon Redshift**, **Google BigQuery**, **Snowflake**):
  - Separate compute and storage for scalable, cost-effective analytics.
  - Built for large datasets, making advanced analytics accessible to smaller organizations.

## Modern Cloud Data Warehouses

### Key Features of Modern Cloud Data Warehouses

- **Massively Parallel Processing (MPP)**: Enables parallelized computation across multiple nodes for scalable data processing.
- **Elastic Scalability**: Spin up/down compute clusters on demand to match workload requirements (vs. upfront costs of on-premises systems).
- **Columnar Storage Architecture**: Enhances analytical query performance through compression and column-based organization.
- **Separation of Compute and Storage**: Enables independent scaling and cost optimization (data stored in limitless object storage).

#### Amazon Redshift MPP Architecture Example

- **Cluster Structure**: Composed of a **leader node** (manages query planning) and **compute nodes** (execute queries).
- **Node Slices**: Compute nodes divide resources (CPU, memory, disk) into slices for parallel processing.
- **Query Workflow**:
  - **Leader node** parses queries, creates execution plans, and distributes tasks.
  - **Compute nodes/slices** process data in parallel, return results to the leader for aggregation.

### ELT Pattern Support

- **Extract, Load, Transform (ELT)**:
  - Load raw data directly into the warehouse before transformation.
  - Leverages MPP power for in-warehouse data transformations, speeding up ingestion.
- **Benefits**: Faster data availability for stakeholders vs. traditional ETL.

### Evolution Beyond Traditional Data Warehouses

- **Unstructured Data Limitations**:
  - Cloud data warehouses excel with structured/semi-structured data but lack native support for unstructured data (e.g., images, audio).
  - **Data Lakes** complement warehouses by storing raw, diverse data formats for ML/advanced analytics.

### Performance and Cost Benefits

- **High-Volume Analytics**: Scalable MPP architecture handles petabytes of data efficiently.
- **Cost Optimization**: Pay-as-you-go pricing model aligns with usage (no upfront infrastructure investment).
- **Modern Workload Support**: Integrates with machine learning and complex analysis pipelines.

## Data Lakes - Key Architectural Ideas

### Data Lake Fundamentals

- **Purpose**: Central repository for **structured**, **semi-structured**, and **unstructured data** (e.g., text, audio, video) without fixed schemas.
- **Schema-on-read**: Schema is applied during data retrieval (vs. **schema-on-write** in data warehouses).
- **Motivation**: Overcomes limitations of data warehouses for diverse data types and high-volume ingestion.

### Data Lake 1.0: Implementation & Shortcomings

#### Core Components

- **Storage**:
  - Initially used **Hadoop Distributed File System (HDFS)**.
  - Later shifted to **cloud object storage** (e.g., **Amazon S3**) for cost efficiency and scalability.
- **Processing Tools**:
  - **MapReduce**, **Apache Spark**, **Hive**, **Presto** for transformations and queries.

#### Key Challenges

- **Data Swamps**: Lack of governance led to disorganized, low-quality data.
- **Limited Data Management**:
  - No native **data cataloging** or **discovery tools**, complicating data retrieval.
  - Poor **data integrity/quality** controls (e.g., outdated or inaccurate data).
- **Write-Only Architecture**:
  - Difficulty executing SQL-like updates/deletes (critical for **GDPR compliance**).
- **Query Inefficiency**:
  - Complex operations (e.g., joins) required custom coding (e.g., MapReduce jobs).

#### Partial Success Cases

- Tech giants like **Netflix** and **Meta (Facebook)** built custom tools to extract value despite challenges.
- Most organizations struggled with high costs and operational complexity.

### Transition to Next-Generation Data Lakes

- **Emerging Solutions**: Improved tools for organizing, querying, and governing data lakes.
- **Focus Areas**: Enhanced data cataloging, schema enforcement, and compliance support.

## Next-Generation Data Lakes

### Data Lake Organization

#### Data Zones

- Organize data into **zones** based on processing stages to enhance governance and usability.
- **Common zone pattern**:
  - **Landing/Raw Zone**: Stores unprocessed raw data ingested from source systems.
  - **Cleaned/Transformed Zone**: Holds standardized, validated data with PII removed (e.g., using formats like **Parquet**, **Avro**, or **ORC**).
  - **Curated/Enriched Zone**: Contains business-logic-modeled data ready for consumption.
- **Flexibility**: Zone count and names adjust to compliance needs (e.g., stricter regulations may require 4+ stages).

#### Partitioning

- **Technique**: Splits datasets into manageable parts (e.g., by time, location) to optimize query performance.
- **Benefit**: Query engines scan only relevant partitions, reducing latency.

#### Data Catalog

- **Purpose**: Centralizes metadata (ownership, definitions, schema history) for discoverability and governance.
- **Features**:
  - Enables schema tracking and search across business terms.
  - Mitigates "data swamp" issues by providing clarity on data relationships.

### Challenges with Data Lakes + Warehouses

- **Dual-system overhead**: Requires costly ETL pipelines to move subsets of data lakes into warehouses.
- **Risks**: Increased failure points in pipelines lead to data quality and duplication issues.
- **Cost inefficiency**: Higher storage costs for warehouses and redundant processing.

### Data Lakehouse Architecture

- **Concept**: Merges benefits of **data lakes** (low-cost, multi-format storage) and **data warehouses** (high-performance queries).
- **Key Advantages**:
  - **Unified storage**: Eliminates redundant ETL steps between systems.
  - **Schema enforcement**: Combines schema-on-read flexibility with schema-on-write reliability.
  - **Cost-effectiveness**: Reduces overhead of maintaining separate systems.
- **Outcome**: Supports scalable analytics and machine learning on a single platform.

## Lab Walkthrough - Simple Data Lake with AWS Glue

### Lab Overview

- **Objective**: Transform raw JSON data in **Amazon S3** into **Parquet** format for efficient querying.
- **Key tasks**:
  - **Extract, Transform, Load (ETL)**: Use **AWS Glue jobs** defined via **Terraform**.
  - **Data cataloging**: Populate metadata via **Glue crawler** for SQL access via **Amazon Athena**.
- **Optional exploration**: Analyze effects of **compression** and **partitioning** on storage/performance.

### Data Processing Workflow

#### Product Reviews Transformation

- **Raw JSON fields**: Reviewer ID/name, product ID, review text/summary, rating, timestamp, and helpfulness metrics.
- **Transformations**:
  - **Split helpfulness** into `helpful_votes` and `total_votes`.
  - **Extract year/month** from timestamps for partitioning.
  - **Write to Parquet** in S3 with future partitioning.

#### Metadata Transformation

- **Raw JSON fields**: Product ID, category, related products, sales rank/category.
- **Transformations**:
  - **Unnest and split** sales rank into `sales_category` and `sales_rank`.
  - **Drop nulls** in numerical columns; replace nulls with empty strings in others.

### Terraform Configuration

#### IAM Roles and Policies

- **Role creation**:
  - Grant **Glue** permissions to access S3 via `assume_role_policy`.
  - **Permissions**: Allow all S3 actions (read, write, list) on the lab bucket.
- **Policy details**:
  - Applied via `aws_iam_role_policy` for granular control over resources.

#### Glue Jobs Setup

- **Infrastructure as Code**:
  - **Two Glue jobs** defined for reviews/metadata processing (Python shell scripts).
  - **Resource arguments**:
    - **Scripts** stored in a dedicated S3 bucket.
    - **Worker configuration**: Define Spark worker count and type for parallelism.
    - **Timeout**: Prevents long-running jobs due to errors.
  - **Job parameters**:
    - **Paths**: Source (raw JSON) and destination (Parquet) in S3.
    - **Compression**: Optional selection (e.g., Snappy, GZIP).
    - **Partition columns**: Year/month for optimized querying.

### Querying and Outputs

- **Athena integration**:
  - Query curated Parquet data after Glue crawler catalogs it.
- **Terraform outputs**:
  - **Glue job names** and **IAM role ARN** for reuse in crawler setup.

### Compression and Partitioning Exploration

- **Optional post-lab analysis**:
  - **Compression trade-offs**: Compare storage savings vs. query speed.
  - **Partitioning impact**: Measure performance gains using partitioned columns in queries.

### Data Transformation Process

#### Review Transformation Script

- **Script inputs**: Extracts arguments from **Glue job** (source path, target path, compression type, partition columns).
- **GlueContext with Spark**: Manages S3 connections, extracts data into **Glue/Pandas DataFrames**, enables distributed processing.
- **Critical steps**:
  - Read raw JSON from S3 into **DynamicFrame** via `createDynamicFrame`.
  - Split `helpful` field into **helpful_votes** and **total_votes**.
  - Extract **year/month** from timestamps for partitioning.
  - Convert data to **Parquet** format with optional compression (e.g., Snappy).

#### Metadata Transformation

- Follows similar workflow: Unnest **sales rank**, clean nulls, and structure data for analysis.

### Terraform and Resource Setup

- **Environment configuration**: Define variables (S3 bucket, regions) via CLI before execution.
- **Resource creation**:
  - **Glue jobs**: Defined in Terraform with worker count, timeout, and script paths.
  - **IAM roles**: Grant permissions for Glue to access S3 and orchestrate jobs.
- **Outputs**: Returns Glue job names and role ARN for crawler setup.

### Glue Job Execution and Data Cataloging

- **Post-Terraform steps**:
  - Run Glue jobs via CLI or console; monitor status in AWS Management Console.

### Experiment Overview

- **Objective**: Analyze impacts of **compression** (Snappy, Gzip, uncompressed) and **partitioning** on storage efficiency/query performance.

#### Experiment 1: Compression Impact (Uncompressed vs. Snappy)

- **Metadata dataset** tested with two configurations:
  - **Uncompressed**: Larger storage footprint, fewer Parquet files (auto-partitioned by AWS Glue).
  - **Snappy compression**: Reduced storage by ~65%, retained auto-partitioning.

#### Experiment 2: Compression Algorithms (Snappy vs. Gzip)

- **Snappy**: Faster processing, moderate compression (balance of speed/size).
- **Gzip**: 30-50% smaller storage vs. Snappy but slower due to higher CPU usage.
- **Trade-off**: Choose Gzip for long-term storage, Snappy for frequent queries.

#### Experiment 3: Partitioning Efficiency (Year/Month vs. None)

- **Review dataset** partitioned by `year/month`:
  - **Partitioned**: 556 Parquet files (organized by time), optimized for time-range queries in **Amazon Athena**.
  - **Non-partitioned**: 4 auto-partitioned files, less efficient for targeted queries.
- **Key benefit**: Partitioning reduces scanned data volume, lowering query costs.

#### Experiment 4: Poor Partitioning (ASIN Column)

- **ASIN (review ID)** as partition key:
  - Created thousands of tiny files (one per review).
  - Caused job timeout (>15 mins) due to excessive file handling overhead.
- **Lesson**: Avoid high-cardinality keys (e.g., IDs) to prevent performance degradation.

### Results and Best Practices

- **Compression**: Use Snappy for active data, Gzip for archival.
- **Partitioning**: Align with query patterns (e.g., time-based) to minimize scans.
- **Avoid**: Over-partitioning leads to "small files problem" (slow queries, high costs).

# Data Lakehouses

## The Data Lakehouse Architecture

- [What is a medalion architecture?](https://www.databricks.com/glossary/medallion-architecture)

### Core Components

- **Storage layer**: Built on **object storage** (e.g., Amazon S3), storing structured/semi-structured/unstructured data.
- **Data zones**: Organized using the **medallion architecture** (by Databricks):
  - **Bronze**: Raw, unprocessed data.
  - **Silver**: Cleaned, validated data (e.g., standardized schemas, PII removed).
  - **Gold**: Curated, business-modeled data for analytics/ML.

### Key Features

#### Data Management

- **Schema enforcement**: Ensures data adheres to formats/quality standards during ingestion, with **schema evolution** support.
- **ACID compliance**: Guarantees **atomicity, consistency, isolation, durability** for concurrent read/write operations.
- **File formats**: Transformed data stored as **Parquet** for efficiency (storage/query performance) and cross-engine accessibility.

#### Governance and Security

- **Access controls**: Granular permissions for datasets/tables.
- **Data lineage tracking**: Audits data lineage and version history.
- **Regulatory compliance**: Supports GDPR, CCPA via **incremental updates/deletions** and historical data rollbacks.

### Unified Analytics

- **Multi-workload support**: Combines:
  - **SQL/BI** reporting (like traditional warehouses).
  - **Machine learning** and big data processing (leveraging raw lake data).
  - **Connector APIs** for integration with external tools (e.g., Spark, Pandas).

### Industry Adoption

- [Exploring the Architecture of Apache Iceberg, Delta Lake, and Apache Hudi](https://www.dremio.com/blog/exploring-the-architecture-of-apache-iceberg-delta-lake-and-apache-hudi/)
- [Delta Lake](https://delta.io/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [Apache Hudi](https://hudi.apache.org/)
- **Vendors/Solutions**: Databricks, Delta Lake, AWS Lake Formation, Snowflake, Apache Iceberg.
- **Open-source contributions**: Tools simplifying schema management, metadata cataloging, and ACID transactions in lakehouse architectures.

## Data Lakehouse Implementation

- [Modern data lake centric analytics platform (AWS)](https://docs.aws.amazon.com/whitepapers/latest/aws-serverless-data-analytics-pipeline/logical-architecture-of-modern-data-lake-centric-analytics-platforms.html)

### Converged Architectures: Cloud Warehouses vs. Data Lakes

- **Hybrid evolution**:
  - **Cloud data warehouses** now integrate **data lake features** (e.g., unstructured data support).
  - **Data lakes** adopt **warehouse-like traits** (schema enforcement, SQL querying).
- **Goal**: Unify cost-efficient storage (lakes) with transactional/analytical efficiency (warehouses).

### Open Table Formats for Lakehouse Implementation

- [What is an Open Table Format? & Why to use one?](https://www.startdataengineering.com/post/what_why_table_format/)
- **Key solutions**: **Delta Lake**, **Apache Iceberg**, **Apache Hudi**.
- **Core features**:
  - **ACID compliance**: Atomic updates, deletions, and consistency.
  - **Time travel**: Access historical data via snapshots; rollback capabilities.
  - **Schema/partition evolution**: Modify schemas/partitions without breaking queries.
- **Cross-engine compatibility**: Open formats (e.g., Parquet) allow diverse tools (Spark, Presto) to access data without duplication.

### Inside Apache Iceberg

#### Metadata Architecture

1. **Storage layer**: Data stored as **Parquet files** (optimized for query efficiency).
2. **Manifest files**:
   - Track data file metadata (e.g., stats, partitions) for each update.
3. **Manifest lists**: Map manifest files to snapshots.
4. **Metadata layer**:
   - **JSON metadata files**: Define schemas, partitions, and current snapshot pointers.
   - **Catalog pointers**: Direct queries to the latest metadata version.

#### Query Workflow

- **Process**:
  1. Query engine checks Iceberg catalog for the **current metadata file**.
  2. Retrieves relevant **manifest list** → **manifest files** → **Parquet data files**.
- **Optimization**: Skips irrelevant files using metadata (faster queries).

### Choosing Between Warehouses, Lakes, or Lakehouses

- **Early-stage companies**:
  - Use **read replicas** of production DBs for small structured datasets.
- **Growing data volume**:
  - **Cloud data warehouses** (Snowflake, Redshift): Centralize structured/semi-structured data for BI/reporting.
- **Unstructured data/ML needs**:
  - **Data lakes** (low-cost) with evolution to **lakehouse** for ACID transactions and governance.

### The Future of Unified Data Platforms

- **Convergence trend**:
  - **Blended capabilities**: Future platforms merge warehouse/lake strengths (e.g., Snowflake’s Iceberg integration).
  - **Flexible adoption**: Choose architectures based on use cases (ML, real-time analytics, compliance).
- **Lakehouse dominance**: Expected for organizations needing **unified storage, governance, and multi-workload support** (SQL, ML, streaming).

## Lakehouse Architecture on AWS

### Core Components

#### AWS Lake Formation

- **Purpose**: Automates setup/management of **data lakes** and **lakehouses**, reducing manual steps for governance, access, and cataloging.
- **Key workflows**:
  - **Data ingestion**: Moves data from sources (S3, RDBMS, NoSQL) into the lakehouse.
  - **Cataloging**: Uses **AWS Glue crawlers** to catalog metadata.
  - **Permissions**: Centralizes **IAM policies** for fine-grained access control (e.g., column/row-level security).

#### Integration with Existing Tools

- **Built on AWS Glue**: Leverages **Glue ETL jobs**, workflows, and crawlers for data transformations and catalog synchronization.
- **IAM**: Enables secure, policy-based access to S3 datasets and metadata.

### Architectural Layers

#### Storage Layer

- **Amazon S3**: Primary storage for raw/processed data (cost-effective, scalable).
- **Amazon Redshift**: Stores modeled/aggregated datasets for high-performance queries.

#### Processing Layer

- **Transformation tools**:
  - **AWS Glue**: Serverless ETL jobs.
  - **Amazon EMR/Managed Flink**: Big data processing (Spark, Hadoop).
  - **Redshift SQL**: Transform data directly in Redshift clusters.

#### Catalog Layer

- **AWS Glue Data Catalog**: Central metadata repository managed via **Lake Formation**.
  - Tracks schemas, partitions, and data lineage.
  - **Governance**: Enforces access policies (e.g., restrict sensitive columns).

#### Consumption Layer

- **Analytics**:
  - **Amazon Athena**: SQL queries on S3 data.
  - **Redshift Spectrum**: Query S3 datasets directly from Redshift.
- **BI & ML**:
  - **QuickSight**: Visualizations/dashboards.
  - **SageMaker**: Train models on lakehouse data.

### Key Advantages

- **Simplified governance**: Lake Formation automates **data lineage tracking** and schema versioning.
- **Unified access**: Combines **structured** (Redshift) and **unstructured** (S3) data for cross-use-case analytics.
- **Cost optimization**: Decoupled storage (S3) and compute (Redshift/Athena) for scalable workloads.

## Implementing a Lakehouse on AWS

### Storage Layer

- **Key components**: **Amazon S3** (data lake) and **Amazon Redshift** (data warehouse) form a dual storage approach.
- **Amazon S3**: Stores **structured, semi-structured, and unstructured data** with **cost efficiency** and scalability.
- **Amazon Redshift**: Stores **highly curated, structured/semi-structured data** with predefined schemas for analytics.
- **Redshift Spectrum**: Enables querying data in **S3** directly from Redshift, avoiding **ETL pipelines** and data redundancy.

### Catalog Layer

- **AWS Lake Formation**: Manages metadata via **AWS Glue** for centralized data discovery and governance.
- **Glue Crawlers**: Automatically detect datasets, update schema/partition information, and maintain metadata.
- **Dynamic metadata**: Regularly updated to handle **evolving schemas** and new partitions in storage layers.

#### Apache Iceberg Tables

- Support **schema versioning** and **time travel** for tracking historical data changes.
- Integrate with **AWS Glue** using Parquet format, enabling schema evolution without disrupting existing processes.

### Consumption Layer

- Querying services enable unified access to data across S3 and Redshift.

#### Redshift Spectrum

- **Unified SQL interface**: Combines datasets from **S3 (data lake)** and **Redshift (data warehouse)** in a single query.
- **Benefits**:
  - Reduces **data latency** by querying data in place.
  - Supports **massive parallelism** for large datasets.
  - Avoids data duplication; multiple clusters can query the same S3 dataset.

#### Amazon Athena

- **Serverless SQL queries**: Directly query **S3** data without infrastructure management; pay per query.
- **Federated queries**: Access data in **Redshift** and other sources via connectors.
- **Schema on read**: Uses metadata from the **Glue catalog** to apply schemas during queries.

### Key Advantages

- **Integration**: **Redshift Spectrum** and **Athena** eliminate ETL pipelines, reducing costs and errors.
- **Cost efficiency**: S3 for bulk storage; Redshift for structured analytics.
- **Flexibility**: **Iceberg tables** and **Glue crawlers** handle schema evolution and metadata management.

## Lab Walkthrough - Building a Data Lakehouse with AWS Lake Formation and Apache Iceberg

- [AWS Lake Formation](https://aws.amazon.com/lake-formation/features/)
- [Lake Formation set up](https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html)
- [AWS post: Use AWS Glue ETL to perform merge, partition evolution, and schema evolution on Apache Iceberg](https://aws.amazon.com/blogs/big-data/use-aws-glue-etl-to-perform-merge-partition-evolution-and-schema-evolution-on-apache-iceberg/)
- [Using the Iceberg framework in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html)

### Data Lake Architecture

- **Medallion-like architecture** using **Amazon S3** as underlying storage with three zones: **landing zone**, **curated zone**, and **presentation zone**.
- Governed by **AWS Lake Formation** for fine-grained permissions and metadata management.
- Utilizes **AWS Glue Data Catalog** for table metadata and **Apache Iceberg** for table formats.

### Data Ingestion (Landing Zone)

- **Source Systems**:
  - **RDS MySQL** database with classic models data (8 tables exported as CSV files).
  - **S3 bucket** containing JSON customer ratings.
- **Storage Details**:
  - CSV files stored under `landing-zone/rds/<table_name>`.
  - JSON ratings stored under `landing-zone/json/ratings` with added `ingest_ts` timestamp.

### Data Processing (Curated Zone)

#### CSV Processing (Transformation 1)

- **Schema enforcement**: Cast columns to predefined types and add metadata (`ingest_ts` and `source`).
- **Storage**: Parquet files using Snappy compression stored in `curated-zone/<table_name>`.
- **Glue Catalog**: Metadata written to tables in the **curated_zone** database.

#### ML Data Preparation (Transformation 2)

- **Data combination**: Join customer/product info from CSV tables with JSON ratings.
- **Storage**: Iceberg format in `curated-zone/ratings4ml/iceberg` for schema evolution and time travel.

#### Ratings Update (Transformation 3)

- **UPSERT logic**: Update existing customer-product rating pairs.
- **Storage**: Iceberg format in `curated-zone/ratings/iceberg`.

### Data Storage Formats

- **Landing Zone**: Raw CSV/JSON files.
- **Curated Zone**:
  - Processed data as Parquet files (schema-enforced).
  - Iceberg tables for ML/ratings data.
- **Presentation Zone**:
  - **Iceberg tables** for analytics (e.g., average sales by month) and ML (e.g., ratings4ml).

### Catalog Integration & Querying

- **AWS Glue Data Catalog**:
  - Curated and presentation zone databases map to S3 data locations.
- **Amazon Athena**:
  - Creates presentation tables via SQL (e.g., `CREATE TABLE ... AS SELECT ...`).
  - Associates tables with **presentation_zone** database.

### Governance & Tooling

- **AWS Lake Formation**: Manages access controls for S3 data.
- **Terraform Automation**:
  - Defines **Glue ETL jobs** for ingestion and transformations.
  - Preconfigured IAM roles and

### Apache Iceberg File Structure

- **Metadata Layer**:
  - Stores JSON metadata files (schema, table location, timestamps) and Avro manifest lists/manifests.
  - Snapshot UUIDs track table updates.
- **Data Layer**:
  - **Parquet files** store actual data under `data/` prefix in S3.

#### Catalog Layer

- Managed via **AWS Glue Data Catalog**, linking Iceberg tables to S3 paths.
- **Curated zone** and **presentation zone databases** organize catalog tables.

### Schema Evolution & Time Travel

- **Schema Evolution**:
  - Adding columns (e.g., optional lab task) updates only metadata files, not data files.
- **Time Travel**:
  - Query historical table versions using snapshot IDs.

### AWS Lake Formation Governance

- **Fine-Grained Permissions**:
  - **Metadata-level**: Control access to Glue databases, tables, and columns.
  - **Storage-level**: Restrict access to underlying S3 data.
- **Permission Model**:
  - Augments (not replaces) IAM policies. Users still need:
    - **IAM policies** for Glue, Athena, and Lake Formation services.
    - **Lake Formation grants** for specific datasets.

#### Lab Implementation Example

- **Lab Role**: Full access to all catalog tables and data.
- **ML User Role**: Restricted to `ratings4ml` table in the presentation zone.
- Permissions enforced via Lake Formation, overriding broad IAM policies.

### Optional Lab Features

- Explore **Terraform** for schema changes (`alter_table` module).
- Validate permissions by testing ML user access restrictions.

# Week 2 Quiz

## Questions

1. According to Bill Inmon’s definition of a data warehouse presented in this week’s videos, which of the following are attributes of a data warehouse? Select all that apply.
   1. Time-variant
   2. Subject-oriented
   3. Schema flexibility
   4. Integrated
   5. Nonvolatile
2. Which of the following are features that distinguish cloud data warehouses like Amazon Redshift, Google BigQuery, and Snowflake from traditional on-premises data warehouse solutions? Select all that apply.
   1. They allow for on-demand scaling of compute clusters, expanding on the massively parallel processing (MPP) architecture.
   2. They are based on OLTP systems rather than OLAP.
   3. They separate compute from storage.
   4. They eliminate the need for ETL processes.
3. What was the primary challenge associated with the first-generation data lakes, or "data lake 1.0"?
   1. They primarily supported structured data, limiting versatility.
   2. They lacked effective data management, leading to data swamps.
   3. They were too expensive to implement due to the high cost of storage.
4. According to this week’s videos, which of the following statements about data zones is/are true? Select all that apply.
   1. Data zones are used to organize data, where each zone houses data that has been processed to varying degrees.
   2. The first zone is called the “landing zone” and the last zone is called the “curated zone”
   3. The number of zones should be up to 3.
   4. After processing the raw data, the processed data is typically stored using open file formats in the zones associated with the transformed data.
5. Data partitioning divides a dataset into smaller, more manageable parts based on a set of criteria. What is the purpose of data partitioning?
   1. To convert data into open file formats like Parquet, Avro, or ORC
   2. To segregate sensitive information into separate partitions, helping you comply with data privacy regulations
   3. To improve query performance
   4. To reduce the amount of data that needs to be cleaned and transformed
6. Which of the following is/are true about the data lakehouse architecture? Select all that apply.
   1. It uses proprietary file formats to store data.
   2. It combines the flexible and low-cost storage benefits of a data lake with the superior query performance and robust data management of a data warehouse into one unified architecture.
   3. It typically adheres to the ACID principles.
7. Which of the following is true about open table formats?
   1. They provide you with only the most current snapshot of the data, ensuring a single source of truth.
   2. They are used to store processed data in the cleaned and curated zones of the storage layer in a data lakehouse.
   3. They store operations on your data tables as a series of snapshots that reflect the state of the data at a given time.
   4. They enforce a single strict schema and way of partitioning, ensuring data integrity.
8. Suppose that you want to perform fast analytical queries on large volumes of tabular data collected from multiple departments such as marketing, sales, and product. Which of the following is the most suitable storage option for this scenario?
   1. A data lake
   2. A transactional database
   3. A data warehouse
9. Suppose that you want to store a large volume of video and image data that you will later use to train a deep learning model. Which of the following is the most suitable storage option for this scenario?
   1. A transactional database
   2. A data warehouse
   3. A data lake

## Answers

1. 1, 2, 4 & 5
   1. A data warehouse stores current and historical data.
   2. A data warehouse organizes and stores data around key subjects or domains of the business.
   3. A data warehouse brings together data from different sources and ensures that it is stored in a consistent way with a predefined schema.
   4. The data in a data warehouse is read-only and cannot be deleted or updated in the general sense, persevering historical records.
2. 1 & 3
   1. Cloud data warehouses typically implement a massively parallel processing (MPP) architecture. Unlike with on-premises data warehouse solutions where you need to appropriately size and set up an MPP system up front, cloud data warehouses allow you to spin up compute clusters on demand.
   2. By separating compute from storage, you can scale processing and storage resources independent of each other and pay only for the resources you use.
3. 2
   1. First-generation data lakes became data swamps due to the absence of proper data management practices. This included challenges like the lack of data cataloging and discovery tools, making it difficult to locate or understand data relationships, or ensure data integrity and quality.
4. 1 & 4
   1. This is how data zones are described in this week’s videos.
   2. Open file formats, such as Parquet, store data in a standard format on disk more efficiently. They also allow a wide range of analytical engines to directly access the data.
5. 3
   1. With data partitions, the query engine only needs to scan the partitions that contain the data relevant to the query, resulting in faster query performance.
6. 2 & 3
   1. This enables your data users to concurrently read, insert, update, and delete data, while ensuring that your data is reliable for analytical processes.
7. 3
8. 3
9. 3
