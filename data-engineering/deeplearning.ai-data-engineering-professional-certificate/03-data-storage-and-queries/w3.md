- [Batch Queries](#batch-queries)
  - [The Life of a Query](#the-life-of-a-query)
    - [DBMS Architecture Components](#dbms-architecture-components)
      - [Query Parser](#query-parser)
      - [Query Optimizer](#query-optimizer)
      - [Execution Engine](#execution-engine)
    - [Query Execution Plans](#query-execution-plans)
      - [Execution Plan Example (Full Table Scan)](#execution-plan-example-full-table-scan)
      - [Execution Plan Example (Index Scan)](#execution-plan-example-index-scan)
    - [Execution Plan Features](#execution-plan-features)
  - [Index Deep Dive](#index-deep-dive)
    - [Overview of Indexes](#overview-of-indexes)
      - [B-Tree Index Structure](#b-tree-index-structure)
    - [Query Performance with Indexes](#query-performance-with-indexes)
    - [Index Analysis Example](#index-analysis-example)
    - [Indexing in Columnar Storage](#indexing-in-columnar-storage)
    - [Key Takeaways](#key-takeaways)
  - [Additional Index Examples](#additional-index-examples)
    - [Example 1](#example-1)
    - [Example 2](#example-2)
  - [Retrieving Only the Data You Need](#retrieving-only-the-data-you-need)
    - [Risks of `SELECT *` Queries](#risks-of-select--queries)
    - [Pruning Techniques for Query Optimization](#pruning-techniques-for-query-optimization)
      - [Row-Based Pruning](#row-based-pruning)
      - [Column-Based Pruning](#column-based-pruning)
      - [Partition Pruning](#partition-pruning)
    - [General Best Practices](#general-best-practices)
  - [The Join Statement](#the-join-statement)
    - [Common Join Implementation Methods](#common-join-implementation-methods)
      - [Nested Loop Join](#nested-loop-join)
      - [Index-Based Nested Loop Join](#index-based-nested-loop-join)
      - [Hash Join](#hash-join)
    - [Data Modeling Considerations](#data-modeling-considerations)
      - [Impact of Schema Design](#impact-of-schema-design)
    - [Challenges with Joins](#challenges-with-joins)
      - [Many-to-Many Relationships](#many-to-many-relationships)
      - [Query Performance](#query-performance)
    - [Best Practices](#best-practices)
  - [Aggregate Queries](#aggregate-queries)
  - [Designing Systems for Analytical Workloads](#designing-systems-for-analytical-workloads)
    - [Aggregate Query Support](#aggregate-query-support)
    - [Row-Oriented vs Columnar Storage](#row-oriented-vs-columnar-storage)
  - [Amazon Redshift Cloud Data Warehouse](#amazon-redshift-cloud-data-warehouse)
    - [Columnar Storage](#columnar-storage)
    - [Massively Parallel Processing (MPP)](#massively-parallel-processing-mpp)
    - [Table Design Considerations](#table-design-considerations)
      - [Distribution Styles](#distribution-styles)
      - [Sort Keys](#sort-keys)
    - [Performance Takeaways](#performance-takeaways)
  - [Lab Walkthrough - Comparing the Query Performance Between Row and Columnar Storage](#lab-walkthrough---comparing-the-query-performance-between-row-and-columnar-storage)
    - [Lab Experiment Overview](#lab-experiment-overview)
      - [Key Tasks in the Lab](#key-tasks-in-the-lab)
    - [Key Findings](#key-findings)
      - [Analytical Query Performance](#analytical-query-performance)
      - [Write/Delete Query Performance](#writedelete-query-performance)
    - [Lab Execution Notes](#lab-execution-notes)
  - [Additional Query Strategies](#additional-query-strategies)
    - [Complex Query Example: DVD Rental Database](#complex-query-example-dvd-rental-database)
    - [Query Optimization Strategies](#query-optimization-strategies)
      - [Query Caching](#query-caching)
      - [Common Table Expressions (CTEs)](#common-table-expressions-ctes)
    - [Database Maintenance Techniques](#database-maintenance-techniques)
      - [Vacuuming](#vacuuming)
        - [Issues Caused by Dead Records](#issues-caused-by-dead-records)
    - [Key Takeaways](#key-takeaways-1)
- [Streaming Queries](#streaming-queries)
  - [Queries Streaming Data](#queries-streaming-data)
    - [Streaming Data Challenges](#streaming-data-challenges)
    - [Stream Processing Systems](#stream-processing-systems)
      - [Windowed Query Types](#windowed-query-types)
        - [Session Window](#session-window)
        - [Fixed-Time (Tumbling) Window](#fixed-time-tumbling-window)
        - [Sliding Window](#sliding-window)
    - [Stream Joining and Enrichment](#stream-joining-and-enrichment)
  - [Deploying an Application with Amazon Managed Service for Apache Flink](#deploying-an-application-with-amazon-managed-service-for-apache-flink)
    - [Deployment Options on AWS](#deployment-options-on-aws)
      - [Managed Service Advantages](#managed-service-advantages)
    - [Core Concepts \& Demo Workflow](#core-concepts--demo-workflow)
      - [Creating a Flink Application](#creating-a-flink-application)
      - [Studio Notebooks](#studio-notebooks)
    - [Monitoring \& Visualization](#monitoring--visualization)
  - [Deploying a Studio Notebook with Amazon Managed Service for Apache Flink](#deploying-a-studio-notebook-with-amazon-managed-service-for-apache-flink)
    - [Flink Connectors Overview](#flink-connectors-overview)
    - [Creating a Studio Notebook with Amazon Managed Service for Flink](#creating-a-studio-notebook-with-amazon-managed-service-for-flink)
      - [Studio Notebook Components](#studio-notebook-components)
    - [Integration with AWS Glue](#integration-with-aws-glue)
- [Week 3 Quiz](#week-3-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Batch Queries

## The Life of a Query

### DBMS Architecture Components

- **Transport System**: Receives queries and passes them to the query processor.
- **Query Processor**: Manages query parsing, validation, optimization, and execution.

#### Query Parser

- Breaks queries into tokens (keywords, table/attribute names, operators).
- **Validation steps**: Checks syntax, verifies table/attribute existence, and validates user access permissions.
- Converts SQL into **byte code** (machine-readable steps for query execution).

#### Query Optimizer

- Generates multiple **execution plans** based on operations, indexes, and data scan size.
- Calculates **cost values** (e.g., I/O, computation, memory) for each plan.
- Selects the **least expensive plan** to optimize resource efficiency.

#### Execution Engine

- Executes the chosen plan and returns query results.

### Query Execution Plans

- **Key purpose**: Understand performance or troubleshoot slow queries.
- Accessible via commands like **`EXPLAIN`**, which details steps, costs, and statistics.

#### Execution Plan Example (Full Table Scan)

- **PostgreSQL example**: `EXPLAIN SELECT * FROM customer;`
  - **Sequential scan**: Scans entire table.
  - **Cost metrics**:
    - **Startup cost**: 0 units.
    - **Total cost**: 14.99 units.
  - **Result metrics**: 599 rows returned, with row width of 26 bytes.

#### Execution Plan Example (Index Scan)

- **PostgreSQL example**: `EXPLAIN SELECT * FROM customer WHERE customer_id = 3;`
  - **Index scan**: Uses index on `customer_id` column for faster lookup.
  - **Total cost**: Lower than full table scan (0.28 units vs. 14.99).

### Execution Plan Features

- **Index usage**: Metadata structures reduce scan time by efficiently locating data.
- **Applicability**: Supported in relational databases, NoSQL systems, data warehouses, and lakehouses.

## Index Deep Dive

- [Use the index luke](https://use-the-index-luke.com/sql/table-of-contents)

### Overview of Indexes

- An **index** is a separate, ordered data structure that improves query speed by enabling efficient data retrieval (like a book’s alphabetical index).
- Stores data from one or more columns in a defined order, allowing binary search instead of full table scans.
- **Query optimizer** uses indexes to reduce query cost by choosing optimal execution plans.

#### B-Tree Index Structure

- Indexes are implemented using a **balanced search tree (B-tree)** with linked nodes.
- **Leaf nodes** store sorted index entries and reference table rows, while **branch nodes** guide traversal.
- Enables **efficient updates** (e.g., inserting/deleting entries) while maintaining order via doubly linked blocks.

### Query Performance with Indexes

- **Example**: Querying orders from “Canada” traverses the B-tree from the root to leaf nodes, reducing scan time from O(n) to O(log n).
- **Impact of duplicates**: Non-unique indexes may require horizontal leaf-node scans, potentially negating efficiency gains.
- **Best practices**:
  - Create indexes on columns used in frequent, performance-sensitive queries.
  - Avoid overloading tables with unnecessary indexes, as maintenance impacts write performance.

### Index Analysis Example

- **Without index**: A query on `rental_id` in a payment table uses a sequential scan (high cost).
- **With index**: Creating an index on `rental_id` reduces query cost by ~30x via an index scan.

### Indexing in Columnar Storage

- **Amazon Redshift**: Uses **sort keys** to physically order rows on disk, similar to indexes, reducing scanned rows for queries.
- **BigQuery**: Implements **cluster keys** for the same purpose (equivalent to sort keys).
- **Benefits**: Enhances query performance by leveraging sorted data organization in columnar storage systems.

### Key Takeaways

- **Indexes** and **sort/cluster keys** are critical for optimizing read-heavy workloads.
- Design choices (e.g., selecting appropriate columns, balancing read/write performance) directly impact database efficiency.

## Additional Index Examples

In these examples, the [`classicmodels` database](https://www.mysqltutorial.org/getting-started-with-mysql/mysql-sample-database/) was used.

### Example 1

Consider the following query:

```sql
SELECT first_name, last_name
FROM employees
WHERE employee_id = 123
```

If `employee_id` is the primary key , then an index is automatically created on the primary key. So if you check the query plan of such statement, you will see that the DBMS will use the index to scan the table and get the results.

![dbms-query-plan-index-scan](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/dbms-query-plan-index-scan.png)

### Example 2

Consider the following query:

```sql
SELECT productcode, priceeach
FROM orderdetails
WHERE ordernumber = 10101
```

The `orderdetails` table has a composite primary key, which consists of (`productcode`, `ordernumber`). The index created is on the `productcode` column. So if you check the query plan of such statement, you will see the DBMS will perform a full table scan.

![dbms-query-plan-full-table-scan-seq-scan](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/dbms-query-plan-full-table-scan-seq-scan.png)

Now if you instead choose the composite primary key to consist of (`ordernumber`, `productcode`), then the table will create an index on the `ordernumber` column. The DBMS will use the index to get the results for this query.

![dbms-query-plan-index-scan-composite-primary](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/03-data-storage-and-queries/assets/dbms-query-plan-index-scan-composite-primary.png)

## Retrieving Only the Data You Need

### Risks of `SELECT *` Queries

- **Full table scans**: Occur when querying all columns/rows without filters (`SELECT *`), leading to high resource usage.
- **Production database impact**: Example shows an analyst's `SELECT *` command crashing a critical system for 3 days.
- **Cloud cost implications**: **Cloud pay-as-you-go models** charge for data scanned and compute resources used during full-table queries.

### Pruning Techniques for Query Optimization

#### Row-Based Pruning

- **Use case**: Filter rows using `WHERE` clauses (e.g., `rental_id = 1`).
- **Performance boost**: Leverage **indexes** (traditional databases) or **sort/cluster keys** (columnar storage like **BigQuery** or **Amazon Redshift**).

#### Column-Based Pruning

- **Key practice**: Specify only required columns (e.g., `SELECT customer_id, rental_id`).
- **Benefit**: Reduces scanned data by excluding irrelevant columns.

#### Partition Pruning

- **Implementation**: Partition data by keys like date/location (e.g., partition by `order_date` in **BigQuery**).
- **Efficiency**: Queries filter on partitions first (e.g., `order_date = '2023-04-01' AND country = 'USA'`) to limit scans.

### General Best Practices

- **Rule of thumb**: Query only necessary data to avoid performance degradation and costs.
- **Storage considerations**: Use partitioning and filtering to minimize data scanning.

## The Join Statement

- **Inner join**: Combines rows from two tables based on matching columns (e.g., `customer_id`).
- Crucial for merging normalized datasets but resource-intensive due to row scanning.

### Common Join Implementation Methods

#### Nested Loop Join

- Scans every row in one table against all rows in another, creating nested iterations.
- Default method but slow for large datasets due to full-table scans.

#### Index-Based Nested Loop Join

- Uses **B-tree indexes** to quickly locate matching rows, reducing full-table scans.
- Requires pre-existing indexes on join columns (e.g., `customer_id`).

#### Hash Join

- Splits tables into **hash buckets** using a hash function, then matches rows within buckets.
- Efficient for large datasets, especially when parallelized.

### Data Modeling Considerations

#### Impact of Schema Design

- **Normalized schemas**: Reduce redundancy but require complex joins (e.g., joining 3+ tables for analytics).
- **Star schema**: Simplifies queries by pre-joining data into fact and dimension tables (e.g., `fact_orders` + `dim_locations`).
- **Denormalized tables**: Eliminate joins by combining attributes but increase redundancy.

### Challenges with Joins

#### Many-to-Many Relationships

- **Row explosion**: Occurs when joining tables like `payments` and `orders` on non-unique keys (e.g., `customer_id`), creating redundant rows.
- **Mitigation**: Use intermediate mapping tables to link related entities (e.g., `payment_order_map`).

#### Query Performance

- Joins are resource-intensive due to **full-table scans** or **index lookups**.
- Pre-aggregating data or optimizing schemas reduces query complexity for end-users.

### Best Practices

- **Model data** to minimize necessary joins for common queries.
- **Monitor query plans** to avoid unintended row explosion.
- **Use indexes** and **hash buckets** to accelerate large-scale joins.

## Aggregate Queries

## Designing Systems for Analytical Workloads

### Aggregate Query Support

- **Aggregate queries**: Compute summary values (e.g., **min**, **max**, **average**, **count**) from columns across large datasets.
- **Optimization options**: Use full table scans or **B-tree indexes** (if available) to expedite queries like finding minimum values.
- **GROUP BY clause**: Enables grouping results by specific columns (e.g., country) to calculate summary values per group.
- **Partitioning methods**: Achieved via sorting, hash functions, or leveraging indexes on grouping attributes.

### Row-Oriented vs Columnar Storage

- **Row-oriented storage**:

  - Stores entire rows contiguously on disk.
  - Inefficient for analytical queries (requires transferring entire rows to access specific columns).
  - Suitable for small datasets but scales poorly for large analytical workloads.

- **Columnar storage**:
  - Stores column values contiguously on disk.
  - Transfers only relevant columns for analytical queries (e.g., price), reducing I/O overhead.
  - Ideal for large datasets and **aggregate queries** due to efficiency in data retrieval.

## Amazon Redshift Cloud Data Warehouse

- [Sort keys](https://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data.html)
- [`EXPLAIN`](https://docs.aws.amazon.com/redshift/latest/dg/r_EXPLAIN.html)
- [Factors affecting query performance](https://docs.aws.amazon.com/redshift/latest/dg/c-query-performance.html)
- [Amazon Redshift best practices for designing tables](https://docs.aws.amazon.com/redshift/latest/dg/c_designing-tables-best-practices.html)
- [Amazon Redshift best practices for designing queries](https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html)
- [Choose the best sort key](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)
- [Choose the best distribution style](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html)

### Columnar Storage

- **Columnar data storage**: Stored column-by-column, optimized for aggregations in **OLAP workloads**.
- **Benefits**:
  - **Efficient analytical queries**: Scans only relevant columns for filters/aggregations.
  - **Enhanced data compression**: Similar column values compress well, reducing storage/disk I/O.
  - Operates on compressed data directly in memory for faster query execution.

### Massively Parallel Processing (MPP)

- **Cluster components**:
  - **Leader node**: Coordinates queries, generates execution plans.
  - **Compute nodes**: Store data subsets and execute query steps in parallel.
  - **Slices**: Subdivide compute nodes to process portions of data independently.
- **Query execution flow**:
  1. Leader node parses queries, creates execution plans, and distributes tasks.
  2. Slices process data locally, minimizing cross-node communication.
  3. Leader aggregates results into a final output.
- **Scaling**: Performance improves with more nodes but depends on workload balance.

### Table Design Considerations

#### Distribution Styles

- **Goals**:
  - **Even data distribution**: Avoid **data skew** for balanced workload.
  - **Minimize cross-node data movement**: Reduce network traffic during joins/aggregations.
- **Distribution options**:
  - **KEY**: Distribute rows based on a column value, ideal for join-heavy tables.
  - **EVEN**: Round-robin distribution for tables without frequent joins.
  - **ALL**: Full copy on all nodes; best for small, slow-moving tables joined with large tables.
  - **AUTO (default)**: Redshift chooses optimal style based on table data.

#### Sort Keys

- **Sort key purpose**: Physically orders data on disk for faster filtering/joins.
- **Usage**:
  - Filters or joins on sort key columns reduce disk I/O via zone maps (metadata filtering).
  - Example: Sorting sales data by **order_date** optimizes date-range queries.
- Functions similarly to OLTP database indexes but optimized for columnar scans.

### Performance Takeaways

- **Optimization requires alignment**: Combine **columnar storage**, **MPP**, and **table design** (distribution style/sort keys) for peak efficiency.
- **Best practices**:
  - Use **KEY distribution** for large joined tables, **ALL** for small static tables.
  - Define **sort keys** aligned with frequent query filters/joins.
  - Avoid **data skew** and excessive cross-node data shuffling.
- **Trade-offs**: **ALL distribution** increases storage/load time, while **AUTO** simplifies decisions for dynamic workloads.

## Lab Walkthrough - Comparing the Query Performance Between Row and Columnar Storage

### Lab Experiment Overview

- Analyzes **execution times** of **analytical**, **insert**, and **delete queries** on **PostgreSQL** (row-based) and **Redshift** (column-based) databases.
- Uses **TPCH benchmark** datasets and queries to simulate business questions and workload scenarios.
- **Dataset structure**: Includes orders, line items, customers, and suppliers linked via an entity-relationship diagram (ERD).

#### Key Tasks in the Lab

- Compare runtime of analytical queries (TPCH benchmark) between databases.
- Insert 50 random rows into line item tables and measure performance.
- Delete these rows and compare execution times.

### Key Findings

#### Analytical Query Performance

- **Column-based (Redshift)**: All queries completed in **milliseconds to seconds**.
- **Row-based (PostgreSQL)**: All queries took **minutes** to execute.
- Conclusion: **Columnar storage excels at analytical workloads** due to efficient column-oriented data retrieval.

#### Write/Delete Query Performance

- **Row-based (PostgreSQL)**: Inserting/deleting 50 rows was **significantly faster**.
- **Column-based (Redshift)**: Slower for **write/delete operations** due to columnar storage structural overhead.
- Conclusion: **Row-based storage is better suited for transactional workloads** involving frequent updates.

### Lab Execution Notes

- **Connecting to Redshift**: Similar workflow to traditional databases despite being a data warehouse.
- **Recommendation**: Run queries multiple times for reliable averages (optional for time/cost constraints).
- **Expected outcomes**: Clear performance differences between row and column storage for specific query types.

## Additional Query Strategies

### Complex Query Example: DVD Rental Database

- Calculates **total spending** across **film categories** (family, drama, comedy) using the **DVD rental database**.
- Involves **multiple joins**:
  - **Payment** → **Rental** (rental_id) → **Inventory** (inventory_id) → **Film** (film_id) → **Film_category** (film_id) → **Category** (category_id).
- **Filtering**: `WHERE` clause for specific categories, `GROUP BY` category, and `ORDER BY` total spend.
- Frequent execution on large databases can incur high **computational costs**.

### Query Optimization Strategies

#### Query Caching

- **Purpose**: Avoid recomputing frequent queries by storing results for instant retrieval.
- **Benefits**: Reduces database load, lowers costs (especially in **cloud databases**), and improves user experience.

#### Common Table Expressions (CTEs)

- **Readability**: Simplify complex queries using temporary result sets (e.g., `selected_film` and `film_actors_id` CTEs for _Rocky War_ actors).
- **Advantages**: Reduce errors, ease debugging, and enhance collaboration compared to nested subqueries.

### Database Maintenance Techniques

#### Vacuuming

- **Purpose**: Remove **dead records** (obsolete data from updates/deletes) to reclaim disk space and optimize performance.

##### Issues Caused by Dead Records

- **Table bloat**: Excess disk space usage.
- Slower queries due to scanning outdated data.
- Inaccurate **query execution plans** and inefficient indexes.

- **Critical for**: Transactional databases like **PostgreSQL** and **MySQL** with frequent write operations.
- **Impact**: Improves query speed, index efficiency, and resource utilization.

### Key Takeaways

- **CTEs** and **query caching** enhance performance and maintainability for complex queries.
- **Vacuuming** is essential for managing database health in transactional systems.

# Streaming Queries

## Queries Streaming Data

### Streaming Data Challenges

- Requires **real-time query patterns** distinct from batch processing.
- Key tools: **Apache Flink**, **Spark Streaming**, and **Kafka** for applying SQL queries to continuous data streams.

### Stream Processing Systems

- Enable **windowed queries** to bound and analyze data in real time:
  - **Aggregation**: Operations like sum, average, or count over defined time windows.
  - **Windowing types**: Session, fixed-time (tumbling), and sliding windows.

#### Windowed Query Types

##### Session Window

- **Use case**: Irregular event streams (e.g., user website clicks).
- **Defining feature**: Groups events with gaps smaller than a specified duration (e.g., 5 minutes of inactivity).
- **Session uniqueness**: Per-key sessions (e.g., each user has independent session windows).
- **Applications**: Trigger follow-up actions (e.g., emails after user inactivity).

##### Fixed-Time (Tumbling) Window

- **Use case**: Periodic aggregations (e.g., clicks every 20 seconds).
- **Defining feature**: Non-overlapping windows of fixed duration.
- **Advantage**: Lower latency than traditional batch ETL (hourly/daily jobs).

##### Sliding Window

- **Use case**: Overlapping intervals (e.g., 60-second windows every 30 seconds).
- **Defining feature**: Allows calculations like **moving averages**.
- **Benefit**: Captures trends across overlapping time ranges.

### Stream Joining and Enrichment

- **Stream-to-stream joins**: Combines real-time data streams (e.g., web browsing + ad platform data).
  - **Challenge**: Different event rates/latencies require **streaming buffers** for temporary event retention.
- **Stream-to-batch joins**: Enrich streaming data with historical data (e.g., e-commerce clicks + product/user details).
  - **Tools**: Serverless functions or in-memory databases (e.g., DynamoDB) for lookups.

## Deploying an Application with Amazon Managed Service for Apache Flink

### Deployment Options on AWS

- **Apache Flink** can be deployed on AWS through:
  - **Amazon EMR**: Run as a YARN application.
  - **Self-hosted containerized environments**: Using **Amazon EKS** or **Amazon ECS** (DIY approach).
  - **Managed service**: **Amazon Managed Service for Apache Flink** for serverless, fully managed infrastructure.

#### Managed Service Advantages

- **Handles infrastructure management**: Includes compute provisioning, AZ failover resiliency, auto-scaling, backups, and cluster maintenance.
- **Focus on application logic**: Users write Flink code locally while AWS manages operational overhead.

### Core Concepts & Demo Workflow

#### Creating a Flink Application

- **Blueprint-based setup**: Use CloudFormation to deploy resources (e.g., **Amazon Kinesis Data Stream** for real-time data, **Amazon S3** for storage).
- **Demo application overview**:
  - Simulates stock ticker data streaming into Kinesis.
  - Flink job filters stocks priced ≥$1 and writes results to S3.
- **Code structure**:
  - Boilerplate setup for Kinesis source/S3 sink.
    **Transformation logic**: Filtering stock data via `flatMap` operator in Java.

#### Studio Notebooks

- **Purpose**: Interactive development/exploration using **Apache Zeppelin** notebooks.
- **Features**:
  - Browser-based interface with support for SQL, Python, and Scala.
  - Ideal for prototyping, testing scenarios, or ad hoc analysis (e.g., visualizing stream processing results).

### Monitoring & Visualization

- **Apache Flink Dashboard**:
  - Displays job graphs (e.g., data flow from Kinesis → filter → S3).
  - **Metrics**: Track records processed, operator status, and data flow visualization.

## Deploying a Studio Notebook with Amazon Managed Service for Apache Flink

### Flink Connectors Overview

- **Connectors**: Enable integration with external systems like **Amazon Kinesis**, **S3**, **DynamoDB**, **MSK**, and **relational databases** via JDBC.
- **Supported services**:
  - **Message queues**: MSK (Managed Streaming for Apache Kafka).
  - **NoSQL databases**: Amazon DynamoDB.
  - **Cloud storage**: Amazon S3.
  - **Traditional databases**: MySQL/PostgreSQL via Amazon RDS.
- **Function**: Facilitate seamless data ingestion, transformation, and output in real-time pipelines.

### Creating a Studio Notebook with Amazon Managed Service for Flink

- **Blueprint use**: Preconfigured template sets up demo data flow between **MSK topics** and **Apache Flink**.
- **Workflow**:
  - Deploy blueprint to auto-generate resources (MSK topic, Flink studio notebook).
  - Demo data simulates **real-time stock ticker prices** for experimentation.
- **Notebook interface**: Launched via **Apache Zeppelin**, supporting SQL, Python, Scala, and real-time visualizations.

#### Studio Notebook Components

- **Paragraphs**: Modular units for code execution (e.g., SQL queries, UDF definitions).
  - Independent execution for iterative development/testing.
- **Flink SQL integration**:
  - **%flink interpreter** runs Flink-specific code.
  - **Example**: `RandomTickerUDF` generates mock stock data for testing.
- **Visualizations**: Supports tables, line/bar charts for real-time insights.
- **Time-window queries**:
  - Reset data via paragraph triggers to test window-based aggregations.
  - Ensures continuous data flow for streaming analytics.

### Integration with AWS Glue

- **Metadata management**: AWS Glue stores schema and connection details for data sources/destinations.
- **Blueprint automation**:
  - Auto-creates Glue databases/tables during deployment.
  - Links to MSK topics for schema discovery.
- **Real-world setup**: Manual specification of Glue databases/tables required for custom pipelines.

# Week 3 Quiz

## Questions

1. Which of the following tasks are performed by the query parser? Select all that apply.
   1. Ensuring that all table and attribute names references in the query exist in the database and that the user has appropriate access to these attributes.
   2. Carrying out the sequence of operations in the execution plan and producing the query results.
   3. Devising an execution plan to retrieve the results from the storage layer.
   4. Breaking down the query into query tokens and validate syntax
2. How does the query optimizer ensure that your queries are performing as well as they can?
   1. The query optimizer validates and corrects the syntax of your query.
   2. The query optimizer shows you different execution plans so you can choose the most appropriate plan to execute your query.
   3. The query optimizer suggests more efficient ways to write your queries.
   4. The query optimizer devises an execution plan that is the least expensive based on I/O cost for transferring data, and computation and memory usage.
3. How does a database index affect query performance?
   1. It improves query performance by rewriting the query in an optimal way.
   2. It organizes data from one or more columns in a well-defined order in a separate data structure, allowing a DBMS to search the ordered index, speeding up the performance of certain queries.
   3. It helps the query optimizer calculate the cost of each execution plan, ensuring that the plan that results in the best query performance is selected.
4. How is an index implemented in a database?
   1. It divides data into doubly-linked blocks stored in a balanced B-tree structure.
   2. It divides data into multiple tables based on some criteria, such as date or location.
   3. It stores data sequentially in a separate table that you can perform binary search on.
5. True or False: In general, querying a B-tree index is more efficient when the index is defined on a column with lots of repeated values.
   1. False
   2. True
6. Which of the following is/are query best practices that were described in this week’s videos?
   1. Specify only the columns you need
   2. Avoid using common table expressions, or CTEs, in complex queries
   3. Use “where” to filter out irrelevant rows
   4. Cache the results of frequently executed complex queries
7. Which of the following best describes the Hash-join method?
   1. You use two different hash functions to map the rows from each table to buckets based on the value of their primary keys. Within each bucket, rows that share matching join attribute are combined.
   2. You use two different hash functions to map the rows from each table to buckets based on the value of the join attribute. Within each bucket, rows that share matching join attributes are combined.
   3. You use a single hash function to map the rows from each table to buckets based on the value of the join attribute. Within each bucket, rows that share matching join attributes are combined.
   4. You use a hash functions to map the rows from the smaller table to buckets based on the value of the join attribute. Within each bucket, a row is picked randomly. Then the algorithm iterates through the rows of the larger table and combines rows that share matching join attributes.
8. Consider the following query:

   ```
   Select Max(Price)
   From orders
   Group by Country
   ```

   Which of the following storage models is most efficient for executing this aggregate query on large datasets?

   1. Row-oriented storage with full table scans
   2. Row-oriented storage with a B-tree index on the Country column
   3. Columnar storage with a cluster key on the Country column
   4. Columnar Storage with a cluster key on the Order_id column

9. What is the primary purpose of vacuuming in relational databases?
   1. To cache frequently executed query results
   2. To clean cached results
   3. To remove dead records and free up space from database disk storage
   4. To make queries more readable
10. Which of the following statements best describes a tumbling window in the context of aggregating streaming data?
    1. It groups events into windows of fixed time length that can overlap.
    2. It groups events that occur at similar times together, filtering out periods of time when there are no events.
    3. It groups events into windows of fixed time length that do no overlap.
    4. It retains events in a buffer when joining two event streams.

## Answers

1. 1 & 4
   1. The parser breaks down the query into query tokens, checks for proper syntax, and validates the query by ensuring all table and attribute names exist in the database and that the user has appropriate access to these attributes. Then it converts the SQL code into bytecode.
   2. The parser breaks down the query into query tokens, checks for proper syntax, and validates the query by ensuring all table and attribute names exist in the database and that the user has appropriate access to these attributes. Then it converts the SQL code into bytecode.
2. 4
   1. The query optimizer generates various execution plans based on various factors. Then it calculates the cost for each plan before picking the least expensive plan.
3. 2
   1. An index can help optimize queries containing clauses such as where, join, and group by.
4. 1
5. 1
6. 1, 3 & 4
   1. This is described as column-based pruning.
   2. This is described as row-based pruning.
   3. By caching the results of complex queries, you avoid repeating the execution of expensive complex queries.
7. 3
8. 3
   1. Columnar storage is more efficient for aggregate queries on large datasets since it transfers only the relevant column from disk, which is price in this case. Moreover since the cluster key is specified on the Country column, you can efficiently group the rows by Country.
9. 3
10. 3
