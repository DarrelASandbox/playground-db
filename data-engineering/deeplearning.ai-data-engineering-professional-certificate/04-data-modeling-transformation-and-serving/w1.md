- [Introduction to Data Modeling for Analytics](#introduction-to-data-modeling-for-analytics)
  - [Conceptual, Logical and Physical Data Modeling](#conceptual-logical-and-physical-data-modeling)
    - [Conceptual Data Model](#conceptual-data-model)
      - [Relationships in ER Diagrams](#relationships-in-er-diagrams)
    - [Logical Data Model](#logical-data-model)
    - [Physical Data Model](#physical-data-model)
    - [Data Model Progression](#data-model-progression)
  - [Data Normalization in Relational Databases](#data-normalization-in-relational-databases)
    - [Objectives of Normalization](#objectives-of-normalization)
    - [Normal Forms (1NF, 2NF, 3NF)](#normal-forms-1nf-2nf-3nf)
      - [Denormalized Form](#denormalized-form)
      - [First Normal Form (1NF)](#first-normal-form-1nf)
      - [Second Normal Form (2NF)](#second-normal-form-2nf)
      - [Third Normal Form (3NF)](#third-normal-form-3nf)
    - [Normalization vs. Denormalization](#normalization-vs-denormalization)
    - [Practical Application](#practical-application)
  - [Dimensional Modeling - Star Schema](#dimensional-modeling---star-schema)
    - [Star Schema Overview](#star-schema-overview)
      - [Fact Tables](#fact-tables)
      - [Dimension Tables](#dimension-tables)
    - [Key Concepts in Star Schema](#key-concepts-in-star-schema)
    - [Query Efficiency with Star Schema](#query-efficiency-with-star-schema)
    - [Star Schema vs. Normalized Models](#star-schema-vs-normalized-models)
- [Data Modeling Techniques](#data-modeling-techniques)
  - [Inmon VS Kimball Data Modeling Approaches for Data Warehouses](#inmon-vs-kimball-data-modeling-approaches-for-data-warehouses)
    - [Inmon Approach](#inmon-approach)
      - [Inmon Example](#inmon-example)
    - [Kimball Approach](#kimball-approach)
      - [Kimball Example](#kimball-example)
    - [Comparison \& Use Cases](#comparison--use-cases)
  - [Exercise: From Normalized Model to Star Schema](#exercise-from-normalized-model-to-star-schema)
    - [Star Schema Design Process](#star-schema-design-process)
    - [Dimension Tables](#dimension-tables-1)
      - [Stores Dimension](#stores-dimension)
      - [Items Dimension](#items-dimension)
      - [Date Dimension](#date-dimension)
    - [Fact Table Creation](#fact-table-creation)
    - [Relationships in Star Schema](#relationships-in-star-schema)
    - [Tool Comparison: DBT vs. AWS Glue](#tool-comparison-dbt-vs-aws-glue)
    - [Key Considerations](#key-considerations)
  - [Another Modeling Example](#another-modeling-example)
  - [Conversation About DBT with Drew Banin](#conversation-about-dbt-with-drew-banin)
    - [Introduction to dbt Labs and dbt](#introduction-to-dbt-labs-and-dbt)
      - [What is dbt?](#what-is-dbt)
    - [Evolution of Data Workflows](#evolution-of-data-workflows)
      - [Pre-dbt Challenges](#pre-dbt-challenges)
      - [Post-dbt Paradigm](#post-dbt-paradigm)
    - [Best Practices and Recommendations](#best-practices-and-recommendations)
      - [Building Models in dbt](#building-models-in-dbt)
      - [Software Engineering Principles](#software-engineering-principles)
    - [Community and Resources](#community-and-resources)
  - [Data Vault](#data-vault)
    - [Core Principles](#core-principles)
    - [Data Vault Architecture Layers](#data-vault-architecture-layers)
      - [Staging Layer](#staging-layer)
      - [Enterprise Data Warehouse (EDW) Layer](#enterprise-data-warehouse-edw-layer)
      - [Information Delivery Layer](#information-delivery-layer)
    - [Enterprise Data Warehouse Components](#enterprise-data-warehouse-components)
      - [Hubs](#hubs)
      - [Links](#links)
      - [Satellites](#satellites)
    - [E-Commerce Model Example](#e-commerce-model-example)
      - [Hubs](#hubs-1)
      - [Links](#links-1)
      - [Satellites](#satellites-1)
    - [Flexibility and Traceability](#flexibility-and-traceability)
    - [Comparison and Evolution](#comparison-and-evolution)
  - [One Big Table (OBT)](#one-big-table-obt)
    - [Introduction](#introduction)
    - [Characteristics of One Big Table](#characteristics-of-one-big-table)
    - [Advantages of OBT](#advantages-of-obt)
    - [Columnar Databases and Sparsity](#columnar-databases-and-sparsity)
    - [Disadvantages of OBT](#disadvantages-of-obt)
    - [When to Use OBT](#when-to-use-obt)
    - [Conclusion](#conclusion)
  - [Demo: Transforming Data with dbt](#demo-transforming-data-with-dbt)
    - [Overview](#overview)
    - [Setting Up dbt](#setting-up-dbt)
    - [dbt Project Setup](#dbt-project-setup)
    - [Configuring the Project](#configuring-the-project)
    - [Creating SQL Queries and YAML Files](#creating-sql-queries-and-yaml-files)
    - [Running and Validating dbt Models](#running-and-validating-dbt-models)
    - [Additional dbt Features](#additional-dbt-features)
  - [Pre-Lab Exercise: Transforming a Normalized Model into a Star Schema](#pre-lab-exercise-transforming-a-normalized-model-into-a-star-schema)
    - [Lab preparation: Modeling Exercise](#lab-preparation-modeling-exercise)
      - [Normalized Model](#normalized-model)
      - [Designing the star schema model](#designing-the-star-schema-model)
    - [ERD diagram of the star schema model](#erd-diagram-of-the-star-schema-model)
      - [SQL queries](#sql-queries)
    - [Final Remarks](#final-remarks)
- [Week 1 Quiz](#week-1-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Introduction to Data Modeling for Analytics

## Conceptual, Logical and Physical Data Modeling

### Conceptual Data Model

- **Conceptual data model**: High-level description of business entities, relationships, attributes, and business rules.
- Uses **entity-relationship (ER) diagrams** to visualize connections (e.g., customers, products, orders).
- Focuses on business logic, not technical implementation (e.g., tables, column names in tabular data).

#### Relationships in ER Diagrams

- **One-to-one relationship**: Represented by "one and only one" symbol (e.g., order detail associated with one product).
- **One-to-many relationship**: Represented by "zero or many" symbol (e.g., product linked to multiple order details).
- Relationship direction matters: Orders to order details are **one-to-many**, while order details to orders are **one-to-one**.

### Logical Data Model

- Adds implementation details to the conceptual model:
  - Specifies **column data types** (e.g., integers, strings).
  - Defines **primary keys** (unique identifiers) and **foreign keys** (relationship enforcers).

### Physical Data Model

- **Implementation-focused**: Chooses specific **DBMS** (e.g., PostgreSQL, MySQL) and storage systems.
- Defines technical details:
  - Storage methods (**disk**, **RAM**, or hybrid).
  - Processes like **partitioning** and **replication**.
  - Configuration for scalability and performance.

### Data Model Progression

- **Three stages**:
  1. **Conceptual**: Abstract business-focused design.
  2. **Logical**: Adds structural details.
  3. **Physical**: Specifies tools and technical implementation.
- Ensures alignment between business needs and technical execution.

## Data Normalization in Relational Databases

### Objectives of Normalization

- **Primary goal**: Reduce data redundancy and ensure **referential integrity**.
- **Key objectives** (Edgar Codd, 1970):
  - Eliminate insertion, update, and deletion dependencies.
  - Minimize restructuring needs when adding new data types.
  - Extend application lifecycle by maintaining stable schemas.

### Normal Forms (1NF, 2NF, 3NF)

#### Denormalized Form

- Contains nested data and redundant entries (e.g., order items as nested objects).
- No unique primary key constraints beyond basic identifiers.

#### First Normal Form (1NF)

- **Requirements**:
  - Unique columns with single values (no nested data).
  - Composite primary key for uniqueness (e.g., **orderId + itemNumber**).
- Example: Flatten nested order items into separate columns.

#### Second Normal Form (2NF)

- **Requirements**:
  - Satisfy 1NF.
  - Remove **partial dependencies** (non-key columns dependent on part of a composite key).
- Example: Split sales order table into **orders** (orderId, customerId, orderDate) and **order items** (orderId, itemNumber, SKU, price, quantity).

#### Third Normal Form (3NF)

- **Requirements**:
  - Satisfy 2NF.
  - Eliminate **transitive dependencies** (non-key columns dependent on other non-key columns).
- Example: Create separate **items** (SKU, name, price) and **customers** (customerId, name, address) tables.

### Normalization vs. Denormalization

- **Normalization benefits**:
  - Reduces redundancy and improves data integrity.
  - Supports efficient write operations.
- **Denormalization benefits**:
  - Improves read performance by avoiding joins.
  - Suitable for analytical queries or read-heavy systems.
- **Key consideration**: Choose based on use case (transactional vs. analytical workloads).

### Practical Application

- Lab exercise: Convert denormalized data to 3NF by:
  - Splitting tables to resolve dependencies.
  - Defining primary/foreign keys for referential integrity.

## Dimensional Modeling - Star Schema

### Star Schema Overview

- **Dimensional data model** designed for faster analytical queries and business-friendly data representation.
- Centers around a **fact table** containing business measures, surrounded by **dimension tables** providing context (resembling a star structure).

#### Fact Tables

- **Contain quantitative measures**: Metrics like trip duration, price, or sales amounts generated by business events.
- **Grain**: Level of detail per row (e.g., atomic grain = one row per ride in rideshare data).
- **Immutable**: Append-only, unchangeable data (narrow and long structure with many rows).

#### Dimension Tables

- **Provide contextual attributes**: Describe "who," "what," "where," and "when" of events (e.g., customer, driver, location).
- **Wide and short**: Many descriptive columns but fewer rows.
- **Conformed dimensions**: Reusable across multiple star schemas (e.g., shared customer or location data).

### Key Concepts in Star Schema

- **Foreign keys**: Connect fact tables to dimension tables.
- **Surrogate keys**: Replace natural keys to unify data from diverse source systems and decouple from source database changes.
- **Composite keys**: Fact tables may use combined keys (e.g., order number + order line number).

### Query Efficiency with Star Schema

- **Simpler analytical queries**: Fewer joins and intuitive filtering/grouping via dimensions.
- **Example**: Calculating total sales by product line in the USA requires:
  - Joining fact table with product and location dimensions.
  - Filtering for the USA and aggregating sales.
- **Comparison with normalized models**: Complex queries in normalized forms involve multiple joins (e.g., linking orders, products, customers) vs. streamlined star schema queries.

### Star Schema vs. Normalized Models

- **Normalized models**:
  - Focus on **data integrity** and **reducing redundancy**.
  - Use complex joins to maintain relational consistency.
- **Star schema**:
  - Prioritizes **analytical performance** and **business usability**.
  - Optimized for read-heavy workloads with denormalized structures.
- **Use cases**: Normalized for transactional systems; star schema for data warehouses/analytics.

# Data Modeling Techniques

## Inmon VS Kimball Data Modeling Approaches for Data Warehouses

- [Building the data warehouse](https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445)
- [The data warehouse toolkit](https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802)

### Inmon Approach

- **Bill Inmon**: Known as the father of the data warehouse, introduced the approach in 1989.
- **Core principles**:
  - **Subject-oriented**: Organizes data into business domains (e.g., products, orders, customers).
  - **Integrated, non-volatile, time-variant**: Ensures consistency, historical tracking, and non-modification.
  - **Granular corporate data**: Stores detailed data in a **highly normalized third normal form (3NF)** to minimize redundancy and ensure integrity.
- **Implementation flow**:
  1. Consolidate data from source systems (e.g., orders, inventory).
  2. Normalize and store in a centralized **enterprise data warehouse**.
  3. Serve downstream **department-specific data marts** (e.g., sales, marketing) via star schemas.
- **Advantages**:
  - Single source of truth with strong data consistency.
  - Flexible for future analytical needs, even if undefined.

#### Inmon Example

- **E-commerce scenario**:
  - Sources: Orders, inventory, marketing systems.
  - Data warehouse stores normalized tables (3NF).
  - Data marts are derived for departments (e.g., sales analytics in star schemas).

### Kimball Approach

- **Ralph Kimball**: Focuses on **dimensional modeling** (star schemas) directly in the data warehouse.
- **Core principles**:
  - **Business process-centric**: Models data for specific analytical needs (e.g., sales trends).
  - **Star schemas**: Combines **fact tables** (metrics) with **dimension tables** (descriptive attributes).
  - Faster implementation: Prioritizes speed and usability over normalization.
- **Implementation flow**:
  1. Ingest data from source systems.
  2. Model directly into star schemas within the data warehouse.
- **Advantages**:
  - Rapid iteration and quicker insights.
  - Simpler for end-users to query.
- **Trade-offs**:
  - **Data redundancy**: Due to denormalized structures, leading to potential integrity issues.

#### Kimball Example

- E-commerce scenario:
  - Sources: Orders, inventory, marketing systems.
  - Data warehouse houses star schemas (e.g., sales fact table linked to product/customer dimensions).
  - No intermediate normalization layer.

### Comparison & Use Cases

- **Choose Inmon when**:
  - **Data quality/integrity** is critical.
  - Analytical requirements are undefined or evolving.
  - Centralized governance is a priority.
- **Choose Kimball when**:
  - **Rapid implementation** and business process-specific analytics are needed.
  - Trade-offs in data redundancy are acceptable for faster insights.
- **Hybrid adoption**: Organizations may use both approaches for different use cases or warehouses.

## Exercise: From Normalized Model to Star Schema

### Star Schema Design Process

- Follows **Kimball’s methodology** with four key steps:
  - **Understand business needs**: Identify relevant business processes and define the **grain** (atomic detail level) of the fact table.
  - **Declare the grain**: Example includes individual product items in sales transactions for flexibility.
  - **Select facts**: Choose measurable metrics like quantity sold and price.
  - **Identify dimensions**: Contextual attributes like stores, dates, and brands.

### Dimension Tables

#### Stores Dimension

- Created from **store_id**, name, city, and zipcode.
- **Surrogate key**: Generated via **MD5 hash function** on `store_id` (string) to ensure uniqueness.

#### Items Dimension

- Includes **SKU**, name, and brand.
- **Surrogate key**: Derived by hashing `SKU` to create `item_key`.

#### Date Dimension

- Contains date attributes like day, month, quarter, and year.
- Generated using SQL date-series functions (e.g., `GENERATE_SERIES` in PostgreSQL).

### Fact Table Creation

- **Fact_order_items** table aggregates sales transaction data:
  - **Facts**: Quantity sold (`item_quantity`), item price.
  - **Foreign keys**: `store_key`, `item_key`, `date_key` (linked to dimension tables).
  - **Surrogate key**: `fact_order_key` generated by hashing `order_id` and `item_line_number`.
- SQL joins normalize data from **orders**, **items**, and **stores** tables.

### Relationships in Star Schema

- **Fact-to-dimension relationships**:
  - One-to-one from fact table to **date**, **store**, and **item** dimension tables.
  - One-to-many from dimension tables back to fact table (e.g., one date can link to many transactions).

### Tool Comparison: DBT vs. AWS Glue

- **DBT (Data Build Tool)**:
  - Transforms data **within a single data warehouse** using SQL.
  - Abstracts SQL complexity but cannot join data across sources.
- **AWS Glue**:
  - Moves and transforms data **across multiple sources/targets**.
  - Ideal for cross-source ETL (Extract, Transform, Load).

### Key Considerations

- **Atomic grain selection**: Enables flexibility for unforeseen queries.
- **Surrogate keys**: Ensure stability against source system changes.
- **Natural keys**: Include alongside surrogate keys for interpretability.

## Another Modeling Example

Assume you work at a car rental company and you are tasked with developing a star schema model that can help the company gather and analyze information on rental trends and customer preferences. For example, you'd like to determine peak booking times, identify the most popular cars being rented, and adjust car rental rates based on demand.

1. **Identify the business process and the grain**: the business process consists of the car rental transactions. The grain would be an entire rental booking made by a customer for a particular car. So each row in the fact table should correspond to one rental booking and could be identified by the booking id.
2. **Identify the dimension tables**: to provide context for each rental event, you can create the following dimension tables:
   1. dim_customers that contains the customers' details (name, address, phone number, driver's license number);
   2. dim_cars that contains the cars' information (VIN - Vehicle Identification Number, model, brand, make, color, purchase date);
   3. dim_dates that contains the information of a given date (year, month, time, day, quarter, day of the week).
   4. dim_stores that contains the information of the rental store (zip code, state, city, address)
3. **Identify the fact table**: Each row represents one rental booking which is identified by the booking id. It also contains the dates that describe this rental period: rental start date, rental end rental date, and return date. It also contains the foreign keys: customer key, car key and store key. And the business measures are the booking fee, insurance fee, fuel charge, extra rental days, fuel level, and total cost.

## Conversation About DBT with Drew Banin

### Introduction to dbt Labs and dbt

- **dbt Labs**: Co-founded by **Drew Banin**, focused on transforming data workflows.
- **dbt Core Functionality**: Applies **business logic** to data using SQL/Python, performed directly in data warehouses without moving data.
  - **Key features**: Version control for transformation logic, integration with modern data platforms (warehouses, BI tools), and security/governance benefits.

#### What is dbt?

- **Definition**: Framework for codifying business rules (e.g., time zones, currency formats) into reusable code.
- **Analogy**: Similar to **Ruby on Rails**—provides conventions that simplify development while allowing customization.

### Evolution of Data Workflows

#### Pre-dbt Challenges

- **Chaotic workflows**: Ad-hoc SQL scripts stored locally, lack of version control, and no audit trails.
- **Fragmented processes**: Data transformations applied directly in warehouses with minimal governance.

#### Post-dbt Paradigm

- **Analytics Engineers**: Emerged as a hybrid role bridging business and engineering (termed "purple people").
- **Structured workflows**: Version-controlled SQL, modular code, and community-driven practices.

### Best Practices and Recommendations

#### Building Models in dbt

- **Modular architecture**: Split logic into staging, intermediate, and dimensional models.
- **SQL style guides**: Ensure consistency in column naming and code structure across teams.
- **Testing**: Implement unit tests and leverage dbt's built-in testing frameworks.

#### Software Engineering Principles

- **CI/CD**: Automate deployments and code reviews via pull requests.
- **Planning over speed**: Avoid "spaghetti code" by prioritizing thoughtful design upfront.
- **Analogous practices**: Apply version control (Git), testing, and code review workflows.

### Community and Resources

- **dbt Community**: Global meetups, **Slack channels**, and annual **Coalesce conference**.
- **Learning resources**: Official documentation, courses, and community-driven content.

## Data Vault

- [Building a Scalable Data Warehouse with Data Vault 2.0](https://www.oreilly.com/library/view/building-a-scalable/9780128026489/)
- [ Data Vault 2.0 Modeling Basics](https://vertabelo.com/blog/data-vault-series-data-vault-2-0-modeling-basics/)

### Core Principles

- **Data Vault vs Inmon/Kimball**: Focuses on **separating structural aspects** (business entities/relationships) from **descriptive attributes**.
- Designed for **flexible, agile, and scalable data warehouses** that adapt to business changes.
- **Dan Linstedt** introduced the approach in the 1990s.
- **Insert-only ingestion**: Data in the staging layer is not altered or filtered.

### Data Vault Architecture Layers

#### Staging Layer

- Loads raw data from source systems.
- Minimal processing: Ensures data type compliance without enforcing business logic.

#### Enterprise Data Warehouse (EDW) Layer

- **Three core components**: Hubs, links, and satellites.
- **Hubs**: Store unique **business keys** (e.g., customer IDs) with **hash keys**, load dates, and source references.
- **Links**: Model relationships/transactions between hubs (e.g., customer-order links) using hashed keys.
- **Satellites**: Contain descriptive attributes (e.g., customer names) linked to hubs/links via hash keys and load dates.

#### Information Delivery Layer

- Feeds downstream **data marts** (e.g., star schemas) for business use cases.
- Applies transformations like aggregation and grouping to meet user needs.

### Enterprise Data Warehouse Components

#### Hubs

- **Primary purpose**: Represent **core business concepts** (e.g., customers, orders).
- **Key fields**:
  - **Hash key**: Generated from business keys (primary key).
  - **Load date**: Timestamp of first ingestion.
  - **Record source**: Origin system identifier.

#### Links

- Capture **relationships** (e.g., customer placing an order).
- Support **many-to-many relationships** (e.g., multiple customers per order).
- **Flexibility**: New relationships can be added without redesigning existing structures.

#### Satellites

- Provide **descriptive context** (e.g., customer demographics).
- **Primary key**: Combines parent hub/link hash key and load date.

### E-Commerce Model Example

#### Hubs

- **Customer**: Business key = `customer_id`.
- **Order**: Business key = `order_id`.
- **Item**: Business key = `SKU`.

#### Links

- **Customer-Order Link**: Connects customers to orders.
- **Item-Order Link**: Tracks items in orders (with satellite for quantity).

#### Satellites

- **Customer Satellite**: Includes name, zip code.
- **Order-Item Link Satellite**: Stores item quantities per order.

### Flexibility and Traceability

- **No data quality labels**: Data Vault stores all data without categorizing it as "good" or "bad".
- **Adaptability**: Structural changes (e.g., adding a `service` hub) require minimal re-engineering.
- **Auditability**: Full traceability to source systems via `record source` and load dates.

### Comparison and Evolution

- **Popular approaches**: Contrasts with **Inmon** (top-down) and **Kimball** (dimensional modeling).
- **Emerging trend**: **One Big Table** model for analytical use cases (simpler schema for query efficiency).
- **Further study**: Recommended books by Data Vault creators for deeper understanding.

## One Big Table (OBT)

### Introduction

- **Traditional data modeling**: Developed for expensive, on-premises data warehouses with limited compute and storage.
- **One Big Table (OBT)**: A more relaxed approach becoming increasingly common.

### Characteristics of One Big Table

- **Definition**: All data is thrown into a single, wide table, typically in a columnar database.
- **Column Count**: Can have thousands of columns.
- **Normalization**: Highly denormalized and flexible.
- **Example**: Similar to a denormalized table, combining various data types, where each row represents a customer order.
- **Relation to Kimball**: Denormalized extension where facts and dimensions are present in the same table.
- **Benefit**: Frees data analysts from complex joins.
- **Performance**: Can run analytical queries faster than on normalized data or star schemas.
- **Impact on Scan Performance**: The wide table contains all necessary data, improving scan performance.

### Advantages of OBT

- **Low Cost**: Enabled by low cost of cloud storage.
- **Flexibility**: Accommodates flexible schemas with nested data.
- **Storage Optimization**: Nested data can be stored without worrying about optimal weight in storage.

### Columnar Databases and Sparsity

- **Wide tables**: Usually sparse, meaning many entries are null values.
- **Row-oriented databases**: Inefficient for storing and reading sparse wide tables due to fixed space allocation and complete row reads.
- **Columnar databases**: Optimize storage and processing by reading only selected columns; reading nulls is essentially free.

### Disadvantages of OBT

- **Loss of Business Logic**: Blending data can obscure business logic.
- **Complex Data Structures**: Requires complex structures, like arrays, for storing nested data.
- **Performance Issues**: Updates and aggregations on nested data may have poor performance.

### When to Use OBT

- **Suitable Use Case**: When dealing with a lot of data needing more flexibility than traditional approaches offer.

### Conclusion

- **No One-Size-Fits-All**: Choose the best approach based on flexibility, data integrity, and ease of use by downstream stakeholders.
- **Next Steps**: Lab exercise to practice modeling normalized data into a star schema and an OBT.
- **Additional Resources**: Demo on dbt and an exercise to practice SQL queries.

## Demo: Transforming Data with dbt

### Overview

- Using **dbt** to model normalized data into a star schema.
- Setting up a local **Postgres database** with five tables: **order items**, **orders**, **customers**, **items**, and **stores**.
- Creating a new schema labeled **star schema**.
- Creating the **fact order items table** and the **dim stores, dim items, and dim date dimension tables**.

### Setting Up dbt

- **dbt Environments**:
  - **dbt Core**: Open source command line tool, connects to databases through adapters.
  - **dbt Cloud**: Hosted environment with a browser-based interface.
- **Installation**:
  - Create a new virtual environment.
  - Activate the virtual environment.
  - Install **dbt core** and the **dbt-postgres adapter**.
- **Adapters**:
  - Used by **dbt core** to connect to the **Postgres database**.
  - Different adapters for different databases/data warehouses; list available in dbt documentation.

### dbt Project Setup

- **dbt init Command**: Creates the dbt project folder.
- **Project Folder Structure**:
  - **models**: Contains SQL files defining tables in the star schema; also contains YAML files for model configurations, testing and documentation.
  - **analyses**: Contains SQL statements that are not considered models.
  - **macros**: Stores reusable SQL code snippets.
  - **seed**: Contains CSV files to load data.
  - **snapshots**: Tracks changes in tables over time.
  - **test**: SQL statements for performing specific tests on data.
- **dbt_project.yml**: Project configuration file specifying name, version, profile, directory configurations, and default model configurations.

### Configuring the Project

- **Profiles**:
  - Profile defined in `profiles.yml` file.
  - Contains details for connecting to the database.
  - Multiple targets can be defined, such as development and production.
  - Specifies database type, credentials (host, username, password, port, database name), number of threads, and default schema.
- **Connection Verification**: Use `dbt debug` command to verify the database connection.
- **Security**: Move `profiles.yml` to the hidden `.dbt` folder in the home directory to secure credentials.

### Creating SQL Queries and YAML Files

- Create SQL files for each dimension and fact table inside the `models` subfolder (each `file_name.sql` will represent a table).
- Create a `schema.yml` file to document information about the models and specify tests:
  - **Models**: Defines the name of the table and the names of the columns.
  - **Description**: Add description about the table and each column using a _description_ key.
  - **Tests**:
    - **unique**: Verifies that the values of store key are unique.
    - **not_null**: Verifies that the values of store key are not null.
    - Other generic tests:
      - **accepted_values**: Checks values against a list.
      - **relationships**: Verifies relationships between columns.

### Running and Validating dbt Models

- **Updating Default Configurations**: Modify `dbt_project.yml` to specify the `star schema models` subdirectory and set the `materialized` key to `table` to ensure tables are created.
- **Running dbt**: Use the `dbt run` command to create the tables. To specify a subdirectory use the `-S` or `--select` options with `dbt run -S star schema models.`
- **Verification**: Check the created tables within the specified schema using a database connection.
- **Testing**: Use `dbt test` to validate the data.

### Additional dbt Features

- Use utility functions for surrogate key generation.
- Use dbt date packages to generate a complete date dimension table.

## Pre-Lab Exercise: Transforming a Normalized Model into a Star Schema

### Lab preparation: Modeling Exercise

In the lab you will use dbt to transform data that's in a normalized model into a star schema model. This ungraded practice exercise is designed to prepare you for the lab by going over the star schema model and the SQL queries required to create the fact and dimension tables. Any answer you enter will result in full marks for that question.

#### Normalized Model

![postgresql-classicmodels](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/postgresql-classicmodels.png)

#### Designing the star schema model

Suppose you want to serve the modeled data to a data analyst who is interested in comparing sales across different locations and assessing employee sales performance by analyzing the customer purchases they facilitated.

Recall that you can follow these 4 steps to design the star schema model:

- Select the Business process.
- Declare the Granularity of your data.
- Identify the Dimensions Tables.
- Identify the Facts.

1. Based on the scenario presented, what would be an appropriate business process to model to support the data analyst?
   1. Based on the data analyst's interest in comparing sales across locations and employee performance, the most appropriate business process to model is Order Fulfillment and Sales Transaction. This process captures the details of each order, the products sold, the customer involved, the employee responsible for the sale, and the location where the sale originated.
2. Assuming that you are modeling the sales transaction business process, what would be an appropriate grain (i.e. level of granularity) to model your data?
   1. Given the business process "Order Fulfillment and Sales Transaction" and the goal of analyzing sales across locations and employee performance, an appropriate grain for the star schema would be "One row per line item on an order."
3. Assuming that you are modeling sales transactions, based on the scenario presented, what would be appropriate dimension tables to model?
   1. Based on the scenario and the provided ER diagram, here are the appropriate dimension tables for modeling sales transactions, supporting analysis of sales by location and employee performance:
      1. **Date Dimension**: This is crucial for analyzing sales trends over time. You would likely derive attributes like year, quarter, month, day, day_of_week, etc., from the orderDate field in the Orders table. Example members include 2023, Q1, January, 1, Monday.
      2. **Product Dimension**: Using the Products table, this dimension allows analysis of sales by product category, name, vendor, etc. Key attributes would include productCode, productName, productLine, productScale, productVendor, and productDescription. Example members include S10_1678, Ford Mustang GT, Classic Cars, 1:10, Min Lin Diecast, Detailed diecast model of a Ford Mustang.
      3. **Customer Dimension**: From the Customers table, this dimension enables analysis of sales by customer demographics and purchasing behavior. Relevant attributes are customerNumber, customerName, contactLastName, contactFirstName, city, state, country, and salesRepEmployeeNumber. Example members include 103, Atelier graphique, Carine Schmitt, France, 1337.
      4. **Employee Dimension**: Leveraging the Employees table, this dimension supports evaluation of employee sales performance. Key attributes include employeeNumber, firstName, lastName, officeCode, and jobTitle. Example members include 1002, Murphy, Diane, 1, Sales Rep.
      5. **Office Dimension**: Based on the Offices table, this enables analysis of sales by location/office. Important attributes would include officeCode, city, state, and country. Example members include 1, San Francisco, CA, USA.
4. Assuming that you are modeling sales transactions, based on the scenario, what are appropriate business measures or facts that you should include in the fact table?
   1. Based on the scenario, the business goals, and the provided ER diagram, here are the appropriate business measures (facts) to include in the fact table for a sales transaction fact table, considering the grain is "one row per line item on an order":
      1. **quantityOrdered**: This is the number of units of a product that were ordered in a specific line item. This is directly from the OrderDetails table. This is fundamental for calculating sales volume.
      2. **priceEach**: This represents the price of a single unit of the product at the time of the sale for that specific line item. (From OrderDetails). Multiply this by quantityOrdered to derive revenue for that line item.
      3. **Revenue**: Calculated field. quantityOrdered \* priceEach. This is the financial impact of a line item and is what we want to aggregate across dimensions. This could be a stored column in the fact table or calculated during query time.

### ERD diagram of the star schema model

Here’s the ERD diagram of a potential star schema for modeling the sales transactions. You might have thought of other/additional fields in each table but this is the example that you will implement in the lab.

![erd-diagram-star-schema-model](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/erd-diagram-star-schema-model.png)

#### SQL queries

In the following questions, you will be asked to write the SQL queries for the fact and dimension tables, except for the dim_dates which will you implement in the lab. The questions are just for practice. So make sure to try and write the queries on your own before you submit the exercise and check the solutions.

Notes:

- The normalized model is defined under the “classicmodels” schema.
- The primary key in each table should be generated as a surrogate key. You can use the MD5 function like you saw in the lecture, but in the lab you will practice using the dbt_utils function `generate_surrogate_key` as shown in this example: `{{ dbt_utils.generate_surrogate_key(['customers.customerNumber']) }}`.
- In the lecture, you saw that in each table the natural keys used to compute the surrogate keys were also included in each table. In the lab, you may assume that the mapping between the natural and surrogate keys is stored somewhere else, so you can just include the surrogate keys in each table.

5. Write the SQL queries for `dim_customers`.

![sql-queries-dim-customers](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/sql-queries-dim-customers.png)

```sql
SELECT
    {{ dbt_utils.generate_surrogate_key(['customerNumber']) }} as customer_key,
    customerName as customer_name,
    contactLastName as contact_last_name,
    contactFirstName as contact_first_name,
    phone,
    addressLine1 as address_line_1,
    addressLine2 as address_line_2,
    postalCode as postal_code,
    city,
    state,
    country,
    creditLimit as credit_limit
FROM
    {{ source('classicmodels', 'customers') }}
```

6. Write the SQL queries for `dim_products`. The `product_line_description` in `dim_products` is the description field in the table “productlines”.

![sql-queries-dim-products](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/sql-queries-dim-products.png)

```sql
SELECT
    {{ dbt_utils.generate_surrogate_key(['productCode']) }} as product_key,
    productName as product_name,
    productLine as product_line,
    productScale as product_scale,
    productVendor as product_vendor,
    productDescription as product_description,
    pl.description as product_line_description,
FROM
    {{ source('classicmodels', 'products') }} p
LEFT JOIN
    {{ source('classicmodels', 'productlines') }} pl ON p.productLine = pl.productLine
```

7. Write the SQL queries for `dim_employees`.

![sql-queries-dim-employees](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/sql-queries-dim-employees.png)

```sql
SELECT
    {{ dbt.utils.generate_surrogate_key(['employeeNumber']) }} as employee_key,
    lastName as last_name,
    firstName as first_name,
    jobTitle as job_title
    email
FROM
    {{ source('classicmodels', 'employees') }}
```

8. Write the SQL queries for `dim_offices`.

![sql-queries-dim-offices](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/sql-queries-dim-offices.png)

```sql
SELECT
    {{ dbt.utils.generate_surrogate_key(['offices']) }} as office_key, officeCode as office_code,
    postalCode as postal_code,
    city,
    state,
    country,
    territory
FROM
    {{ source('classicmodels', 'offices') }}
```

9. Write the SQL queries for `fact_orders`.

![sql-queries-fact-orders](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/sql-queries-fact-orders.png)

```sql
SELECT
    {{ dbt_utils.generate_surrogate_key(['o.orderNumber', 'od.productCode']) }} AS fact_order_key,
    {{ dbt_utils.generate_surrogate_key(['c.customerNumber']) }} AS customer_key,
    {{ dbt_utils.generate_surrogate_key(['e.employeeNumber']) }} AS employee_key,
    {{ dbt_utils.generate_surrogate_key(['off.officeCode']) }} AS office_key,
    {{ dbt_utils.generate_surrogate_key(['p.productCode']) }} AS product_key,
    o.orderDate AS order_date,
    o.requiredDate AS order_required_date,
    o.shippedDate AS order_shipped_date,
    od.quantityOrdered AS quantity_ordered,
    od.priceEach AS product_price
FROM
    {{ source('classicmodels', 'orders') }} o
JOIN
    {{ source('classicmodels', 'orderdetails') }} od ON o.orderNumber = od.orderNumber
JOIN
    {{ source('classicmodels', 'customers') }} c ON o.customerNumber = c.customerNumber
JOIN
    {{ source('classicmodels', 'employees') }} e ON c.salesRepEmployeeNumber = e.employeeNumber
JOIN
    {{ source('classicmodels', 'offices') }} off ON e.officeCode = off.officeCode
JOIN
    {{ source('classicmodels', 'products') }} p ON od.productCode = p.productCode
```

### Final Remarks

Make sure to check the solutions to the SQL queries, you will see them again in the lab. But if you'd like, you could also have them prepared before you start the lab.

You will also fill in the schema.yml file for this star schema model. In the lab, we will give you this partially filled schema describing the fact table and the customers table, and you will be asked to add the details for the remaining tables. You may choose to complete it in the lab or, if you'd like, you can fill in the details of the remaining tables before starting the lab to give yourself more time to complete the lab.

At the end of the lab, you will also implement one big table.

# Week 1 Quiz

## Questions

1. According to this week’s videos, what are some attributes of good data models? Select all that apply.\
   1. It should show the flow of data from when it’s generated in source systems to when it’s being served for analytics and machin learning end use cases.
   2. It should create a shared language for business vocabulary among stakeholders.
   3. It should reflect the business goals and logic, while incorporating business rules.
   4. It should represent data in third normal form.
2. From abstract modeling concepts to concrete implementation, which of the following represents the order in which you should model your data?
   1. Logical model, conceptual model, physical model.
   2. Conceptual model, logical model, physical model.
   3. Conceptual model, physical model, logical model.
   4. Physical model, conceptual model, logical model.
3. Assume you work at a car rental agency. You have a table that contains the information of the cars and another table that contains the information of the rental bookings.

   ![car-rental-agency-table](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/car-rental-agency-table.png)

   Each rental booking should be associated with exactly one car. Each car in the car rental agency can be associated with one or more rental booking as long as the rental period does not overlap, and it can be associated with no bookings if it has never been rented yet. Which of the following correctly encodes the relationship between the two tables?

   1. ![car-rental-agency-table-1](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/car-rental-agency-table-1.png)
   2. ![car-rental-agency-table-2](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/car-rental-agency-table-2.png)
   3. ![car-rental-agency-table-3](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/car-rental-agency-table-3.png)

4. Which of the following are attributes of third normal form (3NF)?
   1. The table has a unique primary key.
   2. It can’t contain transitive dependencies.
   3. It can’t contain partial dependencies.
   4. The table can have columns that contain nested data.
5. Here are the first 5 rows of a table from a relational database representing a car rental system. The `rental_id` column is the primary key.

   ![car-rental-system-table](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/car-rental-system-table.png)

   Which of the following statements about dependencies is true?

   1. This table only has a partial dependency.
   2. There are no dependencies in this table.
   3. This table has a partial dependency and a transitive dependency.
   4. This table only has a transitive dependency.

6. Consider the following ER diagram for hotel reservations and billing ([source](https://dba.stackexchange.com/questions/291652/entity-relationship-diagram-for-hotel)):

   ![er-diagram-hotel-reservations-billing.jpg](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/er-diagram-hotel-reservations-billing.jpg)

   You want to create a star schema model to enable the data analyst to analyze the details of an entire stay, such as the stay duration and the detailed charges, segmented by hotel, room, guest information, and arrival date.

   Which of the following should be included as attributes in the fact table of the star schema? Select all that apply.

   1. The various charges, such as the room charge, restaurant changes, late checkout charges, and miscellaneous charges
   2. Summary statistics of all the reservations that took place in each month, including the average, maximum, and minimum number of nights stayed
   3. The room type
   4. The primary key that identifies the customer
   5. The number of nights stayed
   6. The primary key that identifies the stay

7. Which of the following descriptions of dimension tables are true? Select all that apply.
   1. Dimension tables are used to aggregate business measures, such as finding the average or maximum of a particular measure.
   2. Dimension tables are typically narrow and long, with many rows representing each event.
   3. Dimension tables store descriptive information that provides the context for the business measures that result from a business event.
   4. Dimension tables are used to group and filter results of aggregate queries on business measures.
8. What is a key difference between Inmon and Kimball approaches to data modeling?
   1. Kimball’s approach prioritizes data integrity, while Inmon’s approach prioritizes rapid modeling.
   2. Kimball's approach focuses on modeling data using dimensional modeling, while Inmon's approach focuses on modeling data using a highly normalized data model.
   3. Kimball's approach prioritizes data normalization, while Inmon’s approach prioritizes rapid iteration.
9. What is the primary purpose of the staging area in a Data Vault architecture?
   1. To store the unique business keys and their descriptive attributes.
   2. To apply business logic and data transformations.
   3. To load raw data from source systems without altering it.
10. Consider again the following ER diagram for hotel reservation and billing:

    ![er-diagram-hotel-reservations-billing.jpg](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/er-diagram-hotel-reservations-billing.jpg)

    Let’s say you want to convert this model to a data vault model. You decided to start with two hubs: one hub that represents the guests and another hub that represents an entire stay at the hotel, as shown here:

    ![guests-hub-entire-stay-hub.png](/data-engineering/deeplearning.ai-data-engineering-professional-certificate/04-data-modeling-transformation-and-serving/assets/guests-hub-entire-stay-hub.png)

    In addition to the load_date and record_source, what should each hub table contain?

    1. H_guest should contain the business key GuestID plus detailed information about each guest. H_stay should contain the business key BookingID plus detailed information about the stay.
    2. H_guest should contain the business key GuestID as its primary key. H_stay should contain BookingID as its primary key.
    3. H_guest should also contain the GuestID, which is the business key that identifies each guest, and the primary key for the table which is a hash value computed based on the guest ID. H_stay should also contain the BookingID, which is the business key that identifies each stay, and the primary key calculated using the booking ID.

11. In a data vault model, how can you encode the relationship between two hub tables?
    1. You can encode the relationship between the two hubs by incorporating the business key of one hub table inside the other hub table.
    2. You create a satellite table to connect the two tables.
    3. You create a link table linking the business keys of the two tables.
12. What is an advantage of modeling your data as One Big Table (OBT)?
    1. It supports the use of complex data structures which facilitates applying any updates or aggregations to the nested data.
    2. It enhances the ability to maintain business logic.
    3. It reduces the need for complex joins and improves the performance of the analytical queries.

## Answers

1. 2 & 3
   1. Since a data model aims to inform decision-making and facilitate actions, it should serve as a communication tool, creating a shared language among stakeholders.
   2. Since a data model aims to inform decision-making and facilitate actions, it should connect back to the business goals, logic, and rules.
2. 2
3. 2
   1. The symbol on the right means that each car can be associated with zero or more rental bookings. The symbol on the left means that each rental booking can be associated with exactly one car.
4. 1, 2 & 3
   1. In a third normal form, the conditions of the second normal form should be met and transitive dependencies should be removed. One of the conditions of the second normal form is that the conditions of the first normal form are met, which includes that the table should have a unique primary key.
   2. In a third normal form, the conditions of the second normal form should be met and transitive dependencies should be removed.
   3. In a third normal form, the conditions of the second normal form should be met and transitive dependencies should be removed. One of the conditions of the second normal form is that partial dependencies should be removed.
5. 4
   1. A transitive dependency occurs when a subset of non-key columns depends on some non-key columns. Since the `customer_name` and `customer_phone_number` depend on the non-key column `customer_id`, this table does have a transitive dependency.
6. 1, 5 & 6
   1. The fact table should contain the business measures generated by a business event at the lowest possible level of detail. In this case, each row in the fact table should correspond to an entire stay at a hotel and contain the facts generated by this event, such as the charges and number of nights stayed. It should also include a primary key that uniquely identifies each row, meaning each stay, and foreign keys of the dimension tables containing the descriptive details that correspond to the guests, hotels, rooms, and arrival dates.
   2. The fact table should contain the business measures generated by a business event at the lowest possible level of detail. In this case, each row in the fact table should correspond to an entire stay at a hotel and contain the facts generated by this event, such as the charges and number of nights stayed. It should also include a primary key that uniquely identifies each row, meaning each stay, and foreign keys of the dimension tables containing the descriptive details that correspond to the guests, hotels, rooms, and arrival dates.
   3. The fact table should contain the business measures generated by a business event at the lowest possible level of detail. In this case, each row in the fact table should correspond to an entire stay at a hotel and contain the facts generated by this event, such as the charges and number of nights stayed. It should also include a primary key that uniquely identifies each row, meaning each stay, and foreign keys of the dimension tables containing the descriptive details that correspond to the guests, hotels, rooms, and arrival dates.
7. 3 & 4
   1. The dimension tables provide the descriptive context for the facts stored in the fact table.
   2. You can apply aggregate queries to the business measures in a fact table, then use the dimension tables to filter or group the results.
8. 2
   1. Inmon emphasizes a highly normalized data model for ensuring data integrity and consistency, whereas Kimball emphasizes dimensional modeling within the data warehouse itself for quicker access to analytics.
9. 3
   1. The staging area is designed to load raw data directly from source systems in an insert-only manner, without altering the data or enforcing business logic.
10. 3
    1. Great job! The hub should contain the business key, which is used by the business to identify the business concept, and the primary key, which should be calculated by passing the business key through a hash function.
11. 3
    1. The link table in a data vault model is used to capture the relationships between the different business concepts or hubs.
12. 3
    1. By de-normalizing data into a single wide table, OBT eliminates the need for performing complex joins, which can significantly speed up analytics queries.
