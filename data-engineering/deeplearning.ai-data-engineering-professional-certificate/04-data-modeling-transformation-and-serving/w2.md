- [Modeling and Processing Tabular Data for Machine Learning](#modeling-and-processing-tabular-data-for-machine-learning)
  - [Overview](#overview)
    - [Machine Learning Project Lifecycle](#machine-learning-project-lifecycle)
    - [Role of the Data Engineer](#role-of-the-data-engineer)
    - [Data-Centric Approach to Machine Learning](#data-centric-approach-to-machine-learning)
    - [Topics Covered This Week](#topics-covered-this-week)
  - [Machine Learning Overview](#machine-learning-overview)
    - [Introduction](#introduction)
    - [Supervised vs. Unsupervised Learning](#supervised-vs-unsupervised-learning)
      - [Classification vs. Regression](#classification-vs-regression)
    - [Machine Learning Project Lifecycle](#machine-learning-project-lifecycle-1)
      - [Phases](#phases)
    - [Data Engineer's Role](#data-engineers-role)
  - [Modeling Data for Traditional ML Algorithms](#modeling-data-for-traditional-ml-algorithms)
    - [Introduction](#introduction-1)
    - [Pre-processing Steps for Tabular Data](#pre-processing-steps-for-tabular-data)
    - [Common Feature Engineering Operations](#common-feature-engineering-operations)
      - [Handling Missing Values](#handling-missing-values)
      - [Feature Scaling](#feature-scaling)
      - [Converting Categorical Columns to Numerical](#converting-categorical-columns-to-numerical)
    - [Collaboration is Key](#collaboration-is-key)
  - [Conversation with Wes McKinney](#conversation-with-wes-mckinney)
    - [Introduction to Wes McKinney and Pandas](#introduction-to-wes-mckinney-and-pandas)
    - [What is Pandas?](#what-is-pandas)
      - [Pandas tutorials](#pandas-tutorials)
    - [The Inspiration Behind Pandas](#the-inspiration-behind-pandas)
    - [Rise and Success of Pandas](#rise-and-success-of-pandas)
    - [When to Use (or Not Use) Pandas](#when-to-use-or-not-use-pandas)
    - [Advice for Aspiring Data Practitioners](#advice-for-aspiring-data-practitioners)
    - [Future of Data and Data Engineering](#future-of-data-and-data-engineering)
- [Modeling and Processing Unstructured Data for Machine Learning](#modeling-and-processing-unstructured-data-for-machine-learning)
  - [Modeling Image Data for ML Algorithms](#modeling-image-data-for-ml-algorithms)
    - [Image Pre-processing for Machine Learning](#image-pre-processing-for-machine-learning)
      - [Traditional Machine Learning Algorithms (e.g., Regular Neural Networks)](#traditional-machine-learning-algorithms-eg-regular-neural-networks)
      - [Convolutional Neural Networks (CNNs)](#convolutional-neural-networks-cnns)
    - [Image Preparation Techniques](#image-preparation-techniques)
    - [Tools for Image Pre-processing](#tools-for-image-pre-processing)
    - [Pre-processing Other Unstructured Data: PDF Files](#pre-processing-other-unstructured-data-pdf-files)
  - [Code Example: Image Preprocessing Using TensorFlow](#code-example-image-preprocessing-using-tensorflow)
  - [Preprocessing Texts for Analysis and Text Classification](#preprocessing-texts-for-analysis-and-text-classification)
    - [Introduction to Text Pre-processing](#introduction-to-text-pre-processing)
    - [Importance of Text Pre-processing](#importance-of-text-pre-processing)
    - [Text Cleaning](#text-cleaning)
    - [Text Normalization](#text-normalization)
    - [Tokenization](#tokenization)
    - [Stop Word Removal](#stop-word-removal)
    - [Lemmatization](#lemmatization)
    - [Considerations](#considerations)
  - [Coding Example - Text Preprocessing with Python](#coding-example---text-preprocessing-with-python)
  - [Text Vectorization and Embedding](#text-vectorization-and-embedding)
    - [Traditional Vectorization Methods](#traditional-vectorization-methods)
      - [Limitations of Traditional Methods](#limitations-of-traditional-methods)
    - [Word Embeddings](#word-embeddings)
    - [Sentence Embeddings](#sentence-embeddings)
      - [Implementation](#implementation)
      - [Example with Sentence Transformers](#example-with-sentence-transformers)
      - [Similarity Measurement](#similarity-measurement)
      - [Applications of Embeddings](#applications-of-embeddings)
  - [Coding Example - Vectorizing Text with scikit-learn](#coding-example---vectorizing-text-with-scikit-learn)
    - [Sci-kit learn user guide](#sci-kit-learn-user-guide)
    - [Resources for the Demo on Processing Tabular Data with Scikit-Learn](#resources-for-the-demo-on-processing-tabular-data-with-scikit-learn)
- [Optional Machine Learning Courses](#optional-machine-learning-courses)
- [Week 2 Quiz](#week-2-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Modeling and Processing Tabular Data for Machine Learning

- [SeattleDataGuy article - Date Engineering vs Machine Learning pipelines](https://seattledataguy.substack.com/p/data-engineering-vs-machine-learning)

## Overview

- **Focus**: Preparing and modeling data for **machine learning (ML)** use cases.
- **Goal**: Structure data to help data scientists/ML engineers understand data for developing ML systems and discovering patterns.
- **Data engineers** play a crucial role in building and maintaining data pipelines for various ML project stages.

### Machine Learning Project Lifecycle

- **Framework**: Based on the **ML project lifecycle framework**.
- **Phases**:
  - Scoping the ML project.
  - Collecting and preparing the data.
  - Developing the **ML model (algorithm development)**.
  - Deploying the ML system.
- **Data Engineers** role is to build and maintain data pipelines to data one or more stages.

### Role of the Data Engineer

- **Task**: Build and maintain data pipelines. Tasks include:
  - Collecting data from multiple sources.
  - Combining data into suitable formats.
  - Cleaning, converting, and creating features.
  - Storing and sharing data.
- **Responsibilities**:
  - Serving raw data _or_ processing data for ML teams.
  - May also handle featurization, especially in smaller organizations.

### Data-Centric Approach to Machine Learning

- **Emphasis**: Enhancing machine learning systems by collecting high-quality data.
- **Garbage in, garbage out**: The quality of input data determines the output quality.
- Data engineers help extract accurate insights by carefully preparing data.
- Understanding basic ML principles is crucial for providing value to the organization.

### Topics Covered This Week

- Key **ML terminology** and phases of the ML project lifecycle.
- Structuring **tabular data** for classical ML algorithms (with practical exercises in lab 1).
- Modeling and processing **unstructured data**:
  - Preparing **image data** for classical and advanced ML (convolutional neural networks).
  - Working with **text data**: Basic preprocessing techniques and converting text to vectors.
- **Textual data**: Lab 2 provides practical experience.

## Machine Learning Overview

### Introduction

- Overview of essential machine learning concepts for data engineers.
- Covers supervised vs. unsupervised learning, classification vs. regression, and the machine learning project lifecycle.

### Supervised vs. Unsupervised Learning

- **Supervised Learning**: Learning from features and labels in historical data.
- Example 1: Predicting customer churn based on demographics and purchase history.
- Example 2: Predicting product sales using historical product and sales data.
- **Unsupervised Learning**: Discovering patterns in unlabeled data.
- Example: Segmenting customers into groups based on purchasing behaviors without pre-defined labels.

#### Classification vs. Regression

- **Classification**: Supervised learning with categorical labels (e.g., churn or not churn).
- **Regression**: Supervised learning with numerical labels (e.g., sales amount).

### Machine Learning Project Lifecycle

- A framework for understanding how data is used in ML development.

#### Phases

- **Scoping**: Defining the project and business problem.
- **Data**:
  - Determining the necessary features and labels with the ML team.
  - Data engineers primarily involved in defining, establishing baselines, labeling, and organizing data.
- **Algorithm Selection, Training and Error Analysis**:
  - Splitting dataset into **training set** and **test set**.
  - Training with classical algorithms (linear/logistic regression, decision trees, etc.) or complex algorithms (deep neural networks, CNNs, RNNs, LLMs).
  - Cross-validation to select the best model.
  - Evaluating the selected model with the test set and iteratively update features and labels.
- **Deployment**:
  - Final check of system performance and reliability.
  - Data engineers might be responsible for preparing and serving data to the deployed model for retraining and updating.

### Data Engineer's Role

- Focus on setting up data pipelines to support the various phases of the ML project.
- Involved in the data and deployment phases, including serving data for model training, deployment, and maintenance.

## Modeling Data for Traditional ML Algorithms

### Introduction

- Data served for training **classical machine learning algorithms** needs to be in tabular form with numerical values.
- **Feature engineering** involves processing raw columns or creating new features.
- The **machine learning team** decides which features and labels to include.

### Pre-processing Steps for Tabular Data

- **Raw Data Example**: Customer churn dataset with purchases, date, income, time on platform, account type, and churn status.
- **Processed Data**: Separating churn column into labels, handling missing values and duplicates, ensuring numerical values within a similar range, and feature creation.
- **Example**: Created "purchases per minute" feature by dividing "number of items purchased" by "minutes on platform."

### Common Feature Engineering Operations

- Includes handling **missing values**, **feature scaling**, converting **categorical columns** into numerical ones, and creating **new columns**.

#### Handling Missing Values

- Understand **why** values are missing before handling.
- **Deleting Columns/Rows**: Simplest approach, but can lead to unintentionally losing important data. Only delete rows or columns when there's no risk of losing valuable data.
- **Imputing Missing Values**: Replace with mean, median, or similar record value. Introduces potential noise or bias.
- **Example**: Deleted a row with mostly null values and replaced a missing income value with a similar record's value.
- **Best Approach**: Collaborate with the machine learning team.

#### Feature Scaling

- Scale numerical features so values end up within a similar range.
- **Importance**: Machine learning algorithms are optimization algorithms, and vastly different feature values can slow convergence. Can also affect accuracy of distance-based algorithms.
- **Example**: The range of values for the number of items purchased feature will be much smaller than the range for the customer income feature.
- **Standardization**: Subtract column mean and divide by column standard deviation to get a mean of zero and a variance of one.
- **Min-Max Scaling**: Subtract the minimum column value and divide by the difference between the maximum and minimum column values to get values between zero and one.

#### Converting Categorical Columns to Numerical

- Machine learning algorithms expect numerical features.
- **One-Hot Encoding**: Replace a column with multiple columns representing each unique value (e.g., account type: basic, family, platinum).
- **Drawback**: Can significantly increase the number of columns if there are many unique values.
- **Ordinal Encoding**: Assign numerical values based on natural ordering of categories (e.g., account type based on subscription fee: basic=1, family=2, platinum=3).
- **Other Methods**: Hashing, Embeddings.
- **Choice of Method**: Collaborate with the machine learning engineer to choose one appropriate for use case.

### Collaboration is Key

- Work closely with the **machine learning team** to decide on the most appropriate steps and methods for feature engineering for project at hand.

## Conversation with Wes McKinney

### Introduction to Wes McKinney and Pandas

- **Wes McKinney**: Original creator of the **Pandas** project in 2008, open-sourced in 2009.
- Authored "Python for Data Analysis," a staple **Pandas** reference book.
- Worked on **Apache Arrow** and **Ibis**, with entrepreneurial experience with startups like DataPad, Ursa Labs, and Voltron Data.
- Current focus: investing in data companies and facilitating easier data access through open-source data science.

### What is Pandas?

- **Pandas**: A **data management** and **data manipulation** toolkit for loading and manipulating datasets in memory in Python.
- Enables working with data in a spreadsheet/table-like object using a **DataFrame**.
- Provides an API for spreadsheet operations, column operations, and data cleaning.
- **Functionality**: Loading data from various sources, generating plots, and creating formatted tables.
- **Role**: Primarily for **interactive exploratory data analysis**, serving as a precursor to other tools like **scikit-learn**, **TensorFlow**, or **PyTorch**.
- **Focus**: Not intended to be an AI or Machine Learning Toolkit, but rather to facilitate working with data.

#### Pandas tutorials

- [Kaggle Pandas tutorials](https://www.kaggle.com/learn/pandas)
- [W3 School Pandas tutorials](https://www.w3schools.com/python/pandas/default.asp)

### The Inspiration Behind Pandas

- Created during the **2008 financial crisis** while working at a quantitative hedge fund.
- **Need for faster exploratory data analysis and research** prompted the development of Python-based data tools.
- Existing tools (MATLAB, R, Java, SQL) had limitations.
- The toolkit initially served personal needs but expanded to colleagues and, eventually, the open-source community.
- **Name Origin**: Inspired by the term **"panel data"** common among econometricians, along with an attempt to create an acronym evocative of Python data analysis.

### Rise and Success of Pandas

- **Key Factors**:
  - The growing need for **data science skills**.
  - **Open source** nature made it accessible without cost barriers.
  - The confluence of need, comfort with Python, and open-source software among new tech companies.
  - The availability of McKinney's book.
- **Timing**: Aligned with the rise of **"big data"** and the need for data tools in the corporate world.
- **Python's success**: Cemented by its adoption by Google and Facebook (TensorFlow and PyTorch).

### When to Use (or Not Use) Pandas

- **Good Fit**: Tabular/rectangular data or integrating multiple tabular datasets. Data can be easily fit into a spreadsheet or database.
- **Alternatives**:
  - **Large Datasets**: Databases, **Polars** (a Python DataFrame library).
  - **Clean Machine Learning Datasets**: Libraries like **X-ray** built on top of **NumPy**.

### Advice for Aspiring Data Practitioners

- **Master the Basics**: Learn data manipulation, loading, cleaning, integration, and merging.
- **Develop EDA Skills**: Become proficient in plotting, visualization, and using Jupyter Notebook features.
- **Iterative Analysis**: Adopt an interactive, iterative approach focused on data visualization for quicker feedback and mastery.

### Future of Data and Data Engineering

- **Continued evolution of Python data tools and frameworks**.
- **Python will remain a mainstream language for data work**.
- **AI Assistants**: Integration of AI assistants like **Copilot, ChatGPT, and LLMs** in the workflow for problem-solving and code assistance.
- **Increased Productivity**: Automation of repetitive tasks to focus on creative and value-added work (model development, data analysis).

# Modeling and Processing Unstructured Data for Machine Learning

## Modeling Image Data for ML Algorithms

### Image Pre-processing for Machine Learning

- Machine learning algorithms can identify **patterns in images**, enabling applications like **image classification**, **object detection**, and **pixel segmentation**.
- Pre-processing images depends on the algorithm used.

#### Traditional Machine Learning Algorithms (e.g., Regular Neural Networks)

- Expect data in **tabular form**.
- Images must be **flattened into a long vector of pixel values**.
- **Limitation**: Ignores spatial structure, resulting in loss of spatial information and high-dimensional feature vectors.

#### Convolutional Neural Networks (CNNs)

- Work **directly on images** without flattening.
- Consist of layers identifying features in images.
- **First layers**: Learn generic features (lines, edges, textures).
- **Later layers**: Learn complex patterns specific to the task.
- **Pre-trained CNNs**: Often fine-tuned for specific tasks using custom image sets.

### Image Preparation Techniques

- **Resizing**: Adjusting images to the shape expected by the neural network.
- **Scaling**: Normalizing pixel values within a specific range.
- **Data Augmentation**: Creating new images through geometric transformations (flipping, rotating, cropping, brightness adjustments).
  - **Benefits**: Increases training data size and variety, generally improving model performance.

### Tools for Image Pre-processing

- **TensorFlow**: Provides pre-processing functions for resizing, scaling, and augmenting images.

### Pre-processing Other Unstructured Data: PDF Files

- **PDF Files**: May contain scanned documents needing text or table extraction.
- **Additional techniques**: Link provided for course on pre-processing unstructured data for large language model applications.

## Code Example: Image Preprocessing Using TensorFlow

Feel free to check out [this Google Colab notebook](https://colab.research.google.com/drive/1NVBjtIfGuYoSomwBYkPg6hqyhj7HNhLS?usp=sharing#scrollTo=GB_biegiU5xU) that shows you how to perform some of the image preprocessing steps using TensorFlow, including resizing, flipping, rotating, and adjusting the brightness of an image.

## Preprocessing Texts for Analysis and Text Classification

### Introduction to Text Pre-processing

- Organizations collect large amounts of text data for valuable insights.
- **Data engineers** must pre-process text data for **machine learning engineers**.
- **NLP** enables computers to process, understand, and generate text.
- **Large Language Models (LLMs)** have enhanced text processing capabilities.
- **Goal**: Process textual data for training machine learning systems for NLP tasks.

### Importance of Text Pre-processing

- Textual data might contain typos, inconsistencies, and repetitions.
- **Clean and high-quality data** ensures better model performance.
- Textual data needs to be relevant; irrelevant words and characters should be removed.
- **Classical machine learning algorithms** are sometimes preferred over **LLMs** due to computational cost and when simpler solutions are sufficient.
- The machine learning team might combine numerical, categorical, and textual features.

### Text Cleaning

- **Start**: Removing punctuations, extra spaces, or irrelevant characters.
- Cleaning code written in **Python** is provided in the reading item.

### Text Normalization

- **Normalization**: Standardizes text by making it consistent.
- **Practices**: Converting characters to lowercase, expanding contractions, converting numbers or symbols to characters
- Resolves inconsistencies in spelling and use of the same words.
- **Example**: Converting "AMT" and "Amount" to "amount."
- **Example**: Converting "don't" to "do not."
- Converting common units like "Kg" to "kilograms."
- **Python** code for normalization is in the reading item.

### Tokenization

- **Definition**: Splitting text into individual units or tokens.
- **Tokens**: Can be words, subwords, or short sentences.
- **Example**: Splitting each review into a vector of words.
- **Python**: Achieved with `split` string methods.

### Stop Word Removal

- **Definition**: Removing frequently used words ("is," "are," "the," "for") that don't add meaning.
- **Words**: Referred to as stop words.
- **Methods**: Using a predefined list or NLP libraries (**SpaCy**, **NLTK**, **Gensim**, **TextBlob**).

### Lemmatization

- **Definition**: Replacing each word with its base form (lemma).
- **Example**: Replacing "getting" and "got" with "get."
- **Implementing**: Using NLP libraries to obtain the lemma of each word.

### Considerations

- Required steps depend on the needs of the data scientists and machine learning engineers.
  - Some steps might be skipped or others added.
- Focus depends on chosen models and processing preferences.

## Coding Example - Text Preprocessing with Python

This file [`functions_preprocess_texts.py`] contains python functions that you can use to pre-process texts by:

- removing special characters and extra spaces
- expanding contractions
- converting characters to lower case
- removing stop words
- lemmatizing

## Text Vectorization and Embedding

- [Medium article - Comparing between the embedding models of openAI, cohere, google and others](https://medium.com/@lars.chr.wiik/best-embedding-model-openai-cohere-google-e5-bge-931bfa1962dc)
- [Embedding model: all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- [SentenceTransformers documentation](https://www.sbert.net/index.html)
- [Cohere embeddings](https://docs.cohere.com/docs/embeddings)
  - [Convert text into vectors](https://docs.cohere.com/docs/how-to-convert-text-into-vectors)
  - [Word and sentence embeddings](https://cohere.com/blog/sentence-word-embeddings?_gl=1*r938ho*_ga*MTgxNTg3NDExMS4xNzE3ODk3NjU3*_ga_CRGS116RZS*MTcxODI0MTg3Mi44LjEuMTcxODI0NTI5OS4xNS4wLjA)
- [Word2vec tutorial](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) - article on [Word2vec properties](https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why)
- [Medium article - Word embedding](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)
- [Medium article - From word to sentence embedding](https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917)

### Traditional Vectorization Methods

- Convert preprocessed text data into vectors.
- **Bag of Words**: Assigns a number to each word based on its occurrence frequency.
- Each document is converted into a vector with the same length as the vocabulary.
- Each entry in the vector reflects the number of times the word appears in the document.
- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Accounts for the weight and rarity of each word.
- **Term Frequency**: Number of times a term occurred in a document divided by the total number of words in that document.
- **Inverse Document Frequency**: Indicates how common or rare a word is in the corpus (closer to zero = more common, closer to one = more rare).

#### Limitations of Traditional Methods

- Do not consider the meaning of words or sentences.
- Can result in high-dimensional vectors with sparse values.

### Word Embeddings

- Represent words as vectors to capture their semantic meaning.
- Words with similar meanings are mapped to geometrically close vectors.
- Algorithms: **word2vec** and **GloVe** (trained on large collections of text based on co-occurrences).

### Sentence Embeddings

- Represent entire sentences as vectors, capturing semantic meaning.
- Take into account the position of words in a sentence.
- Lower dimensionality compared to TF-IDF vectors.
- Achieved using pre-trained NLP models, specifically large language models (LLMs).

#### Implementation

- Use frameworks like **sentence-transformers** (Python).
- Leverage APIs from LLM research companies like **OpenAI**, **Anthropic**, and **Google**.

#### Example with Sentence Transformers

- Instantiate a sentence transformer with an open-source LLM model (e.g., `all-MiniLM-L6-v2`).
- Pass cleaned and normalized text reviews to the embedding model.
- Model returns an embedding vector for each review (e.g., 384 entries).

#### Similarity Measurement

- **Cosine similarity**: Measures the similarity between embedding vectors.

#### Applications of Embeddings

- Features for training machine learning algorithms (e.g., product recommendation).
- Clustering.
- Similarity search.

## Coding Example - Vectorizing Text with scikit-learn

This notebook file [`vectorizing_text_sklearn.ipynb`] shows how you use scikit learn to vectorize a list of texts (corpus) using the bag of words and TF-IDF models.

### Sci-kit learn user guide

- [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html)
- [Text extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)

### Resources for the Demo on Processing Tabular Data with Scikit-Learn

- [Customer churn dataset from Kaggle](https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset)

# Optional Machine Learning Courses

1. [Machine Learning Specialization](https://www.deeplearning.ai/courses/machine-learning-specialization/)
2. [Deep Learning Specialization](https://www.deeplearning.ai/courses/deep-learning-specialization/)
   1. In [this video](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs), Andrew explains the best practice of applying the same preprocessing steps and computed statistics from the training set to the testing set
   2. [Course 4](https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning) is all about Convolutional Neural Networks
3. [Natural Language Processing Specialization](https://www.deeplearning.ai/courses/natural-language-processing-specialization/)
4. [Machine Learning in Production Specialization](https://www.deeplearning.ai/courses/machine-learning-in-production/)
5. [Generative AI with LLMs Course](https://www.deeplearning.ai/courses/generative-ai-with-llms/)
6. [Preprocessing Unstructured Data for LLM Applications](https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/)
7. [Understanding and applying text Embeddings](https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/)
8. [Large Language Models with Semantic Search](https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/)
9. [Serverless LLM apps with Amazon Bedrock](https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/)
10. [Pretraining LLMs](https://www.deeplearning.ai/short-courses/pretraining-llms/)
    1. Includes additional preprocessing steps to prepare your textual data for training an LLM

# Week 2 Quiz

## Questions

1. Which type of machine learning task involves using categorical labels to organize data into distinct groups?
   1. Unsupervised Learning
   2. Classification
   3. Deep neural network
   4. Regression
2. At a company with a mature machine learning team, which phase of the machine learning project lifecycle will a data engineer likely be most involved in?
   1. Scoping phase
   2. Deployment phase
   3. Data phase
   4. Algorithm development phase
3. Which of the following is a characteristic of datasets that most classical machine learning algorithms expect to receive as training data?
   1. All numerical values must fall within the range of 0 to 1.
   2. Each column must consist of numerical values that are within a similar range.
   3. Less than half of the data can contain missing values.
   4. Categorical columns may be non-numerical.
4. When preparing training data for a classical machine learning algorithm, what are some methods for handling missing values? Select all that apply.
   1. One-hot encoding
   2. Row and column deletion
   3. Imputation
   4. Standardization
5. What method should you use to scale a column of numerical data so that the resulting column contains values that are between 0 and 1?
   1. Imputation
   2. Feature engineering
   3. Min-max scaling
   4. Standardization
6. Which type of neural network is specifically designed to work directly on images without first flattening them?
   1. Large Language Models
   2. Convolutional Neural Network (CNN)
   3. Recurrent Neural Network (RNN)
   4. Supervised Learning
7. When training a classical machine learning algorithm with textual data, you need to first preprocess and vectorize the text into numerical form. According to this week's videos, which of the following is NOT a common text preprocessing technique?
   1. Normalization
   2. Tokenization
   3. Data augmentation
   4. Removal of stop words
8. What is lemmatization in the context of text preprocessing?
   1. Replacing each word with its base form
   2. Splitting text into individual units
   3. Removing frequently used words
   4. Converting text to lower-case
9. What is the main advantage of using TF-IDF (Term-Frequency Inverse-Document-Frequency) over the Bag of Words method for text vectorization?
   1. It captures the semantic meaning of words
   2. It considers the position of words in a sentence
   3. It reduces the dimensionality of the vector
   4. It accounts for the weight and rarity of each word in the corpus
10. What are the main advantages of using word or sentence embeddings over traditional vectorization techniques like Bag of Words and TF-IDF? Select all that apply.
    1. Text embeddings reduce the dimensionality of the vector.
    2. Text embeddings capture the semantic meaning of words or sentences.
    3. Text embeddings are easier to understand than the results from applying the TF-IDF and Bag of Words techniques to vectorize text.
    4. You need a smaller dataset to train models that generate text embeddings than those used for traditional vectorization techniques.

## Answers

1. 2
   1. Classification is used to organize data into distinct groups using categorical labels, such as predicting whether a customer will churn or not.
2. 3
   1. The data phase is where the machine learning engineer works with the data engineer to determine what features and labels they need to collect for training the machine learning algorithm. Then the data engineer is responsible for collecting and organizing the necessary features and labels. It is a phase where the data engineer will be most involved with.
3. 2
   1. Most classical machine learning algorithms expect training data that is in numerical tabular form with no missing values, where each column contains values within a similar range.
   2. **Option 4**: Most classical machine learning algorithms expect training data to be in numerical tabular form. When preparing the training data, you should convert categorical columns to numerical ones using some form of encoding.
4. 2 & 3
   1. One way to handle missing values is to delete the row or column that contains the missing values, but you should only choose this method if there’s no risk of losing valuable data. You should always understand why the values are missing to help determine the most appropriate way to handle this issue.
   2. One way to handle missing values is to impute, meaning replace, the missing value with some summary statistics like the column mean or median, or with the value from a similar record. You should always understand why the values are missing to help determine the most appropriate way to handle this issue.
5. 3
   1. With min-max scaling, you take each value in the column and subtract the minimum column value. Then you divide by the difference between the maximum and minimum column values. This guarantees that the resulting column values are between 0 and 1.
6. 2
   1. CNNs are designed to work directly on images without the need to flatten them. These types of networks consist of several layers used to identify features in images that can help with the machine learning task.
7. 3
   1. Data augmentation is a technique used to create new data from existing data, and was discussed in the context of preprocessing image data. According to this week's videos, it is not a common text preprocessing technique.
8. 1
   1. Lemmatization involves replacing words with their base form, such as replacing "getting" and "got" with "get".
9. 4
   1. TF-IDF considers both the term frequency and the inverse document frequency, accounting for the weight and rarity of each word.
10. 1 & 2
    1. Traditional vectorization techniques typically result in high-dimensional vectors with very sparse values compared to word and sentence embeddings.
    2. Word and sentence embeddings capture the semantic meaning of words and sentences by mapping similar words and sentences to geometrically closer vectors. Whereas traditional vectorization techniques don’t account for the semantic meaning of the words or sentences.
