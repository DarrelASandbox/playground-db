- [Batch Transformations](#batch-transformations)
  - [Overview - Data Transformation at Scale](#overview---data-transformation-at-scale)
    - [Importance of Data Transformation](#importance-of-data-transformation)
    - [Use Cases](#use-cases)
    - [Considerations for Transformation](#considerations-for-transformation)
    - [Distributed Processing Frameworks](#distributed-processing-frameworks)
    - [Week Overview](#week-overview)
  - [Batch Transformation Patterns and Use Cases](#batch-transformation-patterns-and-use-cases)
    - [Batch Transformation Overview](#batch-transformation-overview)
    - [Transformation Approaches](#transformation-approaches)
      - [Examples of ETL and ELT](#examples-of-etl-and-elt)
    - [Data Wrangling (Cleaning and Normalization)](#data-wrangling-cleaning-and-normalization)
    - [Data Updates](#data-updates)
    - [CDC Patterns](#cdc-patterns)
    - [Handling Deleted Rows](#handling-deleted-rows)
    - [Database Considerations](#database-considerations)
    - [Scalability and Performance](#scalability-and-performance)
  - [Distributed Processing Framework - Hadoop](#distributed-processing-framework---hadoop)
    - [Introduction](#introduction)
    - [The Need for New Tools](#the-need-for-new-tools)
    - [Google's Influence](#googles-influence)
    - [Apache Hadoop](#apache-hadoop)
    - [Relevance of Hadoop](#relevance-of-hadoop)
    - [Hadoop Distributed File System (HDFS)](#hadoop-distributed-file-system-hdfs)
    - [MapReduce Programming Model](#mapreduce-programming-model)
    - [Shortcomings of MapReduce](#shortcomings-of-mapreduce)
    - [Evolution Beyond MapReduce](#evolution-beyond-mapreduce)
    - [Modern Context](#modern-context)
  - [Distributed Processing Framework - Spark](#distributed-processing-framework---spark)
    - [Spark's Origins and Evolution](#sparks-origins-and-evolution)
    - [Spark's Architecture and Capabilities](#sparks-architecture-and-capabilities)
    - [Programming with Spark](#programming-with-spark)
    - [Spark Application Components](#spark-application-components)
    - [Spark Job Execution](#spark-job-execution)
    - [PySpark](#pyspark)
  - [Spark DataFrames for Large-Scale Data](#spark-dataframes-for-large-scale-data)
    - [Introduction](#introduction-1)
    - [Resilient Distributed Datasets (RDDs)](#resilient-distributed-datasets-rdds)
    - [Immutability and Operations](#immutability-and-operations)
      - [Transformations](#transformations)
      - [Actions](#actions)
    - [Lazy Evaluation and Fault Tolerance](#lazy-evaluation-and-fault-tolerance)
  - [Demo: Working with Spark DataFrames Using Python](#demo-working-with-spark-dataframes-using-python)
    - [Overview](#overview)
    - [Setup](#setup)
    - [Creating SparkSession](#creating-sparksession)
    - [Creating DataFrames](#creating-dataframes)
    - [Exploring DataFrames](#exploring-dataframes)
    - [Manipulating DataFrames](#manipulating-dataframes)
    - [Cleaning DataFrames](#cleaning-dataframes)
    - [Aggregating DataFrames](#aggregating-dataframes)
    - [User-Defined Functions (UDFs)](#user-defined-functions-udfs)
      - [Performance Considerations](#performance-considerations)
    - [Interacting with Data Using SQL in PySpark](#interacting-with-data-using-sql-in-pyspark)
    - [Temporary Views](#temporary-views)
    - [User-Defined Functions (UDFs) in SQL Queries](#user-defined-functions-udfs-in-sql-queries)
    - [Joining Multiple Views](#joining-multiple-views)
  - [Amazon EMR](#amazon-emr)
    - [Introduction to Amazon EMR](#introduction-to-amazon-emr)
    - [EMR Architecture and Functionality](#emr-architecture-and-functionality)
    - [Integration with AWS Services](#integration-with-aws-services)
    - [Amazon EMR Studio](#amazon-emr-studio)
      - [EMR Serverless and EMR Studio Setup](#emr-serverless-and-emr-studio-setup)
      - [Workspace and Notebook Configuration](#workspace-and-notebook-configuration)
      - [Example PySpark Script](#example-pyspark-script)
  - [AWS Glue Overview](#aws-glue-overview)
    - [Introduction](#introduction-2)
    - [AWS Glue: Pulling Back the Curtain](#aws-glue-pulling-back-the-curtain)
    - [AWS Glue as a Data Integration Service](#aws-glue-as-a-data-integration-service)
    - [Options for Creating and Running Glue Jobs](#options-for-creating-and-running-glue-jobs)
    - [Key Features of AWS Glue](#key-features-of-aws-glue)
    - [AWS Glue Integration with Other AWS Services](#aws-glue-integration-with-other-aws-services)
    - [Example Pipeline from Earlier Course](#example-pipeline-from-earlier-course)
  - [Demo: AWS Glue Visual ETL](#demo-aws-glue-visual-etl)
    - [Overview](#overview-1)
    - [Data Transformation](#data-transformation)
    - [Building a Glue Job in Glue Studio](#building-a-glue-job-in-glue-studio)
    - [Key Steps and Components](#key-steps-and-components)
    - [Post-Execution](#post-execution)
    - [Alternative Approaches](#alternative-approaches)
  - [Technical Considerations](#technical-considerations)
    - [Choosing Between SQL and Python (PySpark DataFrames)](#choosing-between-sql-and-python-pyspark-dataframes)
    - [When to Use a Distributed Framework (Spark)](#when-to-use-a-distributed-framework-spark)
    - [Key Takeaways](#key-takeaways)
- [Streaming Transformations](#streaming-transformations)
  - [Streaming Processing](#streaming-processing)
    - [Overview](#overview-2)
    - [Streaming Transformation Examples](#streaming-transformation-examples)
    - [Streaming Platforms and Processors](#streaming-platforms-and-processors)
    - [Choosing a Streaming Tool](#choosing-a-streaming-tool)
    - [Latency Considerations](#latency-considerations)
    - [Lab Overview - Streaming Change Data Capture (CDC) Pipeline](#lab-overview---streaming-change-data-capture-cdc-pipeline)
- [Week 3 Quiz](#week-3-quiz)
  - [Questions](#questions)
  - [Answers](#answers)

# Batch Transformations

## Overview - Data Transformation at Scale

### Importance of Data Transformation

- Transformation is a crucial stage in the data engineering lifecycle.
- **Data engineers** add value by manipulating and enhancing data for downstream stakeholders.
- **Examples**: Cleaning, combining data, creating central source of truth.

### Use Cases

- **Data Warehouse**: Transforming data into a **star schema** or **data vault** for easier querying.
- **Data Lake**: Applying transformations by moving data from raw to cleaned/transformed to enriched zones.
- Transformations can be applied to **streaming data** for real-time analytics.

### Considerations for Transformation

- **Data size**.
- **Available hardware**.
- **Performance requirements**.
- **Latency requirements** (for streaming data).
- Choice of **single machine processing** vs. **distributed processing** (e.g., Spark).
- Choice of **SQL** vs. other languages like **Python** for transformation logic.

### Distributed Processing Frameworks

- **Hadoop MapReduce**: Disk-based processing. Considered a legacy technology due to complexity, scaling costs, and maintenance. Understanding the **MapReduce paradigm** is still important.
- **Spark**: In-memory processing framework.

### Week Overview

- Batch transformation use cases.
- Comparison of SQL-based transformations with Python.
- Lab: Implementing DBT transformations outside the data warehouse.
- Interview: Navnit Shukla (AWS) on **AWS Glue Service** (built on Spark) and **Glue Studio**.
- Streaming transformations: Implementing a **Change Data Capture (CDC) pipeline** using **Kafka** and **Flink**.

## Batch Transformation Patterns and Use Cases

- [Should I transform data in my sql query?](https://bertwagner.com/posts/should-i-transform-my-data-in-my-sql-query/)

### Batch Transformation Overview

- **Data engineers** often perform batch processing for analytics and machine learning.
- **Batch transformations**: Manipulating discrete data chunks on a fixed schedule (daily, hourly, etc.).

### Transformation Approaches

- **ETL**: Extract, transform (using external tools), and load.
- **ELT**: Extract, load, and transform (within the data warehouse).
- **EtLT**: Extract, simple transform (clean), load, complex transform (within the data warehouse).

#### Examples of ETL and ELT

- **ETL Example**: **AWS Glue ETL** (Spark-based) to transform data before loading into S3.
- **ELT Example**: **DBT** to transform data within the database itself, leveraging its storage and computing resources. DBT is a SQL tool, not an execution tool.

### Data Wrangling (Cleaning and Normalization)

- Turning messy data into clean data.
- Use data wrangling tools to avoid undifferentiated heavy lifting.
- **Example**: **AWS Glue DataBrew** for visual data preparation.

### Data Updates

- **Truncate and Reload**: Delete all records and reload data. Suitable for small datasets and infrequent updates. Resource-intensive for large datasets.
- **Change Data Capture (CDC)**: Identify changes in the source system and update the target system based on those changes (e.g., using "Last Updated" column or transactional logs).

### CDC Patterns

- **Insert-Only**: Insert new records without changing/deleting old records.
- **Upsert/Merge**: Update target records if a match exists (using primary key or logical condition); otherwise, insert a new record.

### Handling Deleted Rows

- **Hard Delete**: Permanently remove records.
- **Soft Delete**: Mark records as deleted without permanently removing them.

### Database Considerations

- Avoid single row inserts into **OLAP** column-oriented databases (anti-pattern).
- Instead, load data in periodic micro-batch or batch fashion for efficient organization and compression.
- Leverage distributed parallel processing capabilities of distributed OLAP systems when inserting data in bulk.

### Scalability and Performance

- Simple transformations and small datasets: Can be processed on a single machine.
- Complex transformations and large datasets: Require distributed processing frameworks.
- Consider applying transformations outside the data warehouse or inside a data lake.

## Distributed Processing Framework - Hadoop

### Introduction

- Engineers have developed many **big data tools** to handle growing data volumes.
- This video focuses on **Apache Hadoop**, an early framework still relevant today.

### The Need for New Tools

- Traditional **monolithic databases** and **data warehouses** couldn't handle massive data cost-effectively or scalably.
- **Commodity hardware** (servers, RAM, disks) became cheap and common.

### Google's Influence

- **2003**: Google published a paper on the **Google File System (GFS)**, a fault-tolerant distributed file system.
- **2004**: Google published a paper on **MapReduce**, a parallel programming paradigm for large-scale data processing.
- These publications were foundational for data technologies and data engineering.

### Apache Hadoop

- Inspired by Google, Yahoo developed **Apache Hadoop** in **2006**.
- **Hadoop Distributed File System (HDFS)** was based on Google's GFS paper.
- **MapReduce** became part of the Hadoop framework.

### Relevance of Hadoop

- While not cutting-edge, Hadoop concepts are important.
- **MapReduce** influences modern distributed systems.
- **HDFS** is a key component of engines like **Amazon EMR** and **Spark**.

### Hadoop Distributed File System (HDFS)

- Similar to object storage, but combines compute and storage on the same nodes.
- Breaks large files into blocks (under a few hundred MB).
- **Name node**: Manages directories, file metadata, and block locations.
- **Data nodes**: Store the data blocks.
- **Replication**: Each block is typically replicated to three nodes for durability and availability. If a node fails, the name node will instruct the other data nodes to replicate the file blocks.
- In-place data processing

### MapReduce Programming Model

- Sends computation code to nodes containing data (locality), rather than moving data to the application.
- **Map tasks**: Read individual data blocks and produce key-value pairs.
- **Shuffle**: Redistributes results so values with the same key are collected on the same node (often using hashing). The results are redistributed by the key.
- **Reduce**: Aggregates data on each node.
- **Example**: SQL query selecting `user_id` and `count(*)` from `user_events` table, grouped by `user_id`.
- **Benefits**: running queries on single blocks.

### Shortcomings of MapReduce

- Utilizes numerous short-lived MapReduce tasks, reading and writing data to disk.
- No intermediate state is preserved in memory.
- High disk bandwidth utilization and increased processing time.

### Evolution Beyond MapReduce

- New frameworks relax MapReduce constraints for in-memory caching.
- **RAM's speed advantage** over SSD/HDDs can dramatically speed up data processing tasks.
- **Spark**: Designed around in-memory processing. Treats data as a distributed set residing in memory, with disk as a second-class storage layer.

### Modern Context

- Hadoop is no longer cutting-edge, and MapReduce is not widely used.
- Advancements in memory utilization will continue to yield gains.

## Distributed Processing Framework - Spark

- [Spark the definitive guide](https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/)
- [Learning Spark](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf)
- [Using Scala UDFs in PySpark](https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9)
- [PySpark UDF](https://datanoon.com/blog/pyspark_udf/)
- [Dataframes-vs-Sparksql](https://www.confessionsofadataguy.com/dataframes-vs-sparksql-which-one-should-you-choose/)
- [PySpark API Reference](https://spark.apache.org/docs/3.5.1/api/python/reference/index.html)
- [PySpark Installation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)
- [Download Apache Spark](https://spark.apache.org/downloads.html)
- [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)
- [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)

### Spark's Origins and Evolution

- Developed at UC Berkeley in 2009 to address shortcomings of **Hadoop MapReduce**.
- Aims for a simpler and faster distributed framework.
- Supports **in-memory storage** for intermediate results and interactive data processing.
- Evolved to support **streaming processing**, **machine learning**, and **graphing libraries**.
- Continuously developing through the **Apache Spark community**.

### Spark's Architecture and Capabilities

- A computing engine for processing distributed large datasets, unlike **Hadoop**, which integrates compute and storage.
- Enables parallel computations and retains intermediate results **in memory**, limiting disk I/O.
- Can be used locally, in data centers, or in the cloud.
- Can load data from and store results on various persistent storage systems (e.g., relational databases, object storage, **Hadoop file system**).
- Provides a unified platform for different analytical workloads like **SQL queries**, **machine learning**, and **streaming transformations**.

### Programming with Spark

- Supports multiple languages: **Python**, **Java**, **Scala**, and **R** via Spark core APIs.
- Offers built-in libraries:
  - **Spark SQL**: For writing SQL queries.
  - **MLlib**: For machine learning applications.
  - **Spark Structured Streaming**: For interacting with real-time data.
  - **GraphX**: For graph processing.
- Supports external third-party libraries (e.g., connectors for data sources, performance monitoring).

### Spark Application Components

- **Cluster of nodes**:
  - **Driver node**: Central controller.
  - **Cluster manager**: Allocates resources.
  - **Worker nodes**: Execute tasks via **Spark executors**.
- **Partitioning**: Data broken into partitions and allocated to executors closest in the network.
- **Spark Session**: Unified entry point for all Spark functionality (defining dataframes, reading data, SQL queries).

### Spark Job Execution

- Driver translates instructions into **Spark jobs**.
- Jobs are executed based on priority.
- Each job is transformed into a sequence of stages represented as a **Directed Acyclic Graph (DAG)**.
- **DAG**: Execution plan for the job.
- Stages are broken down into parallel tasks written in Spark code.
- Stages with shared dependencies are executed serially; independent stages are run in parallel.
- Driver communicates with **cluster manager** to request resources for executors.
- Executors execute tasks and communicate results back to the driver.
- Driver aggregates computations and returns results.

### PySpark

- **Python API** for Apache Spark.
- Supports all Spark features: **Spark SQL**, **Spark DataFrames**, **machine learning**, and **Structured Streaming**.
- Lab focuses on structured data using **Spark DataFrames** and **Spark SQL**.

## Spark DataFrames for Large-Scale Data

### Introduction

- Last week's transformations used **Pandas DataFrames** on a small dataset.
- **Spark DataFrames** enable working with larger, distributed tabular datasets.
- Spark abstracts away the distributed nature, allowing interaction as a single table.

### Resilient Distributed Datasets (RDDs)

- **Spark DataFrames** are built on **RDDs**: a low-level data structure representing a partitioned collection of records.
- Directly working with **RDDs** requires manual definition and optimization of operations.
- **Spark DataFrames** provide simpler, high-level operations (filtering, selecting, counting, aggregating, grouping). Spark compiles these operations down to the **RDD** level.

### Immutability and Operations

- **Spark DataFrames** and **RDDs** are immutable data structures, making them fault-tolerant (resilient).
- Two types of operations: **Transformations** and **Actions**.

#### Transformations

- Create new data frames from existing ones without modifying the original data (e.g., filtering, selecting, joining, grouping).
- Maintain immutability.

#### Actions

- Trigger the execution of transformations (e.g., count, show, save).

### Lazy Evaluation and Fault Tolerance

- **Spark transformations** are evaluated lazily (not executed immediately).
- Transformations are recorded as a lineage, executed only when an action is invoked.
- Lazy evaluation allows **Spark** to optimize the execution plan.
- Lineage and immutability properties ensure fault tolerance by allowing reproduction of the original state in case of failures.

## Demo: Working with Spark DataFrames Using Python

### Overview

- Create **Spark DataFrames**.
- Manipulate existing DataFrames.
- Clean and aggregate data.
- Define custom functions.

### Setup

- Install **Pyspark** locally: `pip install pyspark`. A Spark cluster will be provided within a Docker container in the lab.
- Install **Findspark**: Automatically adds Pyspark to the system path.
- Initialize Findspark after installation.
- A Spark session will have to be created before working with DataFrames.

### Creating SparkSession

- Import `SparkSession` from `pyspark.sql`.
- Create a Spark session using `SparkSession.builder.getOrCreate()`.
- `getOrCreate()` gets an existing session or creates a new one.
- Details include **Spark version** and session name.

### Creating DataFrames

- Manually: Using a list of tuples for data and specifying a schema.
  - Use `spark.createDataFrame(data, schema)`.
- From external data sources (e.g., CSV files).
  - Use `spark.read.csv("file_path", header=True)`.

### Exploring DataFrames

- `show(n)`: View the first 'n' rows of a DataFrame.
- `columns`: View all column names.
- `select(col1, col2, ...)`: Select specific columns.
- `describe()`: Compute summary statistics for each column (count, mean, stddev, min, max).

### Manipulating DataFrames

- `withColumn(col_name, col_expression)`: Adds or updates a column.
  - **Example**: Creating an "amount" column by multiplying "price" and "quantity".
- `withColumnRenamed(existing_col, new_col)`: Renames a column.
- `drop(col_name)`: Removes a column.
- Spark DataFrames are immutable; these methods return new DataFrames.

### Cleaning DataFrames

- `dropna()`: Removes rows with null values.
- `filter(condition)`: Removes rows based on a Boolean condition.
  - **Example**: Filtering rows where "quantity" is greater than zero.

### Aggregating DataFrames

- `groupBy(col1, col2, ...)`: Groups rows based on specified columns.
- Aggregate functions (e.g., `sum()`, `count()`) are applied after `groupBy()`.
- `orderBy(col_name, ascending=False)`: Orders the results.

### User-Defined Functions (UDFs)

- Spark supports user-defined functions (UDFs).
- Import `udf` from `pyspark.sql.functions`.
- Define a Python function (e.g., converting country names to uppercase).
- Wrap the function using `udf(func, return_type)`. Alternatively, use the `@udf(return_type)` decorator.
- Apply the UDF to a column using `select()` or create the new column using `withColumn()`.

#### Performance Considerations

- **Python UDFs** are less efficient than Scala or Java UDFs. Spark is native to Scala. Each executor is a Java Virtual Machine (JVM) process, and Python does not work directly within the JVM.
- Python UDFs require data serialization and transfer between JVM and Python processes.
- Consider rewriting **Python UDFs** in **Scala** or **Java for better performance**. Registered UDFs can still be used in Python, but the performance will be better (running inside of the JVM) than defining the function in Python.

### Interacting with Data Using SQL in PySpark

- Can use **SQL code**, **Python code**, or a mix of both to manipulate data in PySpark.
- Both types of code compile to the same low-level code.

### Temporary Views

- To use SQL queries, create a **temporary view** from a data frame.
- **Temporary view**: Virtual table that exists only while the Spark session is running.
- Created using `createOrReplaceTempView` method on the data frame: e.g., `transactions_df.createOrReplaceTempView("orders")`.
- **SQL queries are issued using the Spark session object's `.sql()` method**.

### User-Defined Functions (UDFs) in SQL Queries

- Can define and use custom functions within SQL queries.
- **Difference from data frames**: Need to _register_ the function instead of wrapping in a UDF object or using the UDF decorator.
- **Registration**: Use the `register` method of the Spark session object: e.g., `spark.udf.register("udf_to_lower", to_lower_function)`.
- Registered functions can then be used by name in SQL queries.

### Joining Multiple Views

- Possible to create multiple temporary views and join them in a single SQL query.
- Example: Create `product_category_df` and create a temporary view called `items`. Then, join `orders` and `items` tables.

## Amazon EMR

### Introduction to Amazon EMR

- **Amazon EMR** is a big data tool supporting various processing frameworks.
- Similar to **Amazon Redshift**, **EMR** uses massively parallel processing.
- Lab will use **EMR** to run **Spark** jobs in **Amazon EMR Studio** notebook.

### EMR Architecture and Functionality

- **Cluster**: Multiple nodes working in parallel.
- **Parallelization**: Jobs run across nodes, processing data in parallel for faster completion.
- **Elasticity**: Cluster can scale up or down as needed.
- **Data Storage**: Results stored in **Amazon S3**, **HDFS**, or other data stores.
- **Frameworks**: Supports **Apache Spark**, **Hadoop**, **Hive**, **Presto**, **Flink**, **HBase**, and more.

### Integration with AWS Services

- Native integration with other **AWS services**.
- **Amazon EMR File System**: Facilitates integration between **S3** and **EMR**, decoupling compute and storage.
- **EMR** streams data from **S3** to instances for processing.
- Multiple **EMR** clusters can process the same dataset from **S3** in different ways.
- Integrates with **Amazon DynamoDB**, **Amazon RDS**, and **Amazon Redshift**.

### Amazon EMR Studio

- Browser-based **IDE** for **Jupyter Notebooks** running on **EMR** clusters.

#### EMR Serverless and EMR Studio Setup

- **EMR Serverless**: Launch a serverless cluster.
- **EMR Studio**: Manage EMR Serverless applications.
- Enable interactive endpoint for **EMR Studio** for interactive workloads.
- Configure storage location (S3) and studio service role (IAM role).

#### Workspace and Notebook Configuration

- Create workspace within **EMR Studio**.
- Attach notebook to compute resources (EMR Serverless application).
- Select interactive runtime role (IAM role) for calling other **AWS services**.
- Choose kernel (e.g., **PySpark**) for the notebook.

#### Example PySpark Script

- Simple **PySpark** script to calculate the average taxi fare between two dates from a file in **S3**.
- Script runs as a job on the **EMR Serverless** cluster.

## AWS Glue Overview

### Introduction

- **Speaker**: Senior Solution Architect, specialized in analytics, AI, and machine learning.
- Collaborated with DeepLearning.AI on lab exercises.
- **Congratulations** on reaching the final course.

### AWS Glue: Pulling Back the Curtain

- Labs included hands-on data infrastructure aspects.
- **AWS Glue** was used for batch ETL, but details were abstracted.
- The presentation reveals the underlying details of setting up data ingestion and transformation jobs.
- **Behind the scenes**: AWS Glue leverages **Apache Spark** for data processing.
- **Goal**: Show how to create **PySpark scripts** for ETL processes using Apache Spark on AWS Glue.

### AWS Glue as a Data Integration Service

- Ingests data from multiple sources: databases, **Amazon S3**, logs, APIs, streaming platforms, etc.
- Performs transformations and loads data into downstream components (database, data lake, data warehouse).
- **Glue job**: An ETL pipeline created using AWS Glue.

### Options for Creating and Running Glue Jobs

- **AWS Glue DataBrew**: No-code/low-code environment, visual data transformation (like Excel with Spark power).
- **Glue Studio**: Drag-and-drop interface for building ETL pipelines with some Spark code writing. For more advanced users.
- **Jupyter Notebook**: Write Spark code from scratch (or with help from Amazon Q Developer/Amazon Q chatbot).
- **ETL Operations**: Performed using Glue ETL; pipelines extracted using Glue triggers, blueprints, and workflow.

### Key Features of AWS Glue

- **Centralized data catalog and data governance**: Essential for building data lakes and lake houses.
- **AWS Glue Crawler**: Crawls data from sources, extracts metadata, and stores it in the Glue Data Catalog.
- **Metadata**: Includes definitions, structures, data types, and partition information.
- **Serverless**: No resource provisioning or management required.
- **Scalable**: Scales up ETL jobs easily (uses **Data Processing Units (DPUs)**).

### AWS Glue Integration with Other AWS Services

- Integration with **Amazon Athena** (SQL queries), **QuickSight** (BI dashboards), **SageMaker** (ML model building/training/deployment).

### Example Pipeline from Earlier Course

- Ingested normalized data from an **Amazon RDS** database.
- Transformed data into a star schema.
- Loaded data into an **Amazon S3** bucket for data analytics.
- ETL portion of the pipeline was executed by running Terraform script to spin up Glue and run a Glue ETL job.
- Python script (**Glue_jobs.py**) was provided in the lab.

## Demo: AWS Glue Visual ETL

### Overview

- Demonstrates how to use **Glue Studio** to generate **Glue jobs**.
- Generates the `glue_job.py` script used in a previous lab.
- The **purpose of the Glue job**: Ingest normalized data from **Amazon RDS**, transform it into a **star schema**, and load it into **Amazon S3**.

### Data Transformation

- **Normalized data**: Tables from customers, product, productline, orders, orderdetails, payment, employees, and offices.
- **Star schema**: Central fact table (orders) with surrounding dimension tables (customers, products, location).
- **Demonstrates the sections of a PySpark script**:
  - Imports from `awsglue` and `pyspark`.
  - Function definitions (e.g., `sparkSqlQuery`).
  - Using `getResolvedOptions` for command-line arguments (JOB_NAME, glue_connections, glue_databases, target S3 path).
  - Setting up SparkContext and glueContext.
  - ETL portion: extracting data, applying transformations (SQL), loading data into S3.

### Building a Glue Job in Glue Studio

- Demonstrates building a Glue job using the **Visual ETL tool** in Glue Studio.
- Covers adding and configuring elements from the **Sources**, **Transform**, and **Target** tabs.
- **Process:**
  - Choose data source (**MySQL** in this case).
  - Add a database connection (**JDBC connection**).
  - Specify IAM role (**Cloud9-de-c1w2-glue-role**).
  - Preview data.
  - Add transformations (**SQL Query transform**).
  - Enter required **SQL code**.
  - Create target nodes for each dimension and fact table (**Amazon S3**).
  - Set **Parquet** as format.
  - Select **S3 bucket** for the target.
- Click on the "Script" tab to see the generated script.
- Set the number of workers and job timeout in "Job details".
- Run the job from the "Run" tab.

### Key Steps and Components

- Creating a new Glue job from the blank Canvas by selecting "+" button to add elements.
- **Source**:
  - **Action**: Extract data from RDS.
  - **Configuration**: Choose MySQL, define connection, specify table, choose IAM role.
- **Transform**:
  - **Action**: Create a transformation from the customer data.
  - **Configuration**: Choose SQL Query, add SQL code to perform, add SQL aliases for customer and source.
- **Target**:
  - **Action**: Specify location to load the data into an S3 bucket.
  - Set up a target node for each of the fact and dimension tables using **Amazon S3**.
  - **Configuration**: Choose S3 target, parent node, Parquet format.

### Post-Execution

- Verify that the transformed tables now exist in S3.
- Summarizes the steps to **define source, transformation, and target**.

### Alternative Approaches

- Acknowledges other ways to create/run a Glue job: **low-code/no-code** data proof approach or manual **Jupyter Notebook** approach.

## Technical Considerations

### Choosing Between SQL and Python (PySpark DataFrames)

- **Considerations**: **Transformation complexity**, **code reusability & testability**, and the team's **skills and technical background**.
- **Simple transformations** (filtering, grouping, aggregation): Comparable performance between Python and SQL, as both translate to the same Spark execution plan.
- **Complex transformations**: Python on Spark DataFrames may be more efficient if the transformation is not straightforward in SQL.
  - **Example**: Transposing a table (supported in DataFrames using `.T`, not in Spark SQL).
- **Code reusability**: DataFrames facilitate writing more testable, maintainable, modular and reusable code.
- **Team skills**: SQL might be simpler for those more familiar with it.
- **Combining approaches**: Can combine Spark DataFrames and Spark SQL for optimal results.

### When to Use a Distributed Framework (Spark)

- **Pandas**: Not a distributed framework; loads entire data into memory. Suitable for datasets that fit in memory.
- **Spark**: Use when data is too large to fit in memory or to leverage distributed computations for better performance. Using Spark on small datasets can be overkill.
- **Extracting only necessary data**: Best practice regardless of using a single machine or cluster. Minimizes resource usage.
- **Pre-ingestion transformations**: Applying transformations like joining, grouping, and filtering within the database to reduce data size.

### Key Takeaways

- **Coding approach**: Balance the simplicity of SQL with Python's flexibility.
- **Tool choice**: Depends on data size and hardware specifications.
- **Understand trade-offs**: Between different approaches.

# Streaming Transformations

- [Spark vs Flink](https://aws.amazon.com/blogs/big-data/a-side-by-side-comparison-of-apache-spark-and-apache-flink-for-common-streaming-use-cases/)

## Streaming Processing

### Overview

- Streaming queries can be used for both real-time analytics and applying **transformations to streaming data**.
- **Streaming transformations**: Preparing data for downstream consumption by converting a stream of events into another stream.
- **Examples**: enriching data with additional info or joining with another stream.

### Streaming Transformation Examples

- **Enriching IOT events**: Dynamically enriching events with device metadata from a separate database.
- **Window queries**: Dynamically computing roll up statistics on windows and sending output to target stream.
- **Joining streams**: Combining website click stream data with IOT data for a unified view.

### Streaming Platforms and Processors

- Events are streamed via platforms like **Kinesis Data Streams** or **Kafka**.
- Processed using stream processors.
- **Distributed stream processing tools**: **Spark Streaming** and **Flink** (for large datasets).
- Both are open-source and allow **Python** or **SQL** for processing.

### Choosing a Streaming Tool

- Consider the **use case**, **latency requirements**, and **performance capabilities**.
- **Spark Streaming**: Micro-batch processing (near real-time). Accumulates small batches (seconds to minutes) and processes in parallel.
- **Flink**: True streaming system; processes one event at a time with continuous node communication.
- **True streaming** has lower latency but more overhead.

### Latency Considerations

- **Micro-batches**: Suitable for sales metrics published every few minutes.
- **True streaming**: Suitable for detecting malicious attacks requiring millisecond-level metrics.

### Lab Overview - Streaming Change Data Capture (CDC) Pipeline

- Use **Debezium** to capture changes from a source database.
- Push changes to a **Kafka** stream.
- Process changes using **Flink**.

# Week 3 Quiz

## Questions

1. Which of the following statements best describe data wrangling?
   1. Data wrangling is the process of loading data into your target systems.
   2. Data wrangling is the process of taking messy, malformed data and turning it into clean data.
   3. Data wrangling is the process of continuously updating the data in your pipeline to make sure that it is in sync with the data in the source system.
   4. Data wrangling is the process of structuring the data into a target schema.
2. Which of the following statements best describe a soft delete?
   1. A soft delete means that you permanently remove a record from your target system.
   2. A soft delete means that you insert a new record with a deleted flag without modifying the previous version of the record.
   3. A soft delete means that you mark the deleted record as “deleted” indicating that the record is no longer active or valid without removing it from the database .
3. How is modeling the data using dbt different from modeling the data using AWS Glue?
   1. dbt and AWS Glue are both processing engines that allow you to transform your data before loading it into the target system. dbt is a SQL-based tool while AWS glue allows you to write SQL queries and python code.
   2. dbt allows you to extract, load and transform your data within your target system, while AWS Glue allows you to extract, transform your data before loading it into your target system.
   3. dbt is a SQL-tool that facilitates the transformation tasks within a database or data warehouse. AWS Glue, which is based on Spark, enables you to extract and transform your data before loading it into your target system.
   4. AWS Glue is a processing engine that is based on Spark while dbt is based on Hadoop.
4. What is the purpose of the shuffle task that comes after the mapping task within a MapReduce process?
   1. It ensures the same data block is replicated at least three times across the data nodes.
   2. It randomly re-distributes the data across the data nodes to repeat the same mapping computation.
   3. It distributes the results of the map task across the data nodes to ensure that the load is balanced between the data nodes.
   4. It redistributes the results of the mapping task across the cluster so that values with the same key are collected together in the same data node.
5. Hadoop and Spark are both distributed processing frameworks. What is the difference between these two frameworks?
   1. Spark is an in-memory processing framework where all intermediate results are saved in memory. With Hadoop, intermediate results are stored on disk.
   2. Spark provides a combination of processing and storage engine, while Hadoop is a pure computing engine.
   3. Both frameworks process the data in the same way, but Spark supports a variety of workloads such as streaming transformations and Machine learning.
6. Which of the following statements is true about Spark DataFrames?
   1. Spark DataFrames are mutable and can be in-place modified.
   2. Spark DataFrame is a high-level data structure on which you can apply expressive and simple operations without worrying how data is distributed behind the scenes.
   3. With Spark DataFrame, you need to manually code all transformations and optimize them for parallelism.
   4. With Spark Dataframes, you can only use the built-in functions to perform operations on the DataFrame.
7. Which of the following best describes the job of the driver node in the Spark's underlying architecture?
   1. The driver node allocates computational and memory resources across a cluster, and manages these resources.
   2. The driver node translates your code into tasks and assign them to the executors.
   3. The driver node executes the tasks in your code.
8. In Spark, transformations are evaluated in a lazy way. What does lazy evaluation mean?
   1. Transformations are evaluated on a small subset of the data and then applied on the entire dataset when an action is invoked.
   2. Transformations aren’t executed until an action is invoked. They are instead recorded as a lineage.
   3. Transformations are executed in incremental small steps.
9. In which of the following cases should you choose to processing your data using Pandas over Spark DataFrames?
   1. You want to make use of a the variety of the built-in functions
   2. You want to leverage parallel and distributed computation for your application.
   3. The data fits into the memory of the machine on which you’re running your code.
   4. You prefer to write your transformation code in Python over SQL.
10. According to this week's videos, in which of the following cases should you choose coding with SQL in PySpark over working with DataFrames using Python to transform your data?
    1. The transformation is simple and can be written in SQL, which could be easier for your team to understand.
    2. The transformation is complex and is not supported by a built-in function in PySpark.
    3. You want to write modular and testable code.
11. Which of the following describes a main difference between the distributed stream processing tools Spark Streaming and Apache Flink?
    1. Apache Flink performs stream processing in a micro-batch way while Spark Streaming processes events one at a time.
    2. Apache Flink offers lower latency stream processing performance than Spark Streaming.
    3. Spark Streaming can handle complex streaming transformations while Apache Flink can only handle simple streaming transformations.

## Answers

1. 2
2. 3
   1. Soft delete means that you mark the record as “deleted” without permanently removing it from your target system. This operation can improve performance and allow data to be recovered should circumstances arise.
3. 3
   1. As you saw in the Batch Transformation Patterns and Use Cases video, dbt is not an execution tool like Spark. It is a SQL-tool that facilitates transformation within the database or data warehouse. AWS Glue is a processing engine based on Spark, enabling data extraction and transformation before loading into the target system.
4. 4
   1. The shuffle step redistributes the key-value pairs generated by the map tasks so that all pairs with the same key are sent to the same reducer.
5. 1
   1. Spark is designed around in-memory processing, while Hadoop is not.
6. 2
   1. Spark DataFrames are indeed a high-level abstraction built on top of RDDs. With DataFrames, you can leverage high-level operations like filtering, selecting, and aggregating without delving into the intricacies of data partitioning and distribution, which are handled by Spark internally.
7. 2
   1. The driver node is the central controller of a Spark application; It transforms each job into a sequence of stages, and represents these stages as a DAG. Each stage is further broken down into tasks written in Spark code that can run in parallel. Once the DAG execution plan is developed, the driver communicates with the cluster manager to request computational and memory resources for the executors. Each task is assigned to a single core within an executor, and each executor works on a single data partition.
8. 2
   1. Transformations are evaluated lazily, meaning that they are not executed immediately. Instead, they are recorded as a lineage and only executed when an action is invoked.
9. 3
   1. If the dataset is not very large, you can use Pandas instead of Spark.
10. 1
    1. When working with simpler transformations, Spark DataFrames and SQL queries generally result in comparable performance. However, you might find writing SQL queries to be simpler, and easier for your team to understand.
11. 2
    1. Spark Streaming processes data in a micro-batch way, providing near real-time performance. Apache Flink is a true stream processing tool that processes one event at a time in a continuous real-time manner, resulting in lower latency than micro-batch processing systems like Spark Streaming.
